You are an expert evaluator tasked with assessing the accuracy of answers generated by a RAG
(Retrieval-Augmented Generation) system.

**Task**: Evaluate whether the generated answer correctly responds to the given question based on the expectedanswer.

**Question**: {question}

**Expected Answer**: {gold_answers}

**Generated Answer**: {assistant_answer}

**Evaluation Criteria**:
1. **Accuracy (0 or 1)**: Does the generated answer match the factual content of the expected answer?
- 1: The generated answer is factually correct and aligns with the expected answer
- 0: The generated answer is factually incorrect or contradicts the expected answer

**Instructions**:
- Focus on factual correctness, not writing style or format
- Consider partial matches: if the generated answer contains the correct information but includes additional context, it should still be considered accurate
- For numerical answers, check if the values match or are equivalent
- For list answers, check if all key elements are present
**"Not answerable" handling**: 
  - If BOTH the expected answer AND generated answer indicate "Not answerable" or inability to answer, consider it accurate (accuracy=1)
  - If the expected answer provides specific content but the generated answer says "Not answerable", it is INCORRECT (accuracy=0)
  - If the expected answer says "Not answerable" but the generated answer provides specific content, it is INCORRECT (accuracy=0)

**Output Format**:
Please respond with a JSON object containing only:
{{
"accuracy": 0 or 1,
"reasoning": "Brief explanation of your evaluation"
}}
- Output MUST be valid JSON (double quotes only).
- Do NOT use LaTeX, math delimiters, or any backslashes "\" in reasoning.
- If you need to mention N_C, write it as "N_C" or "NC" (no "\" characters).
- No code fences, no extra text.
