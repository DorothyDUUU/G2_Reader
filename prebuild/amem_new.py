import os
import json
import time
import pickle
import asyncio
from typing import Dict, List, Tuple, Any
import numpy as np
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
from openai import OpenAI,AsyncOpenAI

from config.config import (
    LLM_BASE_URL,
    LLM_API_KEY,
    EMBED_BASE_URL,
    EMBED_API_KEY,
    MODELS,
    LLM_GENERATION,
    RESPONSE_FORMAT,
    PROMPTS,
    MEMORY_SYSTEMS_DIR,
    PDF_TMP_DIR,
    DATASETS,
    MAX_CONCURRENCY,
    SAVE_CHECKS,
    MINERU,
    PARALLEL_ANALYSIS,
)

from prebuild.memory_layer import AgenticMemorySystem
from prebuild.visdom_utils import (
    extract_text_from_pdf,
    split_text,
    extract_images_from_pdf,
    encode_image,
    clean_text,
    get_pdf,
)
from prebuild.mineru_utils import extract_chunk_from_mineru, extract_image_from_mineru

from prebuild.usage_tracker import add_chat_usage, add_embed_usage, add_stage_duration, add_single_call_duration
from tenacity import retry, stop_after_attempt, wait_exponential
# Optional: tqdm_asyncio is nice to have; if not available, fallback to asyncio.gather
try:
    from tqdm.asyncio import tqdm_asyncio
    _use_tqdm = True
except Exception:
    _use_tqdm = False


# -----------------------------
# Client
# -----------------------------
client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)
aclient = AsyncOpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)

qwen_client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)
qwen_aclient = AsyncOpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)


_llm_sem_by_loop = {}

def _get_llm_semaphore():
    loop = asyncio.get_running_loop()
    sem = _llm_sem_by_loop.get(id(loop))
    if sem is None:
        sem = asyncio.Semaphore(MAX_CONCURRENCY)
        _llm_sem_by_loop[id(loop)] = sem
    return sem

# -----------------------------
# Utilities
# -----------------------------
async def _gather(tasks: List[asyncio.Task]):
    if _use_tqdm:
        return await tqdm_asyncio.gather(*tasks)
    return await asyncio.gather(*tasks)


def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)


def find_actual_file(base_dir: str, filename: str) -> str | None:
    """Robust matching for potentially mangled filenames (encoding issues)."""
    direct_path = os.path.join(base_dir, filename)
    if os.path.exists(direct_path):
        return direct_path

    try:
        if not os.path.exists(base_dir):
            print(f"Warning: directory does not exist: {base_dir}")
            return None
        all_files = os.listdir(base_dir)
        filename_base, filename_ext = os.path.splitext(filename)
        for actual in all_files:
            ab, ae = os.path.splitext(actual)
            if ae.lower() != filename_ext.lower():
                continue
            if ab.lower() == filename_base.lower():
                return os.path.join(base_dir, actual)
            ne = filename_base.encode("utf-8", errors="ignore").decode("utf-8", errors="ignore")
            na = ab.encode("utf-8", errors="ignore").decode("utf-8", errors="ignore")
            if na.lower() == ne.lower():
                return os.path.join(base_dir, actual)
        from difflib import SequenceMatcher
        best, score = None, 0.8
        for actual in all_files:
            if os.path.splitext(actual)[1].lower() == filename_ext.lower():
                r = SequenceMatcher(None, filename.lower(), actual.lower()).ratio()
                if r > score:
                    best, score = actual, r
        if best:
            print(f"æ‰¾åˆ°ç›¸ä¼¼æ–‡ä»¶: {filename} -> {best} (ç›¸ä¼¼åº¦: {score:.2%})")
            return os.path.join(base_dir, best)
        print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°æ–‡ä»¶: {filename}")
        return None
    except Exception as e:
        print(f"æŸ¥æ‰¾æ–‡ä»¶æ—¶å‡ºé”™ {filename}: {e}")
        return None


# -----------------------------
# Dataset helpers
# -----------------------------
_dataset_cache: Dict[str, pd.DataFrame] = {}

def load_dataset_df(name: str) -> pd.DataFrame:
    cfg = DATASETS[name]
    if name in _dataset_cache:
        return _dataset_cache[name]
    enc = cfg.get("encoding", "utf-8")
    df = pd.read_csv(cfg["csv"], encoding=enc)
    _dataset_cache[name] = df
    return df


def resolve_docs_from_dataset(dataset_name: str, q_id: str, limit: int = 5) -> Tuple[str, List[str]]:
    cfg = DATASETS[dataset_name]
    df = load_dataset_df(dataset_name)
    key, docs_col = cfg["key"], cfg["docs_col"]
    matches = np.where(df[key] == q_id)[0]
    if len(matches) == 0:
        raise ValueError(
            f"é”™è¯¯ï¼šåœ¨ {dataset_name}.csv ä¸­æ‰¾ä¸åˆ° {key}='{q_id}' çš„æ•°æ®ã€‚\n"
            f"å¯ç”¨çš„å‰è‹¥å¹² {key} å€¼ï¼š{df[key].unique()[:10].tolist()}..."
        )
    row = df.iloc[matches[0]].to_dict()
    base_dir = cfg["base_dir"]
    doc_names = list(eval(row[docs_col]))[:limit]
    pdf_paths: List[str] = []
    for doc in doc_names:
        p = find_actual_file(base_dir, doc)
        if p:
            pdf_paths.append(p)
        else:
            print(f"è·³è¿‡æ— æ³•æ‰¾åˆ°çš„æ–‡ä»¶: {doc}")
    print(f"æˆåŠŸæ‰¾åˆ° {len(pdf_paths)}/{len(doc_names)} ä¸ªæ–‡ä»¶")
    return base_dir, pdf_paths

# Mineru dir aggregation
def resolve_docs_from_dataset_mineru(dataset_name: str, q_id: str, limit: int = 5) -> Tuple[str, List[str]]:
    cfg = DATASETS[dataset_name]
    df = load_dataset_df(dataset_name)
    key, docs_col = cfg["key"], cfg["docs_col"]

    matches = np.where(df[key] == q_id)[0]
    if len(matches) == 0:
        raise ValueError(
            f"é”™è¯¯ï¼šåœ¨ {dataset_name}.csv ä¸­æ‰¾ä¸åˆ° {key}='{q_id}' çš„æ•°æ®ã€‚\n"
            f"å¯ç”¨çš„å‰è‹¥å¹² {key} å€¼ï¼š{df[key].unique()[:10].tolist()}..."
        )

    row = df.iloc[matches[0]].to_dict()
    base_dir = cfg["mineru_dir"]

    # ============================
    # âœ¨ å»æ‰ .pdf åç¼€ï¼ˆä½ éœ€è¦çš„éƒ¨åˆ†ï¼‰
    # ============================
    doc_names_raw = list(eval(row[docs_col]))[:limit]
    doc_names = [os.path.splitext(d)[0] for d in doc_names_raw]
    # ============================

    mineru_paths: List[str] = []

    for doc in doc_names:
        p = find_actual_file(base_dir, doc)

        # æ·±å…¥ä¸¤å±‚
        for _ in range(2):
            subs = [d for d in os.listdir(p) if os.path.isdir(os.path.join(p, d))]
            if len(subs) == 1:
                p = os.path.join(p, subs[0])
            else:
                break

        if p:
            mineru_paths.append(p)
        else:
            print(f"è·³è¿‡æ— æ³•æ‰¾åˆ°çš„æ–‡ä»¶: {doc}")

    print(f"æˆåŠŸæ‰¾åˆ° {len(mineru_paths)}/{len(doc_names)} ä¸ªæ–‡ä»¶")
    return base_dir, mineru_paths

def _log_failed_response(raw: str, error: Exception, is_multimodal: bool, user_payload):
    """å°†è§£æå¤±è´¥çš„ LLM å“åº”è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶"""
    from datetime import datetime
    log_dir = os.path.join(MEMORY_SYSTEMS_DIR, "_debug_logs")
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    log_file = os.path.join(log_dir, f"failed_response_{timestamp}.txt")
    
    with open(log_file, "w", encoding="utf-8") as f:
        f.write(f"=" * 80 + "\n")
        f.write(f"æ—¶é—´: {datetime.now().isoformat()}\n")
        f.write(f"ç±»å‹: {'multimodal' if is_multimodal else 'text'}\n")
        f.write(f"é”™è¯¯: {type(error).__name__}: {error}\n")
        f.write(f"=" * 80 + "\n\n")
        
        f.write("ã€åŸå§‹å“åº” (raw response)ã€‘\n")
        f.write("-" * 40 + "\n")
        f.write(raw if raw else "(ç©ºå“åº”)")
        f.write("\n" + "-" * 40 + "\n\n")
        
        f.write(f"å“åº”é•¿åº¦: {len(raw) if raw else 0} å­—ç¬¦\n")
        f.write(f"finish_reason: (è§ä¸Šæ–¹é”™è¯¯ä¿¡æ¯)\n\n")
        
        # è®°å½•è¾“å…¥ï¼ˆå¯¹äºæ–‡æœ¬ç±»å‹ï¼‰
        if not is_multimodal and isinstance(user_payload, str):
            f.write("ã€è¾“å…¥å†…å®¹ (user_payload å‰2000å­—ç¬¦)ã€‘\n")
            f.write("-" * 40 + "\n")
            f.write(user_payload[:2000])
            f.write("\n" + "-" * 40 + "\n")
    
    print(f"[DEBUG] è§£æå¤±è´¥çš„å“åº”å·²ä¿å­˜åˆ°: {log_file}")
    return log_file


async def call_llm_json(system: str, user_payload, *, is_multimodal: bool = False):
    sem = _get_llm_semaphore()
    async with sem:
        call_start = time.time()  # è®°å½•å•æ¬¡è°ƒç”¨å¼€å§‹æ—¶é—´
        try:
            resp = await qwen_aclient.chat.completions.create(
                    model=MODELS["chat"],
                    messages=[
                        {"role": "system", "content": system},
                        {"role": "user", "content": user_payload},
                    ],
                    response_format={"type": "json_object"},
                    # frequency_penalty=0.5,
                    **LLM_GENERATION,
                )
        except Exception as err:
                # network / timeout / Azure filtering
            print(f"[ERROR] LLM request failed: {err}")
            await asyncio.sleep(1)
            raise  # âŒ æŠ›å‡ºå¼‚å¸¸ï¼Œä¸è®°å½•æ­¤æ¬¡è°ƒç”¨æ—¶é—´

        # âœ… åªæœ‰æˆåŠŸçš„è°ƒç”¨æ‰è®¡ç®—è€—æ—¶
        call_duration = time.time() - call_start
        
        try:
            qkind = "analyze_multimodal" if is_multimodal else "analyze_text"
            add_chat_usage(
                getattr(resp, "usage", None),
                {"model": MODELS["chat"], "qkind": qkind}
            )
            # åªè®°å½•æˆåŠŸè°ƒç”¨çš„æœ€å¤§è€—æ—¶
            stage = "image_analysis" if is_multimodal else "text_analysis"
            add_single_call_duration(stage, call_duration)
        except Exception:
            pass 

        raw = resp.choices[0].message.content
        finish_reason = resp.choices[0].finish_reason if resp.choices else None
        
        # æ£€æŸ¥æ˜¯å¦è¢«æˆªæ–­
        if finish_reason == "length":
            print(f"[WARNING] å“åº”è¢«æˆªæ–­ (finish_reason=length)ï¼Œå¯èƒ½å¯¼è‡´ JSON ä¸å®Œæ•´")
        
        try:
            return json.loads(raw)
        except json.JSONDecodeError as e:
            print(f"{e}")
            # å°è¯•æå– JSON å¯¹è±¡
            import re
            m = re.search(r"\{[\s\S]*\}", raw)
            if m:
                try:
                    return json.loads(m.group(0))
                except json.JSONDecodeError as e2:
                    # è®°å½•å¤±è´¥çš„å“åº”
                    _log_failed_response(raw, e2, is_multimodal, user_payload)
                    raise
            # è®°å½•å¤±è´¥çš„å“åº”
            _log_failed_response(raw, e, is_multimodal, user_payload)
            raise
    

async def analyze_content(payload: str, *, modality: str) -> Dict[str, any]:
    """Unified analyzer for text or image.
    - modality: 'text' (payload is plain text) or 'image' (payload is base64 string)
    """
    system = "You must respond with a JSON object."
    if modality == "text":
        content = clean_text(payload)
        user = PROMPTS["text_keyword"] + content
        return await call_llm_json(system, user)
    elif modality == "image":
        user = [
            {"type": "text", "text": PROMPTS["image_keyword"]},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{payload}"}},
        ]
        return await call_llm_json(system, user, is_multimodal=True)
    else:
        raise ValueError("modality must be 'text' or 'image'")

async def analyze_content_mineru(payload: str, *, modality: str, context: str = "", caption: str = "") -> Dict[str, any]:
    """Unified analyzer for text or image.
    - modality: 'text' (payload is plain text) or 'image' (payload is base64 string)
    """
    system = "You must respond with a JSON object."
    if modality == "text":
        content = clean_text(payload)
        user = PROMPTS["text"] + content
        return await call_llm_json(system, user)
    elif modality == "image":
        prompt  = PROMPTS["image_ocr_keyword"].replace("{context}", context).replace("{caption}", caption)
        user = [
            {"type": "text", "text": prompt},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{payload}"}},
        ]

        return await call_llm_json(system, user, is_multimodal=True)
    else:
        raise ValueError("modality must be 'text' or 'image'")
# -----------------------------
# Embedding helpers
# -----------------------------
async def embed_one(text: str, kind: str = "embedding") -> List[float]:
    # ä½¿ç”¨å•ç‹¬çš„ Embedding å¼‚æ­¥å®¢æˆ·ç«¯
    call_start = time.time()  # è®°å½•å•æ¬¡è°ƒç”¨å¼€å§‹æ—¶é—´
    try:
        resp = await embed_aclient.embeddings.create(model=MODELS["embed"], input=text)
    except Exception as err:
        # âŒ API å¤±è´¥ï¼Œä¸è®°å½•æ­¤æ¬¡è°ƒç”¨æ—¶é—´
        print(f"[ERROR] Embedding request failed: {err}")
        raise
    
    # âœ… åªæœ‰æˆåŠŸçš„è°ƒç”¨æ‰è®¡ç®—è€—æ—¶
    call_duration = time.time() - call_start
    
    try:
        add_embed_usage(getattr(resp, "usage", None), {"model": MODELS["embed"], "kind": kind})
        # åªè®°å½•æˆåŠŸè°ƒç”¨çš„æœ€å¤§è€—æ—¶
        add_single_call_duration(kind, call_duration)
    except Exception:
        pass
    return resp.data[0].embedding  # type: ignore


async def embed_many(texts: List[str], kind: str = "embedding") -> List[List[float]]:
    # simple concurrency control to avoid overwhelming server
    sem = asyncio.Semaphore(MAX_CONCURRENCY)

    async def _task(t: str):
        async with sem:
            return await embed_one(t, kind=kind)

    return await asyncio.gather(*[_task(t) for t in texts])


# -----------------------------
# Public API (main entry points)
# -----------------------------
async def construct_memory(
    pdf_path: str,
    *,
    evolve_iters: int = 1,
    window_size: int = 2,
) -> AgenticMemorySystem:
    """
    Build the memory system for a given q_id (dataset) or a remote/local PDF path.

    Keeps visdom_utils & memory_layer imports intact.
    Uses config for URLs, API keys, absolute paths, and prompts.
    """
    print("\n" + "=" * 80)
    print(f"å¼€å§‹æ„å»ºMemory System: {pdf_path}")
    print("=" * 80)

    ensure_dir(MEMORY_SYSTEMS_DIR)
    ensure_dir(PDF_TMP_DIR)

    # Detect dataset by substring; else treat as URL/local file and download
    detected_dataset = next((n for n in DATASETS.keys() if n in pdf_path), None)

    flag_dataset = detected_dataset is not None
    flag_mineru = MINERU 
    name = pdf_path  # use provided key/q_id as memory name for dataset cases
    if flag_dataset:
        if flag_mineru:
            _, pdf_paths = resolve_docs_from_dataset_mineru(detected_dataset, pdf_path)
        else:
            _, pdf_paths = resolve_docs_from_dataset(detected_dataset, pdf_path)
        print(pdf_paths)
    else:
        print("Downloading pdf")
        filename = time.strftime("%Y%m%d_%H%M%S") + ".pdf"
        out_path = os.path.join(PDF_TMP_DIR, filename)
        try:
            await get_pdf(pdf_path, out_path)
        except Exception as e:
            print(f"Error downloading pdf: {e}")
            raise
        pdf_paths = [out_path]

    # Load or initialize memory system
    existing = set(os.listdir(MEMORY_SYSTEMS_DIR))
    if name in existing:
        print(f"Loading existing memory system: {name}")
        ms = AgenticMemorySystem(model_name=MODELS["embed"], llm_model=MODELS["chat"])  # type: ignore
        ms.load_memory_system(name+"_iter_"+str(evolve_iters))
    else:
        print("initializing memory system")
        ms = AgenticMemorySystem(model_name=MODELS["embed"], llm_model=MODELS["chat"])  # type: ignore

        if not flag_mineru:
            # --- Extract text & images ---
            pages = []
            for p in pdf_paths:
                pages.extend(extract_text_from_pdf(p))#é’ˆå¯¹æ¯ä¸ªpdfæå–
            chunks = []
            for page in pages:
                chunks.extend(split_text(page)) #é’ˆå¯¹æ¯ä¸ªpageåˆ†chunk
             
            images = []
            for pdf_path in pdf_paths:
                images.extend(extract_images_from_pdf(pdf_path))  
             
            images_b64 = [encode_image(image) for image in images]
        else:
            chunks = []
            images_b64 = []
            contexts = []
            captions = []
            for p in pdf_paths:
                chunks.extend(extract_chunk_from_mineru(p))  
                imgs, context,caption = extract_image_from_mineru(p)
                images_b64.extend(encode_image(img) for img in imgs)
                contexts.extend(context)
                captions.extend(caption)


        # Analyzeä¹Ÿè¦æ”¹ TODO
        
        # ============================================================
        # å®šä¹‰æ–‡æœ¬å¤„ç†å‡½æ•°ï¼ˆåˆ†æ + åµŒå…¥ï¼‰
        # ============================================================
        async def process_text_content():
            """å¤„ç†æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼šåˆ†æ + åµŒå…¥"""
            print(f"Analyzing textual content ({len(chunks)} chunks)...")
            text_analysis_start = time.time()
            text_tasks = [analyze_content(ch, modality="text") for ch in chunks]
             
            try:
                text_results = await asyncio.gather(*text_tasks, return_exceptions=True)
            except Exception as e:
                print(f"é”™è¯¯ï¼šæ–‡æœ¬åˆ†ææ‰¹é‡å¤„ç†å¤±è´¥: {e}")
                raise
            text_analysis_duration = time.time() - text_analysis_start
            add_stage_duration("text_analysis", text_analysis_duration)
            
            text_contents: Dict[int, Dict[str, Any]] = {}
            failed = 0
            for i, r in enumerate(text_results):
                if isinstance(r, Exception) or not isinstance(r, dict) or "summary" not in r:
                    print(r)
                    failed += 1
                    text_contents[i] = {"summary": "å†…å®¹åˆ†æå¤±è´¥", "keywords": ["æœªçŸ¥"], "tags": ["é”™è¯¯"]}
                else:
                    text_contents[i] = r
            if failed:
                print(f"è­¦å‘Šï¼šå…±æœ‰ {failed}/{len(chunks)} ä¸ªæ–‡æœ¬å—åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼å¡«å……")
            if len(text_contents) == 0:
                raise RuntimeError("âŒ ä¸¥é‡é”™è¯¯ï¼šæ‰€æœ‰æ–‡æœ¬å—åˆ†æå‡å¤±è´¥ï¼")
            print(f"æ–‡æœ¬åˆ†æå®Œæˆ: {len(text_contents) - failed}/{len(chunks)} æˆåŠŸ")
             
            # Embed text
            print("Embedding textual content")
            text_embedding_start = time.time()
            text_docs = [
                f"{text_contents[i]['summary']} keywords: {', '.join(text_contents[i]['keywords'])}"
                for i in range(len(text_contents))
            ]
            try:
                text_vecs = await embed_many(text_docs, kind="text_embedding")
                for i, emb in enumerate(text_vecs):
                    text_contents[i]["embedding"] = [emb]
                print(f"æ–‡æœ¬åµŒå…¥å®Œæˆ: {len(text_vecs)} ä¸ªå‘é‡")
            except Exception as e:
                print(f"é”™è¯¯ï¼šæ–‡æœ¬åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
                raise
            text_embedding_duration = time.time() - text_embedding_start
            add_stage_duration("text_embedding", text_embedding_duration)
            
            return text_contents
        
        # ============================================================
        # å®šä¹‰å›¾åƒå¤„ç†å‡½æ•°ï¼ˆåˆ†æ + åµŒå…¥ï¼‰
        # ============================================================
        async def process_image_content():
            """å¤„ç†æ‰€æœ‰å›¾åƒå†…å®¹ï¼šåˆ†æ + åµŒå…¥"""
            print(f"Analyzing visual content ({len(images_b64)} images)...")
            image_contents: Dict[int, Dict[str, Any]] = {}
            
            if not images_b64:
                print("è·³è¿‡å›¾åƒåˆ†æï¼šæœªä»PDFä¸­æå–åˆ°å›¾åƒ")
                return image_contents
            
            image_analysis_start = time.time()
            if not MINERU:
                img_tasks = [analyze_content(b64, modality="image") for b64 in images_b64]
            else:
                img_tasks = [analyze_content_mineru(b64, modality="image", context=contexts[i], caption=captions[i]) for i, b64 in enumerate(images_b64)]
            
            try:
                img_results = await asyncio.gather(*img_tasks, return_exceptions=True)
            except Exception as e:
                print(f"é”™è¯¯ï¼šå›¾åƒåˆ†ææ‰¹é‡å¤„ç†å¤±è´¥: {e}")
                raise
            image_analysis_duration = time.time() - image_analysis_start
            add_stage_duration("image_analysis", image_analysis_duration)
            
            img_failed = 0
            for i, r in enumerate(img_results):
                if isinstance(r, Exception) or not isinstance(r, dict) or "summary" not in r:
                    img_failed += 1
                    image_contents[i] = {"summary": "å›¾åƒåˆ†æå¤±è´¥", "keywords": ["æœªçŸ¥"], "tags": ["é”™è¯¯"]}
                else:
                    image_contents[i] = r
            if img_failed:
                print(f"è­¦å‘Šï¼šå…±æœ‰ {img_failed}/{len(images_b64)} ä¸ªå›¾åƒåˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼å¡«å……")
            if img_failed == len(images_b64):
                print("è­¦å‘Šï¼šæ‰€æœ‰å›¾åƒåˆ†æå‡å¤±è´¥ï¼å°†è·³è¿‡å›¾åƒnotesæ·»åŠ ã€‚")
                return {}
            else:
                print(f"å›¾åƒåˆ†æå®Œæˆ: {len(image_contents) - img_failed}/{len(images_b64)} æˆåŠŸ")
            
            # Embed images (if any valid)
            if image_contents:
                print("Embedding visual content")
                image_embedding_start = time.time()
                img_docs = [
                    f"{image_contents[i]['summary']} keywords: {', '.join(image_contents[i]['keywords'])}"
                    for i in range(len(image_contents))
                ]
                try:
                    img_vecs = await embed_many(img_docs, kind="image_embedding")
                    for i, emb in enumerate(img_vecs):
                        image_contents[i]["embedding"] = [emb]
                    print(f"å›¾åƒåµŒå…¥å®Œæˆ: {len(img_vecs)} ä¸ªå‘é‡")
                except Exception as e:
                    print(f"é”™è¯¯ï¼šå›¾åƒåµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
                    raise
                image_embedding_duration = time.time() - image_embedding_start
                add_stage_duration("image_embedding", image_embedding_duration)
            
            return image_contents
        
        if PARALLEL_ANALYSIS:
            print("ğŸš€ ä½¿ç”¨å¹¶è¡Œåˆ†ææ¨¡å¼ï¼ˆæ–‡æœ¬å’Œå›¾åƒåŒæ—¶å¤„ç†ï¼‰")
            parallel_start = time.time()
            text_contents, image_contents = await asyncio.gather(
                process_text_content(),
                process_image_content()
            )
            parallel_duration = time.time() - parallel_start
            print(f"âœ“ å¹¶è¡Œå¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {parallel_duration:.2f}ç§’")
        else:
            print("ğŸ“‹ ä½¿ç”¨ä¸²è¡Œåˆ†ææ¨¡å¼ï¼ˆæ–‡æœ¬å’Œå›¾åƒä¾æ¬¡å¤„ç†ï¼‰")
            text_contents = await process_text_content()
            image_contents = await process_image_content()
         
        # --- Add notes ---
        # filter out the notes with "No meaningful information" in the summary
        to_remove = [i for i, text_content in text_contents.items() if "No meaningful information" in text_content["summary"]]
        print(f"è¿‡æ»¤æ‰ {len(to_remove)} æ¡åŒ…å« 'No meaningful information' çš„æ–‡æœ¬notes")
        
        print("Adding textual notes to memory system")
        ok, bad = 0, 0
        for i, ch in enumerate(chunks):
            if i in to_remove:
                continue
            try:
                ms.add_note(
                    content=ch,
                    context=text_contents[i]["summary"],
                    keywords=text_contents[i]["keywords"],
                    tags=text_contents[i]["tags"],
                    category="text",
                    pre_embeddings=text_contents[i]["embedding"],
                )
                ok += 1
            except Exception as e:
                bad += 1
                if bad <= 3:
                    import traceback; traceback.print_exc()
        print(f"æ–‡æœ¬notesæ·»åŠ å®Œæˆ: {ok} æˆåŠŸ, {bad} å¤±è´¥")
        if ok == 0:
            raise RuntimeError("âŒ ä¸¥é‡é”™è¯¯ï¼šæœªèƒ½æˆåŠŸæ·»åŠ ä»»ä½•æ–‡æœ¬noteï¼")
         
        print("Adding visual notes to memory system")
        ok_i, bad_i = 0, 0
        for i, b64 in enumerate(images_b64):
            if i not in image_contents:
                continue
            try:
                ms.add_note(
                    content=b64,
                    context=image_contents[i]["summary"],
                    keywords=image_contents[i]["keywords"],
                    tags=image_contents[i]["tags"],
                    text_content = image_contents[i]["text_content"],
                    category="image",
                    visual=True,
                    pre_embeddings=image_contents[i]["embedding"],
                )
                ok_i += 1
            except Exception:
                bad_i += 1
        print(f"å›¾åƒnotesæ·»åŠ å®Œæˆ: {ok_i} æˆåŠŸ, {bad_i} å¤±è´¥")
         
        # --- Initialize local links for text notes only ---
        text_count = ok
        for i, note in enumerate(list(ms.memories.values())[:text_count]):
            for d in range(-window_size, window_size + 1):
                if d == 0:
                    continue
                j = i + d
                if 0 <= j < text_count:
                    note.links.append(j)
            note.links = list(set(note.links))
        print("links initialized")
        
        
        
        # save the memory system before evolving
        print(f"\nä¿å­˜Memory System: {name}")
        print(f"  å‡†å¤‡ä¿å­˜ {len(ms.memories)} æ¡è®°å¿†...")
        ms.save_memory_system(name+"_iter_0")
         
        # --- Evolve (optional) ---
        for it in range(evolve_iters):
            print(f"Evolving memory system: iteration {it + 1}")
            evolution_start = time.time()
            _ = await ms.process_memory_all()
            evolution_duration = time.time() - evolution_start
            add_stage_duration("memory_evolution", evolution_duration)
            
            print(f"Re-embedding after evolution iteration {it + 1}")
            re_embedding_start = time.time()
            meta = [n.context + " keywords: " + ", ".join(n.keywords) for n in ms.memories.values()]
            pre = await embed_many(meta, kind="re_embedding")
            ms.reset_retriever()
            ms.retriever.add_documents(meta, pre)
            re_embedding_duration = time.time() - re_embedding_start
            add_stage_duration("re_embedding", re_embedding_duration)
            
            # exclude itself in links
            for i, note in enumerate(list(ms.memories.values())):
                note.links = [j for j in note.links if j != i]
                
            # save the memory system after each iteration
            print(f"\nä¿å­˜Memory System: {name}")
            print(f"  å‡†å¤‡ä¿å­˜ {len(ms.memories)} æ¡è®°å¿†...")
            ms.save_memory_system(name+"_iter_"+str(it+1))

    if not flag_dataset:
        for p in pdf_paths:
            try:
                os.remove(p)
            except Exception:
                pass

    print("\n" + "=" * 80)
    print("Memory System æ„å»ºå®Œæˆï¼")
    print(f"   - åç§°: {name}")
    print(f"   - æ€»è®°å¿†æ•°: {len(ms.memories)}")
    print(f"   - ä¿å­˜è·¯å¾„: {MEMORY_SYSTEMS_DIR}/{name}/")
    print("=" * 80 + "\n")
    return ms


def search_memory(memory_system: AgenticMemorySystem, query_or_keywords, k: int = 10, modality: str = "all", method: str = "semantic", top_k_text: int = 5, top_k_image: int = 5):
    if method == "semantic":
        _, notes = memory_system.find_related_notes_original(str(query_or_keywords), k=k, modality=modality)
        return notes
    elif method == "keywords":
        return memory_system.search_keyword(keywords=list(query_or_keywords), modality=modality, top_k_text=top_k_text, top_k_image=top_k_image)
    else:
        _, notes = memory_system.find_related_notes_original(str(query_or_keywords), k=k, modality=modality)
        return notes
