{"_id": "paper_tab_0", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What baseline approaches using state-of-the-art PDTB taggers were employed for the evaluation of causality prediction in the automatic causal explanation analysis pipeline?", "answer": "[{'answer': 'state-of-the-art PDTB taggers', 'type': 'extractive'}, {'answer': 'Linear SVM, RBF SVM, and Random Forest', 'type': 'abstractive'}]", "main_doc": "1809.01202.pdf", "documents": "['1809.01202.pdf', '1908.06083.pdf', '2002.01984.pdf', '1809.03449.pdf', '2002.06675.pdf', '2002.01207.pdf', '1912.01772.pdf', '1809.02279.pdf', '1904.05584.pdf', '1704.05907.pdf', '1910.14497.pdf', '1903.00172.pdf', '1911.05153.pdf', '1901.04899.pdf']"}
{"_id": "paper_tab_1", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the joint goal accuracy and inference time complexity, does the model demonstrate superior performance in the single-domain WoZ2.0 test set or the multi-domain MultiWoZ test set?", "answer": "[{'answer': 'single-domain setting', 'type': 'abstractive'}]", "main_doc": "1909.00754.pdf", "documents": "['1909.00754.pdf', '1710.09340.pdf', '2004.01980.pdf', '1910.12129.pdf', '1701.06538.pdf', '1811.12254.pdf', '1902.10525.pdf']"}
{"_id": "paper_tab_2", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which hierarchical model variant achieves a higher BLEU score of 17.5 compared to the Flat scenario (16.7)?", "answer": "[{'answer': 'Hierarchical-k', 'type': 'extractive'}]", "main_doc": "1912.10011.pdf", "documents": "['1912.10011.pdf', '1809.08298.pdf', '1810.12085.pdf', '1905.11901.pdf', '1908.10084.pdf', '2001.08051.pdf', '1909.08041.pdf', '1910.14497.pdf', '1909.08859.pdf', '2004.04721.pdf', '1809.00540.pdf', '1909.11297.pdf', '1610.00879.pdf', '2002.02070.pdf', '2002.01359.pdf', '1811.02906.pdf']"}
{"_id": "paper_tab_3", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What were the F-scores of the best performing model on the NALCS (English) and LMS (Traditional Chinese) test sets, as reported in the video highlight prediction paper using multimodal and multilingual audience chat reactions?", "answer": "[{'answer': 'Best model achieved F-score 74.7 on NALCS and F-score of 70.0 on LMS on test set', 'type': 'abstractive'}]", "main_doc": "1707.08559.pdf", "documents": "['1707.08559.pdf', '1909.03135.pdf', '2003.11563.pdf', '1707.05236.pdf', '1906.05474.pdf', '2002.01664.pdf', '1901.08079.pdf', '2002.08899.pdf', '1904.05584.pdf', '1810.12085.pdf', '1712.00991.pdf', '1901.05280.pdf', '1709.05413.pdf', '1901.02262.pdf', '1701.06538.pdf', '1909.00252.pdf', '1707.03569.pdf', '1912.01772.pdf']"}
{"_id": "paper_tab_4", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the average token count of claims in the PERSPECTRUM dataset?", "answer": "[{'answer': 'Average claim length is 8.9 tokens.', 'type': 'abstractive'}]", "main_doc": "1906.03538.pdf", "documents": "['1906.03538.pdf', '1705.00108.pdf', '1901.09755.pdf', '1908.05828.pdf', '2001.08051.pdf', '1705.01214.pdf', '1909.08089.pdf', '2002.08899.pdf']"}
{"_id": "paper_tab_5", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does incorporating shallow syntax-aware embeddings lead to better performance on the sentiment classification task compared to using standard ELMo embeddings?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'No', 'type': 'boolean'}]", "main_doc": "1908.11047.pdf", "documents": "['1908.11047.pdf', '1910.03467.pdf', '1703.06492.pdf', '1810.12885.pdf', '1906.11180.pdf', '1809.10644.pdf']"}
{"_id": "paper_tab_6", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Out of the 9 probe tasks, how many tasks show better performance with the mSynC model compared to baseline ELMo-transformer embeddings in the \"Shallow Syntax in Deep Water\" study?", "answer": "[{'answer': 'performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks', 'type': 'extractive'}, {'answer': '3', 'type': 'abstractive'}]", "main_doc": "1908.11047.pdf", "documents": "['1908.11047.pdf', '2003.11645.pdf', '1906.01081.pdf', '1904.10500.pdf', '1911.08673.pdf', '1907.09369.pdf']"}
{"_id": "paper_tab_7", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What specific set of black-box probes, as described in the evidence section of this paper, were used to analyze the role of shallow syntactic awareness in the enhanced ELMo architecture's embeddings for capturing linguistic properties?", "answer": "[{'answer': 'CCG Supertagging CCGBank , PTB part-of-speech tagging, EWT part-of-speech tagging,\\nChunking, Named Entity Recognition, Semantic Tagging, Grammar Error Detection, Preposition Supersense Role, Preposition Supersense Function, Event Factuality Detection', 'type': 'abstractive'}, {'answer': 'Probes are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases.', 'type': 'extractive'}]", "main_doc": "1908.11047.pdf", "documents": "['1908.11047.pdf', '2001.10161.pdf', '1911.11951.pdf', '1911.01799.pdf', '1908.07816.pdf', '1811.02906.pdf', '1809.02279.pdf', '1904.10500.pdf', '1910.10288.pdf']"}
{"_id": "paper_tab_8", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the findings in *Shallow Syntax in Deep Water*, how does mSynC's performance on downstream NLP tasks, especially in chunk tag prediction, compare to that of ELMo-transformer, and what does this imply about the utility of shallow syntactic features?", "answer": "[{'answer': 'only modest gains on three of the four downstream tasks', 'type': 'extractive'}, {'answer': ' the performance differences across all tasks are small enough ', 'type': 'extractive'}]", "main_doc": "1908.11047.pdf", "documents": "['1908.11047.pdf', '1711.11221.pdf', '1611.03382.pdf', '1709.10367.pdf', '1910.00458.pdf', '1701.02877.pdf', '1911.02086.pdf', '1909.09484.pdf']"}
{"_id": "paper_tab_9", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How were the six predefined proficiency indicators scored in the 2017/2018 recordings of the TLT-school corpus for both spoken and written items, and what values did human experts assign to reflect proficiency levels?", "answer": "[{'answer': 'They used 6 indicators for proficiency (same for written and spoken) each marked by bad, medium or good by one expert.', 'type': 'abstractive'}]", "main_doc": "2001.08051.pdf", "documents": "['2001.08051.pdf', '1909.08041.pdf', '1901.02257.pdf', '1912.10435.pdf', '1910.06748.pdf', '1909.00754.pdf', '2001.10161.pdf', '1903.00172.pdf', '1712.03547.pdf', '1904.10503.pdf', '1804.07789.pdf', '2003.04866.pdf', '1901.03866.pdf', '1909.08824.pdf', '1902.00672.pdf', '2002.01664.pdf', '1902.00330.pdf', '1908.05828.pdf']"}
{"_id": "paper_tab_10", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the six linguistic proficiency indicators outlined in the TLT-school corpus that human experts used to assess the English and German utterances of non-native speaking students?", "answer": "[{'answer': '6 indicators:\\n- lexical richness\\n- pronunciation and fluency\\n- syntactical correctness\\n- fulfillment of delivery\\n- coherence and cohesion\\n- communicative, descriptive, narrative skills', 'type': 'abstractive'}]", "main_doc": "2001.08051.pdf", "documents": "['2001.08051.pdf', '1901.05280.pdf', '1712.03547.pdf', '1809.00540.pdf', '1910.12795.pdf', '1603.00968.pdf', '2001.05970.pdf', '1910.04269.pdf', '1806.04330.pdf', '2002.02492.pdf', '1908.08345.pdf', '1901.04899.pdf', '1811.12254.pdf', '1909.13695.pdf', '1804.08050.pdf']"}
{"_id": "paper_tab_11", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the Word Error Rates (WER) achieved by the adapted Automatic Speech Recognition (ASR) system on the non-native English and German children's speech evaluation data in the TLT-school corpus?", "answer": "[{'answer': 'Accuracy not available: WER results are reported 42.6 German, 35.9 English', 'type': 'abstractive'}]", "main_doc": "2001.08051.pdf", "documents": "['2001.08051.pdf', '1804.08139.pdf', '1909.08041.pdf', '1809.09194.pdf', '1804.08050.pdf', '1701.00185.pdf']"}
{"_id": "paper_tab_12", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What metric is used to evaluate the performance of the adapted ASR system on non-native children's English and German speech?", "answer": "[{'answer': 'Speech recognition system is evaluated using WER metric.', 'type': 'abstractive'}]", "main_doc": "2001.08051.pdf", "documents": "['2001.08051.pdf', '1910.06036.pdf', '2004.01878.pdf', '2004.01980.pdf', '1908.05434.pdf', '1911.12579.pdf', '1911.08962.pdf', '1901.09755.pdf', '1610.07809.pdf', '1709.10217.pdf', '1910.03814.pdf', '1902.09393.pdf', '1909.01013.pdf', '1812.10479.pdf', '1704.05907.pdf', '1710.06700.pdf', '2001.10161.pdf']"}
{"_id": "paper_tab_13", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How many utterances were manually transcribed in the TLT-school corpus, across both English and German languages, including train and evaluation sets?", "answer": "[{'answer': 'Total number of transcribed utterances including Train and Test for both Eng and Ger language is 5562 (2188 cleaned)', 'type': 'abstractive'}]", "main_doc": "2001.08051.pdf", "documents": "['2001.08051.pdf', '1809.00540.pdf', '1809.06537.pdf', '1711.00106.pdf', '1811.12254.pdf', '2002.11402.pdf', '1908.11365.pdf', '1904.03288.pdf', '1808.03430.pdf', '1606.00189.pdf', '1910.11769.pdf', '1809.09194.pdf', '1902.09314.pdf', '1909.06937.pdf', '1909.11687.pdf', '1902.09393.pdf', '2002.06424.pdf', '1809.04960.pdf']"}
{"_id": "paper_tab_14", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the baseline models compared in this paper, which summarizes prior work on span and dependency SRL, including models like Fitzgerald2015?", "answer": "[{'answer': '2008 Punyakanok et al. \\n2009 Zhao et al. + ME \\n2008 Toutanova et al. \\n2010 Bjorkelund et al.  \\n2015 FitzGerald et al. \\n2015 Zhou and Xu \\n2016 Roth and Lapata \\n2017 He et al. \\n2017 Marcheggiani et al.\\n2017 Marcheggiani and Titov \\n2018 Tan et al. \\n2018 He et al. \\n2018 Strubell et al. \\n2018 Cai et al. \\n2018 He et al. \\n2018 Li et al. \\n', 'type': 'abstractive'}]", "main_doc": "1901.05280.pdf", "documents": "['1901.05280.pdf', '1910.08987.pdf', '1911.10049.pdf', '1706.08032.pdf', '1806.07711.pdf', '2002.04181.pdf', '1908.06264.pdf', '1910.06036.pdf', '1904.05584.pdf', '2003.12738.pdf', '1909.06162.pdf', '1704.06194.pdf', '1809.04960.pdf', '1911.00069.pdf', '2001.05970.pdf']"}
{"_id": "paper_tab_15", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the eight biomedical NER tasks that were used to compare the performance of GreenBioBERT and BioBERT in the context of inexpensive domain adaptation?", "answer": "[{'answer': 'BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800', 'type': 'abstractive'}, {'answer': 'BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800', 'type': 'abstractive'}]", "main_doc": "2004.03354.pdf", "documents": "['2004.03354.pdf', '1903.09722.pdf', '2003.11645.pdf', '2002.01359.pdf', '1712.03547.pdf', '1908.05434.pdf', '1906.03538.pdf', '1908.10084.pdf', '1809.09795.pdf', '2002.04181.pdf', '2003.03106.pdf', '1908.06267.pdf', '1702.03342.pdf', '1711.00106.pdf']"}
{"_id": "paper_tab_16", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What specific metadata fields are provided, in addition to the claim and its label, in the example claim instance from the MultiFC dataset?", "answer": "[{'answer': 'besides claim, label and claim url, it also includes a claim ID, reason, category, speaker, checker, tags, claim entities, article title, publish data and claim date', 'type': 'abstractive'}]", "main_doc": "1909.03242.pdf", "documents": "['1909.03242.pdf', '1912.01772.pdf', '1905.10810.pdf', '2002.01207.pdf', '1611.04642.pdf', '1811.01088.pdf', '1906.01081.pdf', '1809.05752.pdf', '1704.08960.pdf']"}
{"_id": "paper_tab_17", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the NMI values between the clustered tone contours and the ground truth tone categories for all syllables in both Mandarin and Cantonese, as reported in your phonemic tone clustering analysis?", "answer": "[{'answer': 'NMI between cluster assignments and ground truth tones for all sylables is:\\nMandarin: 0.641\\nCantonese: 0.464', 'type': 'abstractive'}]", "main_doc": "1910.08987.pdf", "documents": "['1910.08987.pdf', '1903.09588.pdf', '1705.01265.pdf', '1910.00912.pdf', '1701.06538.pdf', '1912.10806.pdf', '1911.03597.pdf', '2003.11563.pdf', '1904.05584.pdf', '1910.12795.pdf', '2004.04721.pdf']"}
{"_id": "paper_tab_18", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the paper compare the execution time of their GDSConv-based model with other state-of-the-art keyword spotting models that use traditional acoustic feature extraction techniques?", "answer": "[{'answer': 'No', 'type': 'boolean'}]", "main_doc": "1911.02086.pdf", "documents": "['1911.02086.pdf', '1806.04330.pdf', '2003.03014.pdf', '2002.02492.pdf', '1911.08976.pdf', '1910.06592.pdf', '1811.02906.pdf', '1911.01680.pdf', '1912.01214.pdf', '1908.06264.pdf', '2002.06644.pdf', '1904.10500.pdf', '1710.09340.pdf']"}
{"_id": "paper_tab_19", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How much do the F1 scores improve when using the hybrid attention method and DCA compared to BiDAF and DCN on the SQuAD dataset?", "answer": "[{'answer': 'In terms of F1 score, the Hybrid approach improved by 23.47% and 1.39% on BiDAF and DCN respectively. The DCA approach improved by 23.2% and 1.12% on BiDAF and DCN respectively.', 'type': 'abstractive'}]", "main_doc": "1803.09230.pdf", "documents": "['1803.09230.pdf', '1809.00530.pdf', '1712.05999.pdf', '1910.00912.pdf', '1904.10503.pdf', '2002.06424.pdf', '1911.05153.pdf']"}
{"_id": "paper_tab_20", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the accuracy score of the highest-performing McM model on the 12-class bilingual SMS dataset?", "answer": "[{'answer': 'the best performing model obtained an accuracy of 0.86', 'type': 'abstractive'}]", "main_doc": "1911.13066.pdf", "documents": "['1911.13066.pdf', '1902.09666.pdf', '1812.06864.pdf', '1810.03459.pdf', '1701.09123.pdf', '1902.00330.pdf', '1910.08210.pdf', '1810.10254.pdf', '1804.08139.pdf', '1911.08962.pdf', '1908.08345.pdf']"}
{"_id": "paper_tab_21", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the 12 class labels and their distribution (as given in %) for the bilingual Roman Urdu-English SMS feedback dataset discussed in the study on McM deep learning model for SMS classification?", "answer": "[{'answer': 'Appreciation, Satisfied, Peripheral complaint, Demanded inquiry, Corruption, Lagged response, Unresponsive, Medicine payment, Adverse behavior, Grievance ascribed and Obnoxious/irrelevant', 'type': 'abstractive'}]", "main_doc": "1911.13066.pdf", "documents": "['1911.13066.pdf', '2001.10161.pdf', '2003.12218.pdf', '1809.09795.pdf', '1810.03459.pdf', '2002.02070.pdf', '1910.08210.pdf', '1906.10551.pdf', '1905.10810.pdf', '1804.05918.pdf', '1812.06705.pdf', '1912.03457.pdf', '2003.08385.pdf', '2001.08868.pdf', '1909.00361.pdf', '1911.08962.pdf']"}
{"_id": "paper_tab_22", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What metrics (precision, recall, F1) are reported for evaluating the NER system's performance in the CoNLL 2003 English results?", "answer": "[{'answer': 'Precision, Recall, F1', 'type': 'abstractive'}]", "main_doc": "1701.09123.pdf", "documents": "['1701.09123.pdf', '2003.01769.pdf', '1805.04033.pdf', '1711.11221.pdf']"}
{"_id": "paper_tab_23", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the datasets that were used for training, development, and out-of-domain evaluation in the multilingual named entity recognition experiments, including corpora such as MUC7, SONAR-1, and Ancora 2.0?", "answer": "[{'answer': 'CoNLL 2003, GermEval 2014, CoNLL 2002, Egunkaria, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0', 'type': 'abstractive'}]", "main_doc": "1701.09123.pdf", "documents": "['1701.09123.pdf', '1809.06537.pdf', '1707.05236.pdf', '1905.10810.pdf', '1904.05584.pdf', '2004.04721.pdf', '1812.10479.pdf', '1910.08210.pdf', '1911.02086.pdf', '1809.00540.pdf', '1803.09230.pdf', '1810.09774.pdf', '1701.05574.pdf', '1603.07044.pdf', '1808.09920.pdf']"}
{"_id": "paper_tab_24", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the total number of documents in the dataset in the paper on modeling stock movements using a recurrent state transition model influenced by news events?", "answer": "[{'answer': '553,451 documents', 'type': 'abstractive'}]", "main_doc": "2004.01878.pdf", "documents": "['2004.01878.pdf', '1909.00754.pdf', '1902.10525.pdf', '1707.05236.pdf', '1911.08962.pdf', '1911.02086.pdf', '1909.11687.pdf', '1605.07683.pdf', '1909.06162.pdf', '1809.06537.pdf', '2001.00137.pdf', '1812.10479.pdf', '2001.06286.pdf', '1909.03242.pdf', '1701.03214.pdf']"}
{"_id": "paper_tab_25", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which 12 languages, including both major and less-resourced ones, are listed summarizing the language datasets in the Multi-SimLex lexical resource?", "answer": "[{'answer': 'Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese', 'type': 'abstractive'}, {'answer': 'Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese', 'type': 'abstractive'}]", "main_doc": "2003.04866.pdf", "documents": "['2003.04866.pdf', '1911.01680.pdf', '1901.01010.pdf', '1903.00172.pdf', '1809.06537.pdf', '1603.07044.pdf', '1901.08079.pdf', '2001.05970.pdf']"}
{"_id": "paper_tab_26", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the exact EM and F1 score achieved by the proposed adversarial domain adaptation model to address ASR errors, as reported in the study on mitigating ASR errors in spoken question answering?", "answer": "[{'answer': 'Best results authors obtain is EM 51.10 and F1 63.11', 'type': 'abstractive'}, {'answer': 'EM Score of 51.10', 'type': 'abstractive'}]", "main_doc": "1904.07904.pdf", "documents": "['1904.07904.pdf', '2004.04721.pdf', '1910.06036.pdf', '2003.03106.pdf', '1704.05907.pdf', '2001.08868.pdf', '1603.00968.pdf', '1902.00672.pdf', '1903.09722.pdf', '2002.01664.pdf', '1804.11346.pdf']"}
{"_id": "paper_tab_27", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What baseline models are compared for pun detection and location in terms of precision, recall, and F1-score against the proposed approach?", "answer": "[{'answer': 'They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF.', 'type': 'abstractive'}]", "main_doc": "1909.00175.pdf", "documents": "['1909.00175.pdf', '1912.01772.pdf', '1805.03710.pdf', '1908.11365.pdf', '2002.01359.pdf', '1703.02507.pdf']"}
{"_id": "paper_tab_29", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What baseline models, including LSTM, HRED, VHRED (with and without attention), and others reported in prior work, are compared to the AvgOut-based models in terms of F1 score and dialogue diversity/relevance?", "answer": "[{'answer': 'LSTMs with and without attention, HRED, VHRED with and without attention, MMI and Reranking-RL', 'type': 'abstractive'}]", "main_doc": "2001.05467.pdf", "documents": "['2001.05467.pdf', '1610.00879.pdf', '1605.07683.pdf', '2004.01980.pdf', '1909.01013.pdf', '1603.04513.pdf', '1911.03597.pdf', '2001.00137.pdf', '1910.11235.pdf', '1907.09369.pdf', '1704.08960.pdf', '1605.08675.pdf', '1910.12129.pdf', '2002.00652.pdf', '2002.06424.pdf', '1912.01673.pdf']"}
{"_id": "paper_tab_30", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the reported improvement in both diversity and relevance scores for the MinAvgOut, LFT, RL, and hybrid models compared to the base LSTM-RNN?", "answer": "[{'answer': 'on diversity 6.87 and on relevance 4.6 points higher', 'type': 'abstractive'}]", "main_doc": "2001.05467.pdf", "documents": "['2001.05467.pdf', '2002.04181.pdf', '1704.06194.pdf', '1901.05280.pdf']"}
{"_id": "paper_tab_31", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the ROUGE-1, ROUGE-2, and ROUGE-L scores achieved by the best-performing abstractive model on the CNN/DailyMail, XSum, and NYT datasets, as reported in the \"Text Summarization with Pretrained Encoders\" paper?", "answer": "[{'answer': 'Best results on unigram:\\nCNN/Daily Mail: Rogue F1 43.85\\nNYT: Rogue Recall 49.02\\nXSum: Rogue F1 38.81', 'type': 'abstractive'}, {'answer': 'Highest scores for ROUGE-1, ROUGE-2 and ROUGE-L on CNN/DailyMail test set are 43.85, 20.34 and 39.90 respectively; on the XSum test set 38.81, 16.50 and 31.27 and on the NYT test set 49.02, 31.02 and 45.55', 'type': 'abstractive'}]", "main_doc": "1908.08345.pdf", "documents": "['1908.08345.pdf', '1908.11047.pdf', '1909.01247.pdf', '1809.02279.pdf', '2003.05377.pdf', '1911.01680.pdf', '1910.08987.pdf', '1910.00912.pdf', '1910.03467.pdf', '1708.09609.pdf', '2002.00652.pdf', '1909.03405.pdf', '1910.14537.pdf', '2001.06888.pdf', '2002.01359.pdf', '2004.03744.pdf', '1603.04513.pdf']"}
{"_id": "paper_tab_32", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which dataset, consisting of 1,312 error-annotated sentences with 30,144 tokens and using the M2 Scorer v3.2 for evaluation, did the authors employ for grammatical error correction in their neural network translation models study?", "answer": "[{'answer': 'CoNLL 2014', 'type': 'extractive'}]", "main_doc": "1606.00189.pdf", "documents": "['1606.00189.pdf', '1801.05147.pdf', '1904.10500.pdf', '1905.10810.pdf', '1809.01202.pdf', '1910.14497.pdf', '2001.08051.pdf', '1909.00512.pdf', '1910.08987.pdf']"}
{"_id": "paper_tab_33", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the joint model combining text and visual features, as described in the paper's experimental results, outperform text-only and visual-only models on both Wikipedia articles and arXiv subsets?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'No', 'type': 'boolean'}]", "main_doc": "1901.01010.pdf", "documents": "['1901.01010.pdf', '2002.06424.pdf', '1903.09722.pdf', '1809.01541.pdf', '2001.10161.pdf', '1909.01383.pdf', '1804.11346.pdf', '1812.06864.pdf', '1612.08205.pdf', '1606.00189.pdf']"}
{"_id": "paper_tab_34", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the exact accuracy results on the Wikipedia dataset, as well as the peer-reviewed Archive AI, Archive Computation and Language, and Archive Machine Learning papers, as reported in the document quality assessment paper?", "answer": "[{'answer': '59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers', 'type': 'abstractive'}]", "main_doc": "1901.01010.pdf", "documents": "['1901.01010.pdf', '1901.03866.pdf', '1909.00361.pdf', '1912.01214.pdf', '1705.08142.pdf', '1811.01088.pdf', '1911.07228.pdf', '1911.01680.pdf', '1603.00968.pdf', '1906.03538.pdf', '2003.03014.pdf', '1612.08205.pdf']"}
{"_id": "paper_tab_35", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which datasets are used to report the link prediction results for the Embedded Knowledge Graph Network (EKGN) model in the \"Link Prediction using Embedded Knowledge Graphs\" paper?", "answer": "[{'answer': 'WN18 and FB15k', 'type': 'abstractive'}]", "main_doc": "1611.04642.pdf", "documents": "['1611.04642.pdf', '2004.01980.pdf', '1603.00968.pdf', '1911.07555.pdf', '1910.04269.pdf', '1611.00514.pdf', '2003.11645.pdf', '1910.03467.pdf', '1910.11235.pdf', '1910.11204.pdf', '1808.03430.pdf']"}
{"_id": "paper_tab_36", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the analysis of syntactic patterns in WordNet's glosses, how many distinct semantic roles are introduced?", "answer": "[{'answer': '12', 'type': 'abstractive'}]", "main_doc": "1806.07711.pdf", "documents": "['1806.07711.pdf', '1901.09755.pdf', '2003.07996.pdf', '1809.08298.pdf', '1911.01680.pdf', '1702.03342.pdf', '1909.00694.pdf', '1810.05241.pdf', '1909.06162.pdf', '1804.05918.pdf', '1910.06036.pdf', '2003.11563.pdf', '1912.00864.pdf', '1810.12085.pdf', '1810.12196.pdf', '1911.00069.pdf', '1909.05855.pdf']"}
{"_id": "paper_tab_37", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What specific HPI categories are outlined under the Annotation Instructions for the LSTM model's task of extractive summarization in this study?", "answer": "[{'answer': 'Demographics Age, DiagnosisHistory, MedicationHistory, ProcedureHistory, Symptoms/Signs, Vitals/Labs, Procedures/Results, Meds/Treatments, Movement, Other.', 'type': 'abstractive'}, {'answer': 'Demographics, Diagnosis History, Medication History, Procedure History, Symptoms, Labs, Procedures, Treatments, Hospital movements, and others', 'type': 'abstractive'}]", "main_doc": "1810.12085.pdf", "documents": "['1810.12085.pdf', '1901.03866.pdf', '1909.08041.pdf', '1902.09314.pdf', '1909.01383.pdf', '1902.00672.pdf', '1912.00864.pdf', '2003.04642.pdf', '1608.06757.pdf', '1804.08139.pdf']"}
{"_id": "paper_tab_38", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Did the evaluation of the LSTM model on the 515 annotated history of present illness notes include a comparison with other existing extractive summarization techniques?", "answer": "[{'answer': 'No', 'type': 'boolean'}]", "main_doc": "1810.12085.pdf", "documents": "['1810.12085.pdf', '2001.08868.pdf', '1709.10217.pdf', '1909.00105.pdf', '1909.00175.pdf', '1909.01013.pdf', '2003.04866.pdf', '1901.02257.pdf', '1603.07044.pdf', '1707.08559.pdf', '1911.01680.pdf', '1811.12254.pdf', '1910.14497.pdf', '1901.04899.pdf']"}
{"_id": "paper_tab_39", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "According to previous methods for POS tagging and lemmatization in Czech text processing, which were outperformed by the BERT and Flair-based approaches?", "answer": "[{'answer': 'Table TABREF44, Table TABREF44, Table TABREF47, Table TABREF47', 'type': 'extractive'}]", "main_doc": "1909.03544.pdf", "documents": "['1909.03544.pdf', '1910.06036.pdf', '1909.00105.pdf', '1912.10435.pdf', '1809.04960.pdf', '1909.11467.pdf']"}
{"_id": "paper_tab_40", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "According to the ROUGE metrics, which baseline models are outperformed by the proposed regularization approach in the context of abstractive Chinese social media text summarization?", "answer": "[{'answer': 'RNN-context, SRB, CopyNet, RNN-distract, DRGD', 'type': 'abstractive'}]", "main_doc": "1805.04033.pdf", "documents": "['1805.04033.pdf', '1603.07044.pdf', '1910.12795.pdf', '1902.10525.pdf', '1904.05584.pdf', '1909.06162.pdf', '1711.11221.pdf', '1809.10644.pdf', '2003.06044.pdf', '1909.03242.pdf', '1912.01673.pdf', '1909.09484.pdf', '1606.00189.pdf', '1809.01541.pdf']"}
{"_id": "paper_tab_41", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How did participants in the evaluations on interactive fiction generation rate the neural model's performance in terms of coherence, genre resemblance, and overall quality compared to rule-based methods and human-made games across both mystery and fairy-tale genres?", "answer": "[{'answer': 'the neural approach is generally preferred by a greater percentage of participants than the rules or random, human-made game outperforms them all', 'type': 'extractive'}]", "main_doc": "2001.10161.pdf", "documents": "['2001.10161.pdf', '1801.05147.pdf', '1611.02550.pdf', '1909.00578.pdf', '1810.03459.pdf', '1604.00400.pdf', '1709.10367.pdf', '2004.01980.pdf', '1911.00069.pdf', '2002.11402.pdf', '1707.03569.pdf', '2002.02070.pdf']"}
{"_id": "paper_tab_42", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What specific evaluation metrics, including unlabeled INLINEFORM0 scores, are reported for assessing the performance of your compound probabilistic context-free grammar model compared to baseline models in unsupervised parsing experiments on English and Chinese?", "answer": "[{'answer': 'INLINEFORM0 scores', 'type': 'extractive'}, {'answer': 'Unlabeled sentence-level F1, perplexity, grammatically judgment performance', 'type': 'abstractive'}]", "main_doc": "1906.10225.pdf", "documents": "['1906.10225.pdf', '1908.05828.pdf', '1704.00939.pdf', '1912.13109.pdf', '1909.13714.pdf', '1611.00514.pdf', '1912.01772.pdf', '1910.06592.pdf', '1810.12085.pdf', '1804.08050.pdf', '2001.10161.pdf', '1911.03597.pdf', '2004.01980.pdf', '1805.04033.pdf', '1909.00694.pdf', '1904.09678.pdf']"}
{"_id": "paper_tab_43", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the precision scores for semantic typing and entity matching as reported in the paper, specifically on the S-Lite and R-Lite datasets compared to the baselines?", "answer": "[{'answer': '0.8320 on semantic typing, 0.7194 on entity matching', 'type': 'abstractive'}]", "main_doc": "1906.11180.pdf", "documents": "['1906.11180.pdf', '2004.01980.pdf', '1909.08824.pdf', '2003.04866.pdf', '1910.08210.pdf']"}
{"_id": "paper_tab_44", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the paper summarize any transformer-based models such as BERT or GPT for sentence pair modeling?", "answer": "[{'answer': 'No', 'type': 'boolean'}, {'answer': 'No', 'type': 'boolean'}]", "main_doc": "1806.04330.pdf", "documents": "['1806.04330.pdf', '1902.10525.pdf', '1901.02262.pdf', '1910.04269.pdf', '2003.07996.pdf', '2003.06044.pdf', '1706.08032.pdf']"}
{"_id": "paper_tab_45", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How are non-standard pronunciations and mispronunciations annotated in the transcriptions of the 142-hour Mapudungun medical conversations corpus provided for computational linguistic experiments?", "answer": "[{'answer': 'Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation.', 'type': 'abstractive'}]", "main_doc": "1912.01772.pdf", "documents": "['1912.01772.pdf', '1812.06705.pdf', '1909.00754.pdf', '1810.12885.pdf', '2002.05829.pdf', '1701.02877.pdf', '1904.10503.pdf', '1912.03457.pdf', '1603.04513.pdf', '1901.05280.pdf', '1905.00563.pdf', '1912.08960.pdf', '2002.06675.pdf', '2004.03788.pdf']"}
{"_id": "paper_tab_46", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which NER systems were evaluated with micro-precision, recall, and F1 scores on datasets like CoNLL2003, KORE50, ACE2004, and MSNBC in the 'Robust Named Entity Recognition in Idiosyncratic Domains' paper?", "answer": "[{'answer': 'Babelfy, DBpedia Spotlight, Entityclassifier.eu, FOX, LingPipe MUC-7, NERD-ML, Stanford NER, TagMe 2', 'type': 'abstractive'}]", "main_doc": "1608.06757.pdf", "documents": "['1608.06757.pdf', '1908.07816.pdf', '1704.06194.pdf', '2004.01980.pdf', '1903.09588.pdf', '1704.05907.pdf', '1910.12129.pdf', '1701.02877.pdf', '1812.06864.pdf']"}
{"_id": "paper_tab_47", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What recurrent neural network models are benchmarked against the Pyramidal Recurrent Unit to evaluate its performance on word-level language modeling?", "answer": "[{'answer': 'Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM', 'type': 'abstractive'}]", "main_doc": "1808.09029.pdf", "documents": "['1808.09029.pdf', '1908.11546.pdf', '1905.12260.pdf', '1906.05474.pdf', '2001.05493.pdf', '1912.01772.pdf', '1704.06194.pdf', '1604.00400.pdf', '1910.07481.pdf', '1909.00754.pdf', '1909.08859.pdf']"}
{"_id": "paper_tab_48", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Was the baseline Transformer-based sequence-to-sequence model evaluated on the newly introduced video game data-to-text corpus, and what were its performance metrics, including BLEU, METEOR, ROUGE, CIDEr, and SER?", "answer": "[{'answer': 'Yes, Transformer based seq2seq is evaluated with average BLEU 0.519, METEOR 0.388, ROUGE 0.631 CIDEr 2.531 and SER 2.55%.', 'type': 'abstractive'}]", "main_doc": "1910.12129.pdf", "documents": "['1910.12129.pdf', '1908.06264.pdf', '1909.00361.pdf', '2002.04181.pdf', '2002.06644.pdf', '1812.10479.pdf', '1603.00968.pdf', '1911.04952.pdf', '1910.10288.pdf', '1804.07789.pdf', '1912.01772.pdf', '1809.02279.pdf', '1904.09678.pdf']"}
{"_id": "paper_tab_49", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What linguistic features are specified in the biLSTM-based Arabic diacritic recovery model for case ending reconstruction?", "answer": "[{'answer': 'POS, gender/number and stem POS', 'type': 'abstractive'}]", "main_doc": "2002.01207.pdf", "documents": "['2002.01207.pdf', '1909.06162.pdf', '1911.10049.pdf', '1911.07228.pdf', '1810.05241.pdf']"}
{"_id": "paper_tab_50", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How much lower is the log likelihood for LSTMs compared to HMMs across datasets, both when they have similar numbers of parameters and when the LSTM has an increased number of parameters?", "answer": "[{'answer': 'With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower.', 'type': 'abstractive'}]", "main_doc": "1606.05320.pdf", "documents": "['1606.05320.pdf', '2004.03744.pdf', '1810.12085.pdf', '1911.04952.pdf', '1909.00279.pdf', '1908.05434.pdf', '1903.09588.pdf', '2004.03354.pdf']"}
{"_id": "paper_tab_51", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How do the perplexity and BLEU scores of the best-performing model, using +Anti OT and +Anti UT modifications, compare to the baseline unsupervised machine translation (UMT) model?", "answer": "[{'answer': 'Perplexity of the best model is 65.58 compared to best baseline 105.79.\\nBleu of the best model is 6.57 compared to best baseline 5.50.', 'type': 'abstractive'}]", "main_doc": "1909.00279.pdf", "documents": "['1909.00279.pdf', '1904.05584.pdf', '1911.10049.pdf', '1809.00540.pdf', '1909.03135.pdf', '1709.10367.pdf', '1701.09123.pdf', '1910.08210.pdf', '1806.07711.pdf', '1908.06267.pdf', '1909.00430.pdf', '1910.12795.pdf', '1910.04269.pdf', '1904.09678.pdf', '1909.08041.pdf', '1712.03556.pdf', '1912.01673.pdf', '1909.04002.pdf']"}
{"_id": "paper_tab_52", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the total number of samples in the augmented dataset combining AD-specific picture descriptions with multi-task healthy speech data, as detailed in the study on heterogeneous data for Alzheimer's disease detection?", "answer": "[{'answer': '609', 'type': 'abstractive'}]", "main_doc": "1811.12254.pdf", "documents": "['1811.12254.pdf', '1911.05153.pdf', '1909.02480.pdf', '1910.11204.pdf', '1909.04002.pdf', '2001.10161.pdf', '1906.03538.pdf', '1905.11901.pdf']"}
{"_id": "paper_tab_53", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the table, which presents LID accuracy results for South African languages, include the performance of a baseline model?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]", "main_doc": "1911.07555.pdf", "documents": "['1911.07555.pdf', '1809.09795.pdf', '2002.10361.pdf', '1909.01247.pdf', '1910.02339.pdf', '1809.02286.pdf', '2004.01980.pdf', '1901.04899.pdf']"}
{"_id": "paper_tab_54", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Did the results demonstrate that your hierarchical naive Bayesian and lexicon-based classifier achieved the highest accuracy across the South African and DSL datasets, outperforming other reported methods?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'From all reported results proposed method (NB+Lex) shows best accuracy on all 3 datasets - some models are not evaluated and not available in literature.', 'type': 'abstractive'}]", "main_doc": "1911.07555.pdf", "documents": "['1911.07555.pdf', '1912.01772.pdf', '1703.06492.pdf', '2001.05970.pdf', '1908.11047.pdf', '1706.08032.pdf', '1910.06036.pdf', '1806.04330.pdf', '2002.06644.pdf', '1909.11467.pdf', '2003.11645.pdf', '1909.00578.pdf', '1903.09722.pdf', '1905.11901.pdf', '1710.09340.pdf']"}
{"_id": "paper_tab_55", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How do the MOS naturalness scores, with 95% confidence intervals, compare for DCA and GMM-based attention mechanisms across the Lessac and LJ datasets, and do these results indicate a performance difference?", "answer": "[{'answer': 'About the same performance', 'type': 'abstractive'}]", "main_doc": "1910.10288.pdf", "documents": "['1910.10288.pdf', '2002.04181.pdf', '2003.04642.pdf', '2002.11402.pdf', '2003.12218.pdf', '1809.10644.pdf', '1910.05154.pdf', '1903.09722.pdf', '1909.08859.pdf', '1809.00540.pdf', '1611.03382.pdf', '1908.05434.pdf', '1909.01013.pdf', '2002.10361.pdf']"}
{"_id": "paper_tab_56", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the F0.5 scores on the FCE dataset and both annotations of CoNLL-14 when combining machine translation-based and syntactic pattern-based error generation?", "answer": "[{'answer': 'Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. ', 'type': 'abstractive'}]", "main_doc": "1707.05236.pdf", "documents": "['1707.05236.pdf', '1910.06748.pdf', '1908.11365.pdf', '1809.10644.pdf', '1805.03710.pdf', '1611.00514.pdf', '1911.02821.pdf', '1906.01081.pdf', '1909.11297.pdf', '1904.01608.pdf', '1908.11047.pdf', '2001.08051.pdf', '1701.09123.pdf']"}
{"_id": "paper_tab_57", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What F1 scores did the CM-Net achieve for intent detection and slot filling on the CAIS dataset?", "answer": "[{'answer': 'F1 scores of 86.16 on slot filling and 94.56 on intent detection', 'type': 'abstractive'}]", "main_doc": "1909.06937.pdf", "documents": "['1909.06937.pdf', '1808.03430.pdf', '1606.05320.pdf', '1909.08089.pdf', '1911.01680.pdf', '1909.02480.pdf', '1603.00968.pdf', '1904.10503.pdf', '2003.07996.pdf', '1912.01214.pdf', '1910.03814.pdf', '1701.05574.pdf']"}
{"_id": "paper_tab_58", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How many utterances are present in the CAIS dataset as specified in the study on Collaborative Memory Networks (CM-Net)?", "answer": "[{'answer': '10,001 utterances', 'type': 'abstractive'}]", "main_doc": "1909.06937.pdf", "documents": "['1909.06937.pdf', '1911.11951.pdf', '1707.00110.pdf', '1912.03457.pdf', '1902.09393.pdf']"}
{"_id": "paper_tab_59", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the improvement in average precision of their best-performing model over previous results on the word discrimination task?", "answer": "[{'answer': 'Their best average precision tops previous best result by 0.202', 'type': 'abstractive'}]", "main_doc": "1611.02550.pdf", "documents": "['1611.02550.pdf', '1804.08139.pdf', '1908.11546.pdf', '1804.07789.pdf', '2002.06424.pdf', '1702.03342.pdf', '2003.07723.pdf', '2002.01664.pdf', '1809.01202.pdf', '2001.06888.pdf', '1901.01010.pdf', '1910.06592.pdf', '1810.00663.pdf']"}
{"_id": "paper_tab_60", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the comparative performance of the proposed global greedy parser versus BIAF and STACKPTR in terms of both training convergence time and decoding speed?", "answer": "[{'answer': 'Proposed vs best baseline:\\nDecoding: 8541 vs 8532 tokens/sec\\nTraining: 8h vs 8h', 'type': 'abstractive'}]", "main_doc": "1911.08673.pdf", "documents": "['1911.08673.pdf', '1909.00578.pdf', '1902.09314.pdf', '2004.03354.pdf', '1912.10011.pdf', '1904.07904.pdf', '1904.05584.pdf', '1901.08079.pdf', '1709.10217.pdf', '1902.09393.pdf', '1707.00110.pdf', '2004.03788.pdf']"}
{"_id": "paper_tab_61", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What external datasets are provided for pretraining the neural word segmentation model in this study?", "answer": "[{'answer': \"Raw data from Gigaword, Automatically segmented text from Gigaword, Heterogenous training data from People's Daily, POS data from People's Daily\", 'type': 'abstractive'}]", "main_doc": "1704.08960.pdf", "documents": "['1704.08960.pdf', '1910.13215.pdf', '2003.11645.pdf', '2002.06424.pdf', '2001.06286.pdf', '1804.07789.pdf', '1608.06757.pdf', '2003.04866.pdf', '2004.04721.pdf', '1910.06592.pdf', '1904.10500.pdf', '1910.06748.pdf', '1908.06083.pdf']"}
{"_id": "paper_tab_62", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the precision, recall, and F1-score values of the best-performing teams in the sentence-level classification (SLC) and fragment-level classification (FLC) tasks as reported in the MIC-CIS system's performance comparison?", "answer": "[{'answer': 'For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively).', 'type': 'abstractive'}]", "main_doc": "1909.06162.pdf", "documents": "['1909.06162.pdf', '1612.08205.pdf', '1912.08960.pdf', '1809.09795.pdf', '1812.10479.pdf', '1901.08079.pdf', '1901.01010.pdf', '2002.11910.pdf', '1902.00672.pdf', '1910.14537.pdf', '1909.08824.pdf']"}
{"_id": "paper_tab_63", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What extracted linguistic features, combined with BERT, had the most significant influence on performance in the sentence-level classification (SLC) task in the MIC-CIS system's evaluation for fine-grained propaganda detection?", "answer": "[{'answer': 'Linguistic', 'type': 'abstractive'}, {'answer': 'BERT', 'type': 'extractive'}]", "main_doc": "1909.06162.pdf", "documents": "['1909.06162.pdf', '1901.02257.pdf', '1807.07961.pdf', '1902.09393.pdf', '1809.05752.pdf', '1912.03457.pdf', '1705.08142.pdf']"}
{"_id": "paper_tab_65", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which neural architecture achieved the highest performance for sentence-level (SLC) propaganda detection on the internal Fold1 and external Dev sets?", "answer": "[{'answer': 'BERT', 'type': 'abstractive'}]", "main_doc": "1909.06162.pdf", "documents": "['1909.06162.pdf', '1809.06537.pdf', '2002.00652.pdf', '1910.02339.pdf', '1909.00578.pdf', '2003.08385.pdf', '1812.01704.pdf', '2001.06888.pdf', '1910.12795.pdf']"}
{"_id": "paper_tab_66", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which participating teams achieved higher scores than the MIC-CIS system in both the SLC and FLC tasks?", "answer": "[{'answer': 'For SLC task : Ituorp, ProperGander and YMJA  teams had better results.\\nFor FLC task: newspeak and Antiganda teams had better results.', 'type': 'abstractive'}]", "main_doc": "1909.06162.pdf", "documents": "['1909.06162.pdf', '2002.06675.pdf', '1909.09484.pdf', '2002.11402.pdf', '2003.03044.pdf', '1701.05574.pdf', '1807.07961.pdf', '1908.05828.pdf', '2004.01980.pdf', '1911.02711.pdf', '1906.10225.pdf', '1809.00530.pdf', '1812.06705.pdf']"}
{"_id": "paper_tab_67", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which datasets, including any non-English ones, are discussed in the performance comparison of models for generating one-line biographical descriptions from structured tables?", "answer": "[{'answer': 'English WIKIBIO, French WIKIBIO , German WIKIBIO ', 'type': 'abstractive'}, {'answer': 'WikiBio dataset,  introduce two new biography datasets, one in French and one in German', 'type': 'extractive'}]", "main_doc": "1804.07789.pdf", "documents": "['1804.07789.pdf', '2002.04181.pdf', '2004.04721.pdf', '1908.07245.pdf', '1809.02286.pdf', '1909.00694.pdf']"}
{"_id": "paper_tab_68", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which classifier achieved the highest F1 micro and F1 macro scores during 4-fold cross-validation on the shuffled data set, and what explanation is given for its superior performance?", "answer": "[{'answer': 'Table TABREF10,  The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set,  While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject', 'type': 'extractive'}, {'answer': 'Using F1 Micro measure, the KNN classifier perform 0.6762, the RF 0.6687, SVM 0.6712 and MLP 0.6778.', 'type': 'abstractive'}]", "main_doc": "2002.02070.pdf", "documents": "['2002.02070.pdf', '1701.03214.pdf', '1805.04033.pdf', '1912.00864.pdf', '1709.05413.pdf', '1910.00912.pdf', '1904.10503.pdf', '1908.07245.pdf', '1912.01673.pdf', '1809.01202.pdf', '1707.05236.pdf', '1809.04960.pdf', '1911.03310.pdf', '1901.02262.pdf', '1809.06537.pdf', '1804.08050.pdf', '1906.11180.pdf', '1701.00185.pdf']"}
{"_id": "paper_tab_69", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the study, which machine learning classifiers were evaluated on classifying dealership interaction data?", "answer": "[{'answer': 'KNN\\nRF\\nSVM\\nMLP', 'type': 'abstractive'}, {'answer': ' K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)', 'type': 'extractive'}]", "main_doc": "2002.02070.pdf", "documents": "['2002.02070.pdf', '1909.00430.pdf', '1707.08559.pdf', '1711.11221.pdf', '1711.00106.pdf', '1608.06757.pdf']"}
{"_id": "paper_tab_70", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the RAFAEL system paper provide a comparative analysis of question-answering accuracy between DeepER, traditional NER methods (like Nerf and Liner2), and hybrid entity recognition strategies?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}]", "main_doc": "1605.08675.pdf", "documents": "['1605.08675.pdf', '1908.05434.pdf', '1910.14537.pdf', '1705.08142.pdf']"}
{"_id": "paper_tab_71", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What models are evaluated in the paper's experimental results when comparing the extended Stochastic Answer Network (SAN) to baseline and state-of-the-art methods on SQuAD 2.0?", "answer": "[{'answer': 'SAN Baseline, BNA, DocQA, R.M-Reader, R.M-Reader+Verifier and DocQA+ELMo', 'type': 'abstractive'}, {'answer': 'BNA, DocQA, R.M-Reader, R.M-Reader + Verifier, DocQA + ELMo, R.M-Reader+Verifier+ELMo', 'type': 'abstractive'}]", "main_doc": "1809.09194.pdf", "documents": "['1809.09194.pdf', '1901.03866.pdf', '1704.06194.pdf', '1711.11221.pdf', '1909.09484.pdf', '1608.06757.pdf', '1909.03405.pdf', '1910.13215.pdf', '1911.07228.pdf', '1904.07904.pdf', '1901.01010.pdf', '1911.13066.pdf', '1910.07481.pdf', '1908.07195.pdf', '1809.10644.pdf', '2002.06644.pdf', '1701.05574.pdf']"}
{"_id": "paper_tab_72", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the F1 score improvement of the BERT-pair models, specifically BERT-pair-NLI-B for aspect category detection and BERT-pair-QA-B for aspect category polarity (in 4-way, 3-way, and binary settings) on the SemEval-2014 dataset, compared to prior state-of-the-art results?", "answer": "[{'answer': 'On subtask 3 best proposed model has F1 score of 92.18 compared to best previous F1 score of 88.58.\\nOn subtask 4 best proposed model has 85.9, 89.9 and 95.6 compared to best previous results of 82.9, 84.0 and 89.9 on 4-way, 3-way and binary aspect polarity.', 'type': 'abstractive'}]", "main_doc": "1903.09588.pdf", "documents": "['1903.09588.pdf', '1609.00559.pdf', '1701.02877.pdf', '1909.08824.pdf']"}
{"_id": "paper_tab_73", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the BLEU score differences between the baseline Transformer, 1.5-entmax, and \u03b1-entmax methods on the DE-EN, JA-EN, RO-EN, and EN-DE datasets as reported in the adaptively sparse Transformer paper?", "answer": "[{'answer': 'On the datasets DE-EN, JA-EN, RO-EN, and EN-DE, the baseline achieves 29.79, 21.57, 32.70, and 26.02  BLEU score, respectively. The 1.5-entmax achieves  29.83, 22.13, 33.10, and 25.89 BLEU score, which is a difference of +0.04, +0.56, +0.40, and -0.13 BLEU score versus the baseline. The \u03b1-entmax achieves 29.90, 21.74, 32.89, and 26.93 BLEU score, which is a difference of +0.11, +0.17, +0.19, +0.91 BLEU score versus the baseline.', 'type': 'abstractive'}]", "main_doc": "1909.00015.pdf", "documents": "['1909.00015.pdf', '1901.05280.pdf', '1810.12196.pdf', '1909.00252.pdf', '1909.00361.pdf', '1609.00559.pdf', '1802.06024.pdf', '1909.00694.pdf', '1910.00458.pdf', '1910.13215.pdf', '1909.00105.pdf', '1904.09678.pdf', '1807.07961.pdf', '2002.06424.pdf', '1908.06083.pdf', '1910.10288.pdf']"}
{"_id": "paper_tab_74", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What were the meta-data features, such as Followers, Friends, and URLs, that showed significant differences in distribution between fake news and non-fake news tweets, according to the Kolmogorov-Smirnov test results?", "answer": "[{'answer': 'Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different', 'type': 'abstractive'}]", "main_doc": "1712.05999.pdf", "documents": "['1712.05999.pdf', '1909.00105.pdf', '1810.09774.pdf', '1707.08559.pdf', '1704.05907.pdf', '1912.10435.pdf', '1909.00754.pdf', '1910.11204.pdf', '1909.13714.pdf']"}
{"_id": "paper_tab_75", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the final win rates of the proposed txt2$\\pi$, CNN, and FiLM models on the evaluation set with new environment dynamics for the simplest RTFM variant?", "answer": "[{'answer': 'Proposed model achive 66+-22 win rate, baseline CNN 13+-1  and baseline FiLM 32+-3 .', 'type': 'abstractive'}]", "main_doc": "1910.08210.pdf", "documents": "['1910.08210.pdf', '1909.13375.pdf', '1705.01214.pdf', '2001.08868.pdf', '2001.10161.pdf', '2002.06424.pdf', '1911.02821.pdf', '1910.11769.pdf', '1808.03430.pdf', '1911.08673.pdf', '2004.03788.pdf', '1910.06748.pdf', '1912.01772.pdf', '1903.09588.pdf', '1909.09484.pdf', '1909.08859.pdf']"}
{"_id": "paper_tab_76", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What tasks were the models evaluated on in ReviewQA's test set, specifically relating to aspect detection, rating predictions, and opinion analysis?", "answer": "[{'answer': \"ReviewQA's test set\", 'type': 'extractive'}, {'answer': 'Detection of an aspect in a review, Prediction of the customer general satisfaction, Prediction of the global trend of an aspect in a given review, Prediction of whether the rating of a given aspect is above or under a given value, Prediction of the exact rating of an aspect in a review, Prediction of the list of all the positive/negative aspects mentioned in the review, Comparison between aspects, Prediction of the strengths and weaknesses in a review', 'type': 'abstractive'}]", "main_doc": "1810.12196.pdf", "documents": "['1810.12196.pdf', '1901.05280.pdf', '1909.01247.pdf', '1908.06264.pdf', '1909.00430.pdf', '1910.03814.pdf', '1603.07044.pdf', '1910.02339.pdf', '2002.06424.pdf']"}
{"_id": "paper_tab_77", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the test set perplexity of the proposed MEED model, and how does it compare to the next-best baseline?", "answer": "[{'answer': 'Perplexity of proposed MEED model is 19.795 vs 19.913 of next best result on test set.', 'type': 'abstractive'}]", "main_doc": "1908.07816.pdf", "documents": "['1908.07816.pdf', '1811.01088.pdf', '1809.02286.pdf', '1605.08675.pdf', '2002.01359.pdf', '1810.06743.pdf', '1909.00252.pdf', '1611.03382.pdf', '1701.03214.pdf', '1912.10011.pdf', '1804.08050.pdf', '1811.12254.pdf', '1704.05907.pdf', '1705.01214.pdf']"}
{"_id": "paper_tab_78", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which baseline models, including tree-structured models and non-tree models, were used for comparison when evaluating the performance of the SATA Tree-LSTM on sentence classification tasks?", "answer": "[{'answer': 'Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks', 'type': 'abstractive'}, {'answer': 'Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \\nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018).', 'type': 'abstractive'}]", "main_doc": "1809.02286.pdf", "documents": "['1809.02286.pdf', '1911.08962.pdf', '1910.11769.pdf', '1904.09678.pdf', '1901.01010.pdf', '1901.05280.pdf', '1603.07044.pdf', '1910.11204.pdf', '1808.09029.pdf', '1608.06757.pdf', '1904.05584.pdf', '1605.08675.pdf']"}
{"_id": "paper_tab_79", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which NER datasets are used to analyze model generalization in diverse domains such as newswire, broadcast conversation, Web content, and social media?", "answer": "[{'answer': 'MUC, CoNLL, ACE, OntoNotes, MSM, Ritter, UMBC', 'type': 'abstractive'}]", "main_doc": "1701.02877.pdf", "documents": "['1701.02877.pdf', '1909.03135.pdf', '1910.10288.pdf', '2003.11563.pdf', '1910.06748.pdf', '1909.00361.pdf', '1911.12579.pdf', '2002.01359.pdf', '1712.00991.pdf', '1909.01247.pdf', '1703.02507.pdf', '1909.13714.pdf', '2002.06424.pdf', '1701.00185.pdf', '1604.00400.pdf', '1612.08205.pdf']"}
{"_id": "paper_tab_80", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the language labels included in the Twitter corpus distribution in the paper's language identification system evaluation?", "answer": "[{'answer': 'EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO', 'type': 'abstractive'}]", "main_doc": "1910.06748.pdf", "documents": "['1910.06748.pdf', '1911.01680.pdf', '2003.08385.pdf', '1911.02711.pdf']"}
{"_id": "paper_tab_81", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "According to the study on domain adaptation for identifying products in online cybercrime marketplaces, which four cybercrime forums are analyzed to demonstrate within-forum and cross-forum performance?", "answer": "[{'answer': 'Darkode,  Hack Forums, Blackhat and Nulled.', 'type': 'abstractive'}]", "main_doc": "1708.09609.pdf", "documents": "['1708.09609.pdf', '1802.06024.pdf', '1911.08976.pdf', '1909.00694.pdf', '2002.10361.pdf', '1905.00563.pdf', '2003.05377.pdf', '1909.00512.pdf', '1909.13375.pdf', '2002.11402.pdf', '1905.11901.pdf', '1908.06083.pdf', '2003.03106.pdf']"}
{"_id": "paper_tab_82", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which baseline models, including both neural classifiers and off-the-shelf RC models, are re-implemented and fine-tuned for comparison in the experiments conducted on the real-world civil case dataset in this paper?", "answer": "[{'answer': 'SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ', 'type': 'extractive'}, {'answer': 'SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard', 'type': 'extractive'}]", "main_doc": "1809.06537.pdf", "documents": "['1809.06537.pdf', '1909.08089.pdf', '1908.06267.pdf', '1812.06864.pdf', '1703.06492.pdf', '1806.11432.pdf', '1902.09393.pdf', '1810.00663.pdf', '2003.04642.pdf', '2003.03014.pdf']"}
{"_id": "paper_tab_83", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the benchmark results of the DENS paper, which algorithm and model combination achieved the lowest average micro-F1 score across the 5-fold cross-validation for the emotion analysis task?", "answer": "[{'answer': 'Depeche + SVM', 'type': 'extractive'}]", "main_doc": "1910.11769.pdf", "documents": "['1910.11769.pdf', '1905.00563.pdf', '2003.04642.pdf', '1707.05236.pdf', '2003.06044.pdf']"}
{"_id": "paper_tab_84", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Do Sluice networks outperform single-task models and hard parameter sharing in both in-domain and out-of-domain settings for chunking with POS tagging as an auxiliary task in the OntoNotes 5.0 experiments?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}]", "main_doc": "1705.08142.pdf", "documents": "['1705.08142.pdf', '1911.02086.pdf', '1909.00279.pdf', '1911.01799.pdf', '1901.02262.pdf', '1903.00172.pdf', '1809.00540.pdf', '1704.05907.pdf', '1710.09340.pdf', '1908.10084.pdf', '1805.04033.pdf', '1608.06757.pdf', '1901.02257.pdf', '1706.08032.pdf']"}
{"_id": "paper_tab_85", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the paper compare the performance of models trained with STILTs to BERT and ELMo on the same tasks from the GLUE benchmark?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}]", "main_doc": "1811.01088.pdf", "documents": "['1811.01088.pdf', '1904.10503.pdf', '1912.00864.pdf', '1909.00754.pdf']"}
{"_id": "paper_tab_86", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the total number of documents in the multilingual Twitter corpus for hate speech detection, covering English, Italian, Polish, Portuguese, and Spanish datasets?", "answer": "[{'answer': 'It contains 106,350 documents', 'type': 'abstractive'}]", "main_doc": "2002.10361.pdf", "documents": "['2002.10361.pdf', '1810.09774.pdf', '1908.11047.pdf', '1902.09393.pdf']"}
{"_id": "paper_tab_87", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which hyperparameters, including the number of clusters, word embedding dimensions, and others, were adjusted in the systematic evaluation of clustered word embeddings across named entity recognition and sentiment classification tasks, as described in the paper?", "answer": "[{'answer': 'number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding', 'type': 'abstractive'}, {'answer': 'different number of clusters, different embeddings', 'type': 'extractive'}]", "main_doc": "1705.01265.pdf", "documents": "['1705.01265.pdf', '1809.08298.pdf', '2003.04642.pdf', '1911.02821.pdf', '1906.11180.pdf', '1603.00968.pdf', '1705.00108.pdf']"}
{"_id": "paper_tab_88", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What specific psycholinguistic and basic linguistic features, including punctuation and emoticons, are utilized as meta-data to support aggression detection in both code-mixed and uni-lingual English texts as described in this paper's unified system?", "answer": "[{'answer': 'Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, TF-IDF Emoticon features', 'type': 'abstractive'}]", "main_doc": "2001.05493.pdf", "documents": "['2001.05493.pdf', '1904.10500.pdf', '1908.10084.pdf', '1910.14497.pdf', '1809.01202.pdf', '1801.05147.pdf']"}
{"_id": "paper_tab_89", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How does the Procedural Reasoning Network (PRN) model perform in terms of accuracy in single-task and multi-task training on the visual reasoning tasks in the RecipeQA dataset compared to the previous best models?", "answer": "[{'answer': 'Average accuracy of proposed model vs best prevous result:\\nSingle-task Training: 57.57 vs 55.06\\nMulti-task Training: 50.17 vs 50.59', 'type': 'abstractive'}]", "main_doc": "1909.08859.pdf", "documents": "['1909.08859.pdf', '1906.05474.pdf', '2002.01207.pdf', '1805.04033.pdf', '1909.08824.pdf', '2002.06644.pdf', '1806.04511.pdf', '1809.04960.pdf', '1909.05855.pdf', '1909.02480.pdf']"}
{"_id": "paper_tab_91", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the baseline human evaluation method used to compare against the classifier performance on Dataset-H?", "answer": "[{'answer': 'Human evaluators', 'type': 'abstractive'}]", "main_doc": "1610.00879.pdf", "documents": "['1610.00879.pdf', '2001.05493.pdf', '1910.11235.pdf', '1611.02550.pdf', '1909.00252.pdf', '1909.05855.pdf', '1904.09678.pdf', '1711.11221.pdf', '1909.13695.pdf', '1909.06162.pdf', '2003.12218.pdf']"}
{"_id": "paper_tab_92", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What stylistic and structural features are outlined in the paper for detecting drunk-texting behavior?", "answer": "[{'answer': 'LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors,  Spelling errors, Repeated characters, Capitalisation, Length,  Emoticon (Presence/Count ) \\n and Sentiment Ratio', 'type': 'abstractive'}, {'answer': 'LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors, Spelling errors, Repeated characters, Capitalization, Length, Emoticon (Presence/Count), Sentiment Ratio.', 'type': 'abstractive'}]", "main_doc": "1610.00879.pdf", "documents": "['1610.00879.pdf', '1911.02821.pdf', '2004.04721.pdf', '1909.01247.pdf', '1909.00279.pdf', '1905.00563.pdf', '1909.03544.pdf', '1909.11467.pdf', '1902.00672.pdf', '1911.02711.pdf', '1611.00514.pdf', '1911.04952.pdf', '1611.04798.pdf', '1703.02507.pdf', '1911.07228.pdf', '2001.05493.pdf', '2002.06644.pdf']"}
{"_id": "paper_tab_93", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What metrics are used to evaluate the performance of UniSent in comparison to manually crafted lexica in Czech, German, French, Macedonian, and Spanish?", "answer": "[{'answer': 'Accuracy and the macro-F1 (averaged F1 over positive and negative classes) are used as a measure of quality.', 'type': 'abstractive'}]", "main_doc": "1904.09678.pdf", "documents": "['1904.09678.pdf', '1810.12196.pdf', '1901.02262.pdf', '1912.10011.pdf', '1804.08139.pdf', '1909.01013.pdf', '1909.13695.pdf']"}
{"_id": "paper_tab_94", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "By how much do the BLEU-4 and ROUGE-L scores of the NAGM model outperform the best conventional models, Trans and CLSTM, on both the Oshiete-goo and nfL6 datasets, specifically in terms of exact numerical differences?", "answer": "[{'answer': \"For the Oshiete-goo dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, Trans, by 0.021, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.037.  For the nfL6 dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, CLSTM, by 0.028, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.040. Human evaluation of the NAGM's generated outputs for the Oshiete-goo dataset had 47% ratings of (1), the highest rating, while CLSTM only received 21% ratings of (1). For the nfL6 dataset, the comparison of (1)'s was NAGM's 50% to CLSTM's 30%. \", 'type': 'abstractive'}]", "main_doc": "1912.00864.pdf", "documents": "['1912.00864.pdf', '1811.02906.pdf', '1810.10254.pdf', '1804.07789.pdf', '1909.13375.pdf', '1910.11769.pdf', '1909.03544.pdf', '2001.06888.pdf', '1709.05413.pdf']"}
{"_id": "paper_tab_95", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the distinct citation intent labels identified in both the ACL-ARC and SciCite datasets, and how do they differ in terms of their application and coverage?", "answer": "[{'answer': 'Background, extends, uses, motivation, compare/contrast, and future work for the ACL-ARC dataset. Background, method, result comparison for the SciCite dataset.', 'type': 'abstractive'}]", "main_doc": "1904.01608.pdf", "documents": "['1904.01608.pdf', '2003.05377.pdf', '1908.11365.pdf', '1912.13109.pdf', '1909.08859.pdf', '1810.12196.pdf', '1909.03544.pdf', '2002.06424.pdf', '1705.01214.pdf', '1809.00530.pdf', '1901.02262.pdf', '1911.07555.pdf', '2002.06675.pdf', '1603.04513.pdf', '1711.00106.pdf', '2001.00137.pdf']"}
{"_id": "paper_tab_96", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "According to the results of the study investigating the effects of combining news and price data for short-term stock volatility prediction, which stock market sector achieved the highest accuracy in volatility forecasting?", "answer": "[{'answer': 'Energy with accuracy of 0.538', 'type': 'abstractive'}, {'answer': 'Energy', 'type': 'abstractive'}]", "main_doc": "1812.10479.pdf", "documents": "['1812.10479.pdf', '1909.13695.pdf', '2003.12218.pdf', '1904.09678.pdf', '1901.09755.pdf', '1909.00015.pdf', '1806.07711.pdf', '1909.09587.pdf', '1909.03544.pdf', '2003.07723.pdf', '1909.05855.pdf', '1911.02821.pdf', '1810.05241.pdf', '1901.08079.pdf', '1910.14497.pdf', '2001.00137.pdf', '1911.08673.pdf']"}
{"_id": "paper_tab_97", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the paper focus on datasets from the medical domain, without including general-purpose English datasets?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}]", "main_doc": "1901.08079.pdf", "documents": "['1901.08079.pdf', '1910.00458.pdf', '1810.06743.pdf', '1707.03569.pdf', '1909.00512.pdf', '1907.03060.pdf', '2004.04721.pdf', '1908.06267.pdf', '1801.05147.pdf', '1909.02480.pdf', '2002.06675.pdf']"}
{"_id": "paper_tab_98", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which 12 authorship verification methods were evaluated and classified based on the AV characteristics proposed in Section SECREF3, particularly in relation to their performance on challenging cases like informal chat conversations and cross-topic verification?", "answer": "[{'answer': 'MOCC, OCCAV, COAV, AVeer, GLAD, DistAV, Unmasking, Caravel, GenIM, ImpGI, SPATIUM and NNCD', 'type': 'abstractive'}]", "main_doc": "1906.10551.pdf", "documents": "['1906.10551.pdf', '1905.07464.pdf', '1909.00105.pdf', '1809.09795.pdf', '1804.00079.pdf', '1908.06379.pdf', '1910.06592.pdf', '1901.05280.pdf', '2002.08899.pdf', '1904.01608.pdf']"}
{"_id": "paper_tab_99", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How does SPNet's performance on ROUGE-1 and CIC compare to the best baseline method when evaluated on the MultiWOZ-based abstractive dialog summarization task?", "answer": "[{'answer': 'SPNet vs best baseline:\\nROUGE-1: 90.97 vs 90.68\\nCIC: 70.45 vs 70.25', 'type': 'abstractive'}]", "main_doc": "1910.00825.pdf", "documents": "['1910.00825.pdf', '1809.00540.pdf', '1901.02257.pdf', '1707.05236.pdf', '1712.00991.pdf', '2002.01359.pdf', '1904.09678.pdf', '1809.10644.pdf', '1912.10011.pdf']"}
{"_id": "paper_tab_100", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the performance difference in ROUGE-1, ROUGE-L, and METEOR scores between the best proposed model and the previous top-performing model on both the arXiv and PubMed datasets in the paper \"Extractive Summarization of Long Documents by Combining Global and Local Context\"?", "answer": "[{'answer': 'Best proposed model result vs best previous result:\\nArxiv dataset: Rouge 1 (43.62 vs 42.81), Rouge L (29.30 vs 31.80), Meteor (21.78 vs 21.35)\\nPubmed dataset: Rouge 1 (44.85 vs 44.29), Rouge L (31.48 vs 35.21), Meteor (20.83 vs 20.56)', 'type': 'abstractive'}, {'answer': 'On arXiv dataset, the proposed model outperforms baselie model by (ROUGE-1,2,L)  0.67 0.72 0.77 respectively and by Meteor 0.31.\\n', 'type': 'abstractive'}]", "main_doc": "1909.08089.pdf", "documents": "['1909.08089.pdf', '1906.05474.pdf', '2001.05493.pdf', '1703.07090.pdf', '1610.07809.pdf', '1812.06705.pdf', '2004.03354.pdf']"}
{"_id": "paper_tab_101", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the reported EER, Cmindet, and Cdet values for the Intelligent Voice system on the NIST 2016 Speaker Recognition Evaluation development set?", "answer": "[{'answer': 'EER 16.04, Cmindet 0.6012, Cdet 0.6107', 'type': 'abstractive'}]", "main_doc": "1611.00514.pdf", "documents": "['1611.00514.pdf', '1805.03710.pdf', '1707.00110.pdf', '1910.13215.pdf', '1908.06267.pdf', '1911.12579.pdf', '1603.00968.pdf', '1810.12196.pdf', '1909.00361.pdf', '1809.06537.pdf', '1906.11180.pdf', '1912.00864.pdf', '1701.06538.pdf', '2002.05829.pdf', '1909.00578.pdf', '1911.08673.pdf', '1910.07481.pdf', '1811.02906.pdf']"}
{"_id": "paper_tab_102", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the Mean Absolute Error (MAE), macro-averaged MAE (MAEM), binary classification accuracy, and weighted binary classification accuracy values for the proposed Ordinal Regression Neural Network (ORNN) compared to the previous best state-of-the-art model for the Trafficking-10K dataset, for the task of detecting sex trafficking in escort ads?", "answer": "[{'answer': 'Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)', 'type': 'abstractive'}]", "main_doc": "1908.05434.pdf", "documents": "['1908.05434.pdf', '1911.02711.pdf', '1703.02507.pdf', '1909.00252.pdf', '1908.07816.pdf', '1908.07245.pdf', '1909.00361.pdf', '1802.06024.pdf', '1911.00069.pdf', '1810.09774.pdf', '1909.00512.pdf', '2003.08385.pdf']"}
{"_id": "paper_tab_103", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the accuracy difference between using a user-written (golden) summary and an auto-generated summary in the hierarchically-refined review-centric attention model?", "answer": "[{'answer': '2.7 accuracy points', 'type': 'abstractive'}]", "main_doc": "1911.02711.pdf", "documents": "['1911.02711.pdf', '2003.04866.pdf', '2003.06044.pdf', '1712.03556.pdf', '1804.00079.pdf', '1910.06592.pdf', '1807.07961.pdf', '1906.01081.pdf', '1706.08032.pdf', '1904.07904.pdf', '1909.06162.pdf', '2003.03044.pdf', '1912.08960.pdf']"}
{"_id": "paper_tab_104", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "\"How many shared layers were utilized in the final model configuration for both NER and RE tasks in the ADE dataset?\"", "answer": "[{'answer': '1', 'type': 'abstractive'}]", "main_doc": "2002.06424.pdf", "documents": "['2002.06424.pdf', '1909.09484.pdf', '2001.06888.pdf', '1909.08041.pdf', '1703.07090.pdf', '1709.10217.pdf', '2002.00652.pdf', '2002.01664.pdf', '1812.01704.pdf', '1702.03342.pdf', '2002.02492.pdf', '1704.05907.pdf', '1809.00540.pdf', '2003.03106.pdf', '1809.09194.pdf', '1603.04513.pdf', '1906.10551.pdf', '1910.06036.pdf']"}
{"_id": "paper_tab_105", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How many additional task-specific RNN layers are introduced for Named Entity Recognition (NER) and Relation Extraction (RE) in both the ADE and CoNLL04 datasets in the proposed architecture?", "answer": "[{'answer': '2 for the ADE dataset and 3 for the CoNLL04 dataset', 'type': 'abstractive'}]", "main_doc": "2002.06424.pdf", "documents": "['2002.06424.pdf', '1909.04002.pdf', '1912.08960.pdf', '1911.02821.pdf', '1912.01673.pdf', '1909.00252.pdf', '1704.06194.pdf', '1901.09755.pdf', '1801.05147.pdf', '1810.03459.pdf', '1911.11951.pdf', '1912.10011.pdf', '1904.10503.pdf', '1909.08824.pdf', '2002.10361.pdf', '2003.12218.pdf', '2001.08051.pdf', '1910.14497.pdf']"}
{"_id": "paper_tab_106", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What were the three real-world datasets, including the subverted versions, that were used to evaluate the impact of sentiment detection on toxicity detection in this study?", "answer": "[{'answer': 'Kaggle\\nSubversive Kaggle\\nWikipedia\\nSubversive Wikipedia\\nReddit\\nSubversive Reddit ', 'type': 'abstractive'}]", "main_doc": "1812.01704.pdf", "documents": "['1812.01704.pdf', '1909.11467.pdf', '1912.01673.pdf', '1912.10806.pdf', '2002.01984.pdf', '1703.02507.pdf']"}
{"_id": "paper_tab_107", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which team achieved the top micro-F1 score of 81.5% on the Friends dataset for spoken dialogues in the EmotionX 2019 Challenge?", "answer": "[{'answer': 'IDEA', 'type': 'abstractive'}]", "main_doc": "1909.07734.pdf", "documents": "['1909.07734.pdf', '1910.07481.pdf', '1712.00991.pdf', '1909.00252.pdf', '1712.05999.pdf', '1912.10011.pdf', '1911.08962.pdf', '1808.03430.pdf', '1910.11769.pdf', '1909.08041.pdf', '1707.08559.pdf', '2002.00652.pdf', '1804.05918.pdf', '1603.00968.pdf', '1810.09774.pdf', '1908.06151.pdf', '1901.02262.pdf']"}
{"_id": "paper_tab_108", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What evaluation metrics were used to assess the performance of BERT and ELMo across the ten datasets in the BLUE benchmark?", "answer": "[{'answer': 'BLUE utilizes different metrics for each of the tasks: Pearson correlation coefficient, F-1 scores, micro-averaging, and accuracy', 'type': 'abstractive'}]", "main_doc": "1906.05474.pdf", "documents": "['1906.05474.pdf', '1904.01608.pdf', '1801.05147.pdf', '1908.07816.pdf', '1911.05153.pdf']"}
{"_id": "paper_tab_109", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the specific tasks in the BLUE benchmark in the paper evaluating BERT and ELMo on ten biomedical and clinical datasets, used to assess model performance across various NLP challenges?", "answer": "[{'answer': 'Inference task\\nThe aim of the inference task is to predict whether the premise sentence entails or contradicts the hypothesis sentence, Document multilabel classification\\nThe multilabel classification task predicts multiple labels from the texts., Relation extraction\\nThe aim of the relation extraction task is to predict relations and their types between the two entities mentioned in the sentences., Named entity recognition\\nThe aim of the named entity recognition task is to predict mention spans given in the text , Sentence similarity\\nThe sentence similarity task is to predict similarity scores based on sentence pairs', 'type': 'extractive'}]", "main_doc": "1906.05474.pdf", "documents": "['1906.05474.pdf', '1707.05236.pdf', '1909.13695.pdf', '1705.01214.pdf', '1804.08050.pdf']"}
{"_id": "paper_tab_110", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which summarization algorithms were evaluated using ROUGE unigram f1 scores for peer feedback summarization in the study on supervisor assessments and peer feedback in performance appraisals?", "answer": "[{'answer': 'LSA, TextRank, LexRank and ILP-based summary.', 'type': 'abstractive'}, {'answer': 'LSA, TextRank, LexRank', 'type': 'abstractive'}]", "main_doc": "1712.00991.pdf", "documents": "['1712.00991.pdf', '1710.09340.pdf', '1801.05147.pdf', '1909.03544.pdf']"}
{"_id": "paper_tab_111", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What evaluation metrics are computed to assess the multi-label classification task of matching supervisor assessments to predefined performance perspectives in this paper?", "answer": "[{'answer': 'Precision, Recall, F-measure, accuracy', 'type': 'extractive'}, {'answer': 'Precision, Recall and F-measure', 'type': 'extractive'}]", "main_doc": "1712.00991.pdf", "documents": "['1712.00991.pdf', '1711.00106.pdf', '2003.11563.pdf', '1705.00108.pdf', '1909.01247.pdf', '1911.02821.pdf', '1804.11346.pdf', '1905.06566.pdf', '1611.02550.pdf']"}
{"_id": "paper_tab_112", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which specific machine learning classifiers, including both SciKit Learn implementations and custom-developed approaches, were tested using 5-fold cross-validation in the supervisor assessment text mining experiments?", "answer": "[{'answer': 'Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK and Pattern-based', 'type': 'abstractive'}, {'answer': 'Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK, Pattern-based approach', 'type': 'abstractive'}]", "main_doc": "1712.00991.pdf", "documents": "['1712.00991.pdf', '2003.12738.pdf', '1701.05574.pdf', '1803.09230.pdf', '1911.01799.pdf']"}
{"_id": "paper_tab_113", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How much intelligibility improvement is achieved for the AECNN-T and AECNN-T-SM models when incorporating mimic loss for speech enhancement, based on experiments with CHiME-4 data?", "answer": "[{'answer': 'Improved AECNN-T by 2.1 and AECNN-T-SM BY 0.9', 'type': 'abstractive'}]", "main_doc": "2003.01769.pdf", "documents": "['2003.01769.pdf', '1912.01673.pdf', '1909.09484.pdf', '1701.03214.pdf', '1711.11221.pdf', '1609.00559.pdf', '1901.01010.pdf', '2004.01878.pdf']"}
{"_id": "paper_tab_114", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which models are listed as having previously held the state-of-the-art results on Dutch downstream tasks that RobBERT has now surpassed?", "answer": "[{'answer': 'BERTje BIBREF8, an ULMFiT model (Universal Language Model Fine-tuning for Text Classification model) BIBREF19., mBERT', 'type': 'extractive'}]", "main_doc": "2001.06286.pdf", "documents": "['2001.06286.pdf', '2002.11402.pdf', '1909.13695.pdf', '1904.07904.pdf', '2003.03014.pdf', '1909.13714.pdf', '2003.05377.pdf', '1909.01383.pdf', '1902.00330.pdf', '2004.03744.pdf', '1810.06743.pdf', '1911.00069.pdf', '1807.07961.pdf', '1812.06705.pdf']"}
{"_id": "paper_tab_115", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "By how much does Stacked DeBERT outperform baseline models in F1-score in the Twitter Sentiment Classification task using Kaggle's Sentiment140 Corpus, and what is the average improvement in the intent classification task across the Chatbot NLU corpus?", "answer": "[{'answer': 'In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average', 'type': 'abstractive'}]", "main_doc": "2001.00137.pdf", "documents": "['2001.00137.pdf', '1712.03547.pdf', '2002.06675.pdf', '1909.09587.pdf']"}
{"_id": "paper_tab_116", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the exact micro F1 performance improvement of the proposed model over the highest performing baseline on the ACE2004 and CWEB datasets, as well as the overall average?", "answer": "[{'answer': 'Comparing with the highest performing baseline: 1.3 points on ACE2004 dataset, 0.6 points on CWEB dataset, and 0.86 points in the average of all scores.', 'type': 'abstractive'}]", "main_doc": "1902.00330.pdf", "documents": "['1902.00330.pdf', '1905.06566.pdf', '1911.13066.pdf', '1709.10217.pdf', '1904.01608.pdf', '1805.03710.pdf', '1901.09755.pdf', '2002.11402.pdf', '2002.06675.pdf', '2003.07996.pdf', '1811.02906.pdf', '2004.01980.pdf']"}
{"_id": "paper_tab_117", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the paper present a comparison of multiple approaches for utterance-level intent recognition in autonomous vehicle scenarios?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}]", "main_doc": "1901.04899.pdf", "documents": "['1901.04899.pdf', '1908.06379.pdf', '1705.01214.pdf', '2001.10161.pdf', '1912.01772.pdf', '1912.01673.pdf', '2002.05829.pdf', '1911.02086.pdf', '1910.07481.pdf']"}
{"_id": "paper_tab_118", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What stylistic domains are represented in the annotated sentences from the Romanian newspaper corpus?", "answer": "[{'answer': 'current news, historical news, free time, sports, juridical news pieces, personal adverts, editorials.', 'type': 'abstractive'}]", "main_doc": "1909.01247.pdf", "documents": "['1909.01247.pdf', '1911.07555.pdf', '2004.03788.pdf', '1911.08976.pdf', '1711.11221.pdf', '1901.05280.pdf', '1906.10551.pdf', '1904.05584.pdf', '1810.00663.pdf', '1606.00189.pdf', '1909.03405.pdf', '1811.02906.pdf', '1909.01013.pdf', '2001.08051.pdf', '1906.05474.pdf', '1905.11901.pdf']"}
{"_id": "paper_tab_119", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How much larger are the training datasets used to create the new ELMo embeddings for the seven less-resourced languages, compared to those used in the CoNLL 2017 Shared Task, which sampled data from sources like Wikipedia dump and Common Crawl?", "answer": "[{'answer': 'By 14 times.', 'type': 'abstractive'}, {'answer': 'up to 1.95 times larger', 'type': 'abstractive'}]", "main_doc": "1911.10049.pdf", "documents": "['1911.10049.pdf', '1812.10479.pdf', '1910.04269.pdf', '2004.03354.pdf', '1804.11346.pdf', '1911.01680.pdf', '2002.06644.pdf', '1806.07711.pdf', '1701.05574.pdf', '1709.10367.pdf', '2003.05377.pdf', '2002.01984.pdf', '1611.03382.pdf', '1701.03214.pdf', '1909.05855.pdf', '1908.06379.pdf']"}
{"_id": "paper_tab_120", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the reported improvement in F1 score between ELMo and FastText embeddings for the Estonian NER task in the study on seven less-resourced languages?", "answer": "[{'answer': '5 percent points.', 'type': 'abstractive'}, {'answer': '0.05 F1', 'type': 'abstractive'}]", "main_doc": "1911.10049.pdf", "documents": "['1911.10049.pdf', '1909.06937.pdf', '1603.00968.pdf', '1908.06379.pdf', '1805.03710.pdf', '1801.05147.pdf']"}
{"_id": "paper_tab_121", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What language pairs, including the six zero-shot translation directions using Arabic, Spanish, and Russian from the MultiUN dataset, are evaluated through cross-lingual pre-training in the experiments outlined in this paper?", "answer": "[{'answer': 'De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-En, En-Ru', 'type': 'abstractive'}, {'answer': 'French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation', 'type': 'extractive'}]", "main_doc": "1912.01214.pdf", "documents": "['1912.01214.pdf', '1701.00185.pdf', '1904.01608.pdf', '1709.10217.pdf', '1810.10254.pdf', '2002.02070.pdf']"}
{"_id": "paper_tab_122", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How does the performance of the proposed KG embedding method using entity co-occurrence statistics compare to baseline methods in terms of maintaining comparable KG task performance while significantly improving interpretability?", "answer": "[{'answer': 'Performance was comparable, with the proposed method quite close and sometimes exceeding performance of baseline method.', 'type': 'abstractive'}]", "main_doc": "1712.03547.pdf", "documents": "['1712.03547.pdf', '1906.11180.pdf', '1906.10225.pdf', '1812.10479.pdf', '1705.01214.pdf', '1904.10500.pdf', '2004.03788.pdf', '1912.01673.pdf', '1706.08032.pdf']"}
{"_id": "paper_tab_123", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What baseline models are compared to MGNC-CNN in the experiments involving the use of word2vec, Glove, and syntactic embeddings?", "answer": "[{'answer': 'MC-CNN\\nMVCNN\\nCNN', 'type': 'abstractive'}]", "main_doc": "1603.00968.pdf", "documents": "['1603.00968.pdf', '1909.05855.pdf', '1703.06492.pdf', '1908.11365.pdf', '1701.02877.pdf', '1605.08675.pdf', '1804.00079.pdf', '1703.02507.pdf', '2004.03744.pdf', '1908.11047.pdf', '1909.08859.pdf', '1902.00672.pdf', '1912.10011.pdf', '2001.10161.pdf', '1908.06267.pdf']"}
{"_id": "paper_tab_124", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "By how much does the MGNC-CNN model outperform the baseline models across different datasets like Subj, SST-1, SST-2, TREC, and Irony?", "answer": "[{'answer': 'In terms of Subj the Average MGNC-CNN is better than the average score of baselines by 0.5.  Similarly, Scores of SST-1, SST-2, and TREC where MGNC-CNN has similar improvements. \\nIn case of Irony the difference is about 2.0. \\n', 'type': 'abstractive'}]", "main_doc": "1603.00968.pdf", "documents": "['1603.00968.pdf', '1910.06036.pdf', '1611.03382.pdf', '2001.05970.pdf', '1911.01680.pdf', '1908.06264.pdf', '1908.05434.pdf', '1910.00912.pdf', '2001.08051.pdf', '1909.01013.pdf', '1605.07333.pdf', '1812.06864.pdf', '2004.03788.pdf', '1703.02507.pdf']"}
{"_id": "paper_tab_125", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What baseline and alternative architectures, including those that utilize single and concatenated word embeddings, are compared to MGNC-CNN according to the study?", "answer": "[{'answer': 'standard CNN, C-CNN, MVCNN ', 'type': 'extractive'}]", "main_doc": "1603.00968.pdf", "documents": "['1603.00968.pdf', '1901.05280.pdf', '1902.09393.pdf', '1908.06379.pdf', '1606.05320.pdf', '1806.11432.pdf', '1605.07333.pdf', '1902.09314.pdf']"}
{"_id": "paper_tab_126", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How does the exclusion of pixel data impact the crosslingual semantic similarity scores for the ImageVec model compared to previous methods across all six subtasks?", "answer": "[{'answer': 'performance is significantly degraded without pixel data', 'type': 'abstractive'}]", "main_doc": "1905.12260.pdf", "documents": "['1905.12260.pdf', '1909.02480.pdf', '1909.00105.pdf', '1912.10806.pdf', '1910.05154.pdf', '1910.06592.pdf', '2001.08051.pdf', '1708.09609.pdf', '1912.01214.pdf', '1809.05752.pdf', '1806.04330.pdf', '2002.01984.pdf', '1810.00663.pdf', '2004.01980.pdf', '1909.11687.pdf']"}
{"_id": "paper_tab_127", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the total duration of transcribed speech data available for the eight speakers in the speech corpus used to develop the Ainu language ASR system?", "answer": "[{'answer': 'Transcribed data is available for duration of 38h 54m 38s for 8 speakers.', 'type': 'abstractive'}]", "main_doc": "2002.06675.pdf", "documents": "['2002.06675.pdf', '1910.11204.pdf', '1909.00279.pdf', '1808.09029.pdf', '1912.00864.pdf', '2002.01207.pdf', '1808.03430.pdf', '1705.00108.pdf', '1911.08976.pdf', '1910.08210.pdf', '1804.07789.pdf', '1812.10479.pdf']"}
{"_id": "paper_tab_128", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the Common Voice dataset, is the distribution of audio hours across the released languages described as balanced?", "answer": "[{'answer': 'No', 'type': 'boolean'}]", "main_doc": "1912.06670.pdf", "documents": "['1912.06670.pdf', '1703.07090.pdf', '1805.03710.pdf', '1911.01680.pdf', '1806.11432.pdf', '1707.05236.pdf', '1910.07481.pdf', '1902.09666.pdf', '1912.01673.pdf', '1904.03288.pdf', '1911.04952.pdf', '1603.00968.pdf', '1810.12885.pdf']"}
{"_id": "paper_tab_129", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the F1-scores of BERT for detection, relaxed classification, and strict classification tasks in the anonymisation of Spanish clinical text, and how do they compare to HUBES-PHI and MEDDOCAN as reported in the results?", "answer": "[{'answer': 'F1 scores are:\\nHUBES-PHI: Detection(0.965), Classification relaxed (0.95), Classification strict (0.937)\\nMedoccan: Detection(0.972), Classification (0.967)', 'type': 'abstractive'}, {'answer': 'BERT remains only 0.3 F1-score points behind, and would have achieved the second position among all the MEDDOCAN shared task competitors. Taking into account that only 3% of the gold labels remain incorrectly annotated,  Table ', 'type': 'extractive'}]", "main_doc": "2003.03106.pdf", "documents": "['2003.03106.pdf', '1704.08960.pdf', '1611.04798.pdf', '1901.05280.pdf', '1704.00939.pdf', '1909.11297.pdf', '1806.07711.pdf', '1701.06538.pdf', '1810.05241.pdf', '1909.00252.pdf', '2002.02492.pdf']"}
{"_id": "paper_tab_130", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which variant of the Variational Transformer (GVT with global latent variables or SVT with sequential latent variables) achieves better performance in terms of perplexity, diversity, embeddings similarity, and human evaluation based on the experimental results presented in the paper?", "answer": "[{'answer': 'PPL: SVT\\nDiversity: GVT\\nEmbeddings Similarity: SVT\\nHuman Evaluation: SVT', 'type': 'abstractive'}]", "main_doc": "2003.12738.pdf", "documents": "['2003.12738.pdf', '2002.04181.pdf', '1701.06538.pdf', '1704.08960.pdf', '1901.05280.pdf', '1611.03382.pdf', '1809.10644.pdf']"}
{"_id": "paper_tab_131", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which baseline models are referenced on the ListOps dataset as comparisons, including those from Nangia and Bowman (2018), to evaluate the new recursive model's performance?", "answer": "[{'answer': 'The system is compared to baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM', 'type': 'abstractive'}]", "main_doc": "1902.09393.pdf", "documents": "['1902.09393.pdf', '1909.08089.pdf', '1911.08976.pdf', '2002.05829.pdf', '2002.01207.pdf', '2004.01878.pdf', '1902.10525.pdf', '1906.05474.pdf', '1911.03597.pdf', '1908.11546.pdf', '1812.06705.pdf']"}
{"_id": "paper_tab_132", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "\"According to 'One Size Does Not Fit All: Generating and Evaluating Variable Number of Keyphrases,' what is the approximate size of the StackEx dataset, in terms of the number of questions, used for keyphrase generation?\"", "answer": "[{'answer': 'around 330k questions', 'type': 'abstractive'}]", "main_doc": "1810.05241.pdf", "documents": "['1810.05241.pdf', '2003.03014.pdf', '1810.03459.pdf', '1910.12795.pdf', '1708.09609.pdf', '2001.06888.pdf', '1910.07481.pdf', '1909.00175.pdf', '1809.01202.pdf', '1709.10367.pdf', '1904.05584.pdf', '1806.04511.pdf', '1910.10288.pdf', '1707.03569.pdf', '1712.00991.pdf']"}
{"_id": "paper_tab_133", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What baseline models are listed for comparison against the proposed recurrent generative model on the present-keyphrase portion of the KP20k dataset?", "answer": "[{'answer': 'CopyRNN (Meng et al., 2017), Multi-Task (Ye and Wang, 2018), and TG-Net (Chen et al., 2018b)', 'type': 'abstractive'}, {'answer': 'CopyRNN BIBREF0, KEA BIBREF4 and Maui BIBREF8, CopyRNN*', 'type': 'extractive'}]", "main_doc": "1810.05241.pdf", "documents": "['1810.05241.pdf', '1810.09774.pdf', '1909.05855.pdf', '1908.07195.pdf', '1912.01772.pdf', '1910.14537.pdf', '1702.03342.pdf', '1904.09678.pdf', '2001.08051.pdf']"}
{"_id": "paper_tab_134", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What hyperparameters are evaluated in the Word2Vec study, and how do they impact tasks such as Named Entity Recognition (NER), Sentiment Analysis (SA), and vector quality?", "answer": "[{'answer': 'Dimension size, window size, architecture, algorithm, epochs, hidden dimension size, learning rate, loss function, optimizer algorithm.', 'type': 'abstractive'}, {'answer': 'Hyperparameters explored were: dimension size, window size, architecture, algorithm and epochs.', 'type': 'abstractive'}]", "main_doc": "2003.11645.pdf", "documents": "['2003.11645.pdf', '1909.09270.pdf', '2002.05058.pdf', '2004.04721.pdf', '1804.08050.pdf', '1908.11047.pdf', '1909.00361.pdf', '1910.11769.pdf', '1910.13215.pdf', '2003.05377.pdf', '1810.05241.pdf', '1809.06537.pdf', '1709.05413.pdf', '1810.09774.pdf', '2001.08868.pdf']"}
{"_id": "paper_tab_135", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the study on Word2Vec hyper-parameters present choices relevant to both the skip-gram and CBOW models?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]", "main_doc": "2003.11645.pdf", "documents": "['2003.11645.pdf', '1909.11297.pdf', '1902.09666.pdf', '1705.01265.pdf', '1809.09795.pdf', '1902.00672.pdf', '2002.02070.pdf', '1811.12254.pdf']"}
{"_id": "paper_tab_136", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How did word-representations and basic pre-processing influence the final test set performance in the SemEval-2017 Task 5 (subtask 2) according to the reported results?", "answer": "[{'answer': 'beneficial impact of word-representations and basic pre-processing', 'type': 'extractive'}]", "main_doc": "1704.00939.pdf", "documents": "['1704.00939.pdf', '1909.07734.pdf', '1909.01958.pdf', '1911.00069.pdf', '1912.03457.pdf', '1706.08032.pdf', '2004.03744.pdf', '1704.08960.pdf', '1911.07228.pdf', '1911.02821.pdf', '1810.09774.pdf', '1906.10225.pdf', '1909.04002.pdf', '1909.06162.pdf', '1612.08205.pdf', '1908.07245.pdf']"}
{"_id": "paper_tab_137", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the difference in Exact Match (EM) and F1 metrics between the LARGE-SQUAD and MTMSNlarge models for single-span, number-type, and date-type questions from the DROP development set?", "answer": "[{'answer': 'For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.\\nFor number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. \\nFor date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1.', 'type': 'abstractive'}]", "main_doc": "1909.13375.pdf", "documents": "['1909.13375.pdf', '1909.11687.pdf', '1611.04798.pdf', '1803.09230.pdf', '1905.12260.pdf', '1910.13215.pdf', '1909.00361.pdf', '1708.09609.pdf', '1711.11221.pdf', '1901.09755.pdf', '1909.06937.pdf', '1906.10225.pdf', '1709.05413.pdf', '1911.01799.pdf']"}
{"_id": "paper_tab_138", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the precise EM and F1 scores achieved by the proposed multi-span sequence tagging model on DROP's development and test sets?", "answer": "[{'answer': 'The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev', 'type': 'abstractive'}]", "main_doc": "1909.13375.pdf", "documents": "['1909.13375.pdf', '1709.10217.pdf', '1911.08962.pdf', '1701.02877.pdf', '1901.09755.pdf', '1804.11346.pdf', '1908.05434.pdf', '1911.11951.pdf', '1707.08559.pdf', '1810.06743.pdf', '1811.12254.pdf', '1603.04513.pdf', '2004.04721.pdf']"}
{"_id": "paper_tab_139", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the three incremental levels of document preprocessing implemented in the study to evaluate the robustness of keyphrase extraction models over noisy scientific articles, as described in the SemEval-2010 dataset analysis?", "answer": "[{'answer': 'raw text, text cleaning through document logical structure detection, removal of keyphrase sparse sections of the document', 'type': 'extractive'}, {'answer': 'Level 1, Level 2 and Level 3.', 'type': 'abstractive'}]", "main_doc": "1610.07809.pdf", "documents": "['1610.07809.pdf', '1910.10288.pdf', '1902.00672.pdf', '1705.08142.pdf', '2002.02492.pdf', '1905.00563.pdf', '1705.01214.pdf', '1701.05574.pdf', '1609.00559.pdf', '2003.04866.pdf', '1604.00400.pdf', '2001.10161.pdf', '1812.10479.pdf', '1712.05999.pdf', '1802.06024.pdf', '1909.00252.pdf', '1809.02286.pdf', '1908.06379.pdf']"}
{"_id": "paper_tab_140", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What improvements in the BPRA, APRA, and BLEU metrics does the proposed generative dialog policy (GDP) model achieve over the baseline models on the DSTC2 and Maluuba task-oriented dialogue datasets?", "answer": "[{'answer': 'most of the models have similar performance on BPRA: DSTC2 (+0.0015), Maluuba (+0.0729)\\nGDP achieves the best performance in APRA: DSTC2 (+0.2893), Maluuba (+0.2896)\\nGDP significantly outperforms the baselines on BLEU: DSTC2 (+0.0791), Maluuba (+0.0492)', 'type': 'abstractive'}]", "main_doc": "1909.09484.pdf", "documents": "['1909.09484.pdf', '1910.06036.pdf', '2002.11402.pdf', '1901.04899.pdf']"}
{"_id": "paper_tab_141", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How many rows of labeled text messages are present in the dataset used to train the classifier for categorizing Hinglish social media content into abusive, hate-inducing, and not offensive categories?", "answer": "[{'answer': '3189 rows of text messages', 'type': 'extractive'}, {'answer': 'Resulting dataset was 7934 messages for train and 700 messages for test.', 'type': 'abstractive'}]", "main_doc": "1912.13109.pdf", "documents": "['1912.13109.pdf', '1911.02086.pdf', '1910.10288.pdf', '1910.11235.pdf', '1809.00540.pdf', '1809.09194.pdf', '1605.07333.pdf', '1703.06492.pdf', '1912.01214.pdf', '1908.11365.pdf', '1712.03547.pdf', '1603.07044.pdf', '1909.06937.pdf', '1811.02906.pdf', '2001.08051.pdf', '1908.06151.pdf']"}
{"_id": "paper_tab_142", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the average BLEU score improvement reported when using the DocRepair model for refining 4-sentence fragment translations in the English-Russian translation task?", "answer": "[{'answer': 'On average 0.64 ', 'type': 'abstractive'}]", "main_doc": "1909.01383.pdf", "documents": "['1909.01383.pdf', '1908.06267.pdf', '1906.03538.pdf', '1910.04269.pdf', '1911.01799.pdf', '2003.03044.pdf', '1910.08987.pdf', '1709.10217.pdf', '1909.11467.pdf', '1804.05918.pdf', '1910.03467.pdf', '1910.13215.pdf', '1905.10810.pdf', '1909.08041.pdf', '1712.03547.pdf']"}
{"_id": "paper_tab_143", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What baseline models are listed in the validation results for the Friends dataset used to compare with Emotion BERT in the task of dialogue emotion prediction?", "answer": "[{'answer': 'BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN', 'type': 'abstractive'}, {'answer': 'bag-of-words (BOW), term frequency\u2013inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe', 'type': 'extractive'}]", "main_doc": "1908.06264.pdf", "documents": "['1908.06264.pdf', '2003.03044.pdf', '1706.08032.pdf', '2001.08868.pdf']"}
{"_id": "paper_tab_144", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "In the EmotionX-IDEA study, which BERT model configurations were used for validation on the Friends dataset?", "answer": "[{'answer': 'BERT-base, BERT-large, BERT-uncased, BERT-cased', 'type': 'abstractive'}]", "main_doc": "1908.06264.pdf", "documents": "['1908.06264.pdf', '2003.03044.pdf', '1911.13066.pdf', '1911.08976.pdf', '1901.01010.pdf']"}
{"_id": "paper_tab_145", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What evaluation metric was calculated on the validation set to select the best model for inference in the cascaded multimodal speech translation system developed for the IWSLT 2019 evaluation?", "answer": "[{'answer': 'BLEU scores', 'type': 'abstractive'}]", "main_doc": "1910.13215.pdf", "documents": "['1910.13215.pdf', '1901.08079.pdf', '1810.05241.pdf', '1902.09666.pdf', '2002.00652.pdf']"}
{"_id": "paper_tab_146", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "According to the results, which translation direction\u2014English-French or French-English\u2014demonstrated better performance based on both BLEU and TER metrics?", "answer": "[{'answer': 'French-English', 'type': 'abstractive'}]", "main_doc": "1910.07481.pdf", "documents": "['1910.07481.pdf', '1704.06194.pdf', '1911.12579.pdf', '1909.00578.pdf', '1809.04960.pdf', '2003.12218.pdf', '1803.09230.pdf']"}
{"_id": "paper_tab_147", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the clustering algorithm involve the use of graphical models in comparison to CluStream for multilingual datasets?", "answer": "[{'answer': 'No', 'type': 'boolean'}, {'answer': 'No', 'type': 'boolean'}]", "main_doc": "1809.00540.pdf", "documents": "['1809.00540.pdf', '2004.04721.pdf', '2002.06424.pdf', '1909.00430.pdf', '1701.09123.pdf']"}
{"_id": "paper_tab_148", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What evaluation metrics are used to compare the clustering performance, including precision, recall, and F1 scores, of the proposed algorithm against CluStream?", "answer": "[{'answer': 'F1, precision, recall, accuracy', 'type': 'abstractive'}, {'answer': 'Precision, recall, F1, accuracy', 'type': 'abstractive'}]", "main_doc": "1809.00540.pdf", "documents": "['1809.00540.pdf', '1909.00279.pdf', '1909.11687.pdf', '1704.06194.pdf', '1911.00069.pdf', '1908.06083.pdf', '1707.05236.pdf', '1910.06592.pdf']"}
{"_id": "paper_tab_149", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What word embedding methods are compared against the proposed vector-res and vector-faith methods in this paper's evaluation of improving correlation with human judgments using semantic similarity integration?", "answer": "[{'answer': 'Skip\u2013gram, CBOW', 'type': 'extractive'}, {'answer': 'integrated vector-res, vector-faith, Skip\u2013gram, CBOW', 'type': 'extractive'}]", "main_doc": "1609.00559.pdf", "documents": "['1609.00559.pdf', '1909.07734.pdf', '1903.09722.pdf', '1902.09393.pdf', '1705.01214.pdf', '2002.04181.pdf', '2004.01980.pdf', '2002.11910.pdf', '1911.02086.pdf', '1901.01010.pdf', '1611.02550.pdf', '1909.00694.pdf', '1911.00069.pdf']"}
{"_id": "paper_tab_150", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the percentage improvement of the GTRS model over both SVM and Pawlak rough set models in detecting satirical news tweets, as presented in the experimental results?", "answer": "[{'answer': 'Their GTRS approach got an improvement of 3.89% compared to SVM and 27.91% compared to Pawlak.', 'type': 'abstractive'}]", "main_doc": "2004.03788.pdf", "documents": "['2004.03788.pdf', '2002.01664.pdf', '1910.08987.pdf', '1805.03710.pdf', '1909.13695.pdf', '1704.00939.pdf', '1904.07904.pdf', '1909.04002.pdf', '2003.12738.pdf', '1705.08142.pdf', '1611.03382.pdf', '2002.00652.pdf', '1810.12885.pdf', '1909.06162.pdf', '2003.03106.pdf', '1909.02480.pdf', '1701.03214.pdf']"}
{"_id": "paper_tab_151", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the experimental results, by how many percentage points does the proposed multi-head decoder method improve the Character Error Rate compared to the best-performing multi-head attention model?", "answer": "[{'answer': 'Their average improvement in Character Error Rate over the best MHA model was 0.33 percent points.', 'type': 'abstractive'}]", "main_doc": "1804.08050.pdf", "documents": "['1804.08050.pdf', '2002.06424.pdf', '1803.09230.pdf', '1906.01081.pdf', '2002.00652.pdf', '2002.04181.pdf', '1908.11546.pdf']"}
{"_id": "paper_tab_152", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "In the experimental conditions described, what is the total size, in utterances, of the Corpus of Spontaneous Japanese used for evaluating the multi-head decoder model?", "answer": "[{'answer': '449050', 'type': 'abstractive'}]", "main_doc": "1804.08050.pdf", "documents": "['1804.08050.pdf', '1611.00514.pdf', '1707.03569.pdf', '1909.00105.pdf', '1909.08041.pdf', '2002.01207.pdf', '1905.06566.pdf', '1901.02257.pdf', '1908.11546.pdf', '2002.08899.pdf']"}
{"_id": "paper_tab_153", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What were the F1 scores of the RelAwe with DepPath&RelPath model in the closed and open settings for the Chinese SRL task on the CoNLL-2009 dataset?", "answer": "[{'answer': 'In closed setting 84.22 F1 and in open 87.35 F1.', 'type': 'abstractive'}]", "main_doc": "1910.11204.pdf", "documents": "['1910.11204.pdf', '1804.07789.pdf', '1906.10551.pdf', '1903.09722.pdf', '1809.00540.pdf', '2002.06644.pdf', '1809.02279.pdf', '1809.08298.pdf', '1906.10225.pdf', '1911.08673.pdf', '1606.00189.pdf', '1806.11432.pdf', '1604.00400.pdf', '1807.07961.pdf', '1911.07228.pdf', '1909.01247.pdf']"}
{"_id": "paper_tab_154", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which two strong baseline methods are compared to the syntax-enhanced self-attention model in the SRL results for the Chinese CoNLL-2009 dataset?", "answer": "[{'answer': 'Marcheggiani and Titov (2017) and Cai et al. (2018)', 'type': 'abstractive'}]", "main_doc": "1910.11204.pdf", "documents": "['1910.11204.pdf', '1912.00864.pdf', '1712.00991.pdf', '1905.07464.pdf', '1910.06592.pdf', '1709.05413.pdf', '1911.13066.pdf', '1608.06757.pdf', '1809.03449.pdf', '1902.10525.pdf', '1909.13375.pdf']"}
{"_id": "paper_tab_155", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the paper include macro F1 for the BERT model\u2019s performance on the test set for classifying emotions in the annotated poetry dataset?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]", "main_doc": "2003.07723.pdf", "documents": "['2003.07723.pdf', '1904.10500.pdf', '1909.03242.pdf', '1806.07711.pdf', '2002.00652.pdf', '1603.07044.pdf', '1605.07333.pdf']"}
{"_id": "paper_tab_156", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the UAS (Unlabeled Attachment Score) and LAS (Labeled Attachment Score) achieved by the proposed non-local transition-based parser compared to the strongest state-of-the-art greedy and overall parsers on the PT-SD dataset?", "answer": "[{'answer': 'Proposed method achieves 94.5 UAS and 92.4 LAS  compared to 94.3 and 92.2 of best state-of-the -art greedy based parser. Best state-of-the art parser overall achieves 95.8 UAS and 94.6 LAS.', 'type': 'abstractive'}]", "main_doc": "1710.09340.pdf", "documents": "['1710.09340.pdf', '1909.00754.pdf', '1910.11204.pdf', '1909.07734.pdf', '1908.07195.pdf', '1911.10049.pdf', '1606.05320.pdf', '1812.10479.pdf', '1910.06748.pdf', '1910.08210.pdf', '1910.14497.pdf']"}
{"_id": "paper_tab_157", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the 7 benchmark datasets that were used to evaluate sarcasm and irony detection across Tweets, Reddit posts, and online debates in the study on deep contextualized word representations?", "answer": "[{'answer': 'SemEval 2018 Task 3, BIBREF20, BIBREF4, SARC 2.0, SARC 2.0 pol, Sarcasm Corpus V1 (SC-V1), Sarcasm Corpus V2 (SC-V2)', 'type': 'extractive'}]", "main_doc": "1809.09795.pdf", "documents": "['1809.09795.pdf', '1711.02013.pdf', '1912.01214.pdf', '1909.06937.pdf', '1712.03556.pdf']"}
{"_id": "paper_tab_158", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the differences in recall scores between the proposed BERT-Multilingual, Bi-GRU, CRF-based model and traditional NER systems such as Stanford, SpaCy, and Flair when evaluated using both Traditional NERs and Wikipedia titles as references?", "answer": "[{'answer': 'Between the model and Stanford, Spacy and Flair the differences are 42.91, 25.03, 69.8 with Traditional NERs as reference and  49.88, 43.36, 62.43 with Wikipedia titles as reference.', 'type': 'abstractive'}]", "main_doc": "2002.11402.pdf", "documents": "['2002.11402.pdf', '1805.03710.pdf', '1909.07734.pdf', '1912.01673.pdf', '2002.04181.pdf']"}
{"_id": "paper_tab_159", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the F1 score and recall values reported when comparing your BERT-CRF model's performance on detecting case-less n-grams against traditional Named Entity Recognition systems?", "answer": "[{'answer': 'F1 score and Recall are 68.66, 80.08 with Traditional NERs as reference and 59.56, 69.76 with Wikipedia titles as reference.', 'type': 'abstractive'}]", "main_doc": "2002.11402.pdf", "documents": "['2002.11402.pdf', '1707.08559.pdf', '2003.12738.pdf', '1809.06537.pdf', '2002.01664.pdf', '2003.03106.pdf', '1810.12085.pdf', '1903.09722.pdf', '1701.00185.pdf', '1603.04513.pdf']"}
{"_id": "paper_tab_160", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the accuracy difference between the interpretable system leveraging edit distance and cosine similarity with semantic vectors, and the LSTM model enhanced with ELMo embeddings for spelling correction on the PlEWi dataset?", "answer": "[{'answer': 'Accuracy of best interpretible system was 0.3945 while accuracy of LSTM-ELMo net was 0.6818.', 'type': 'abstractive'}]", "main_doc": "1905.10810.pdf", "documents": "['1905.10810.pdf', '1910.08987.pdf', '2001.08868.pdf', '1809.06537.pdf']"}
{"_id": "paper_tab_161", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Are the experimental datasets used in the paper's proposed emoji-based sentiment analysis exclusively from English-language tweets?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}]", "main_doc": "1807.07961.pdf", "documents": "['1807.07961.pdf', '1906.11180.pdf', '1909.05855.pdf', '2003.12738.pdf', '1909.00694.pdf', '1901.05280.pdf', '2001.05493.pdf']"}
{"_id": "paper_tab_162", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the F1 scores for the SR, HATE, and HAR datasets achieved by the proposed model, as reported in the paper on predictive embeddings for hate speech detection?", "answer": "[{'answer': 'Proposed model achieves 0.86, 0.924, 0.71 F1 score on SR, HATE, HAR datasets respectively.', 'type': 'abstractive'}]", "main_doc": "1809.10644.pdf", "documents": "['1809.10644.pdf', '1911.03310.pdf', '1808.09029.pdf', '2004.03354.pdf', '1711.02013.pdf', '1909.00279.pdf', '1902.09314.pdf', '1704.00939.pdf']"}
{"_id": "paper_tab_163", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the highest classification accuracy reported for a model trained on the combined original and best generated dataset, with the generated datasets filtered at a threshold of 0.6?", "answer": "[{'answer': '82.0%', 'type': 'abstractive'}]", "main_doc": "1607.06025.pdf", "documents": "['1607.06025.pdf', '1909.01247.pdf', '1910.05154.pdf', '1606.05320.pdf', '1908.06264.pdf', '1709.10367.pdf']"}
{"_id": "paper_tab_164", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the three bias evaluation metrics that show reduction when applying the probabilistic bias mitigation method, as indicated by the cosine distance measurements in the experimental results?", "answer": "[{'answer': 'RIPA, Neighborhood Metric, WEAT', 'type': 'abstractive'}]", "main_doc": "1910.14497.pdf", "documents": "['1910.14497.pdf', '1910.11204.pdf', '1804.11346.pdf', '1711.02013.pdf', '2003.07996.pdf', '1701.05574.pdf', '1901.02262.pdf', '1909.01958.pdf', '1705.00108.pdf', '1909.00578.pdf', '1804.07789.pdf', '1810.00663.pdf', '1909.00279.pdf']"}
{"_id": "paper_tab_165", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the state-of-the-art accuracy achieved by the proposed CAS-LSTM architecture on the SNLI benchmark, and how does it leverage the fusion of hidden and memory cell states to outperform existing models?", "answer": "[{'answer': 'In SNLI, our best model achieves the new state-of-the-art accuracy of 87.0%,  we can see that our models outperform other models by large margin, achieving the new state of the art., Our models achieve the new state-of-the-art accuracy on SST-2 and competitive accuracy on SST-5', 'type': 'extractive'}, {'answer': 'accuracy of 87.0%', 'type': 'extractive'}]", "main_doc": "1809.02279.pdf", "documents": "['1809.02279.pdf', '1901.04899.pdf', '1606.05320.pdf', '2002.11402.pdf', '1611.00514.pdf', '1911.08962.pdf', '2002.01664.pdf', '1703.02507.pdf']"}
{"_id": "paper_tab_166", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What system achieved the best F-score in sarcasm detection after incorporating cognitive features from eye-tracking data with traditional features?", "answer": "[{'answer': 'Gaze Sarcasm using Multi Instance Logistic Regression.', 'type': 'abstractive'}, {'answer': 'the MILR classifier', 'type': 'extractive'}]", "main_doc": "1701.05574.pdf", "documents": "['1701.05574.pdf', '1909.11687.pdf', '2002.02492.pdf', '1911.05153.pdf', '1906.10225.pdf', '1908.06151.pdf', '1908.11047.pdf', '1910.06036.pdf']"}
{"_id": "paper_tab_167", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What specific eye-movement-based cognitive features, including fixation and saccade metrics, are listed as contributing to the sarcasm detection model in this paper?", "answer": "[{'answer': 'Readability (RED),  Number of Words (LEN), Avg. Fixation Duration (FDUR), Avg. Fixation Count (FC), Avg. Saccade Length (SL), Regression Count (REG), Skip count (SKIP), Count of regressions from second half\\nto first half of the sentence (RSF), Largest Regression Position (LREG),  Edge density of the saliency gaze\\ngraph (ED),  Fixation Duration at Left/Source\\n(F1H, F1S),  Fixation Duration at Right/Target\\n(F2H, F2S),  Forward Saccade Word Count of\\nSource (PSH, PSS),  Forward SaccadeWord Count of Destination\\n(PSDH, PSDS), Regressive Saccade Word Count of\\nSource (RSH, RSS),  Regressive Saccade Word Count of\\nDestination (RSDH, RSDS)', 'type': 'abstractive'}]", "main_doc": "1701.05574.pdf", "documents": "['1701.05574.pdf', '1909.00361.pdf', '1809.09795.pdf', '2002.04181.pdf', '1902.09393.pdf']"}
{"_id": "paper_tab_168", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How does Go-Explore's phase 1 exploration compare to DQN++ and DRQN++ in terms of interaction efficiency and trajectory optimality when solving the CoinCollector text-based game?", "answer": "[{'answer': ' On the other hand, phase 1 of Go-Explore finds an optimal trajectory with approximately half the interactions with the environment, Moreover, the trajectory length found by Go-Explore is always optimal (i.e. 30 steps) whereas both DQN++ and DRQN++ have an average length of 38 and 42 respectively., Especially interesting is that the performance of DRRN is substantially lower than that of the Go-Explore Seq2Seq model', 'type': 'extractive'}, {'answer': 'On Coin Collector, proposed model finds shorter path in fewer number of interactions with enironment.\\nOn Cooking World, proposed model uses smallest amount of steps and on average has bigger score and number of wins by significant margin.', 'type': 'abstractive'}]", "main_doc": "2001.08868.pdf", "documents": "['2001.08868.pdf', '1605.08675.pdf', '1701.05574.pdf', '1912.10435.pdf', '1910.14497.pdf', '1603.07044.pdf', '1909.11467.pdf', '1709.10217.pdf', '1908.07195.pdf', '1908.11365.pdf', '1904.03288.pdf', '1706.08032.pdf', '2002.02492.pdf', '1703.07090.pdf', '1808.09029.pdf']"}
{"_id": "paper_tab_169", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the F1 score improvements of the LiLi model over the \"Single\" model on the Freebase and WordNet datasets, specifically for known, unknown, and all relation types?", "answer": "[{'answer': 'In case of Freebase knowledge base, LiLi model had better F1 score than the single model by 0.20 , 0.01, 0.159 for kwn, unk, and all test Rel type.  The values for WordNet are 0.25, 0.1, 0.2. \\n', 'type': 'abstractive'}]", "main_doc": "1802.06024.pdf", "documents": "['1802.06024.pdf', '1911.10049.pdf', '1805.03710.pdf', '1612.05270.pdf', '1909.11467.pdf', '1910.08987.pdf', '1909.07734.pdf', '1810.05241.pdf', '1909.09270.pdf', '1912.10011.pdf', '1909.00754.pdf', '1906.05474.pdf', '1709.05413.pdf', '1910.11204.pdf']"}
{"_id": "paper_tab_170", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the specific BABEL languages used for training the multilingual sequence-to-sequence model and the target languages for transfer learning experiments as described in the paper?", "answer": "[{'answer': 'Train languages are: Cantonese, Bengali, Pashto, Turkish, Vietnamese, Haitian, Tamil, Kurdish, Tokpisin and Georgian, while Assamese, Tagalog, Swahili, Lao are used as target languages.', 'type': 'abstractive'}]", "main_doc": "1810.03459.pdf", "documents": "['1810.03459.pdf', '2003.04642.pdf', '1709.10217.pdf', '1910.07481.pdf']"}
{"_id": "paper_tab_171", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How do the authors simulate Wernicke\u2019s and Broca\u2019s aphasia in the LLA-LSTM model by damaging neural modules in the paper?", "answer": "[{'answer': 'Damage to neural modules is done by randomly initializing their weights, causing the loss of all learned information.', 'type': 'abstractive'}]", "main_doc": "2002.08899.pdf", "documents": "['2002.08899.pdf', '1804.05918.pdf', '1910.12129.pdf', '1910.05154.pdf', '1908.07245.pdf', '1707.05236.pdf']"}
{"_id": "paper_tab_172", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How many tweets are annotated in the OLID dataset, which focuses on the distribution of label combinations for categorizing offensive content by type and target?", "answer": "[{'answer': '14,100 tweets', 'type': 'abstractive'}, {'answer': 'Dataset contains total of 14100 annotations.', 'type': 'abstractive'}]", "main_doc": "1902.09666.pdf", "documents": "['1902.09666.pdf', '2002.01359.pdf', '1801.05147.pdf', '1806.04330.pdf', '1912.08960.pdf', '1809.10644.pdf', '1809.02286.pdf', '1908.08345.pdf', '1910.14497.pdf', '1802.06024.pdf', '1811.02906.pdf', '1908.10084.pdf', '1703.07090.pdf', '1709.10367.pdf', '1810.00663.pdf']"}
{"_id": "paper_tab_173", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How many tweets are annotated at each hierarchical level (A, B, and C) in the OLID dataset in the paper on identifying the type and target of offensive posts in social media?", "answer": "[{'answer': 'Level A: 14100 Tweets\\nLevel B: 4640 Tweets\\nLevel C: 4089 Tweets', 'type': 'abstractive'}]", "main_doc": "1902.09666.pdf", "documents": "['1902.09666.pdf', '1908.07245.pdf', '1908.06083.pdf', '1902.00330.pdf', '1910.13215.pdf', '1909.11687.pdf', '1909.09270.pdf', '1911.05153.pdf', '2002.01664.pdf', '1909.06937.pdf', '1909.08859.pdf', '1911.08673.pdf', '2002.10361.pdf']"}
{"_id": "paper_tab_174", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which models are compared to the proposed method, where BLEU scores and road exam results are reported for the EMNLP2017 WMT News dataset?", "answer": "[{'answer': 'TEACHER FORCING (TF), SCHEDULED SAMPLING (SS),  SEQGAN, RANKGAN, LEAKGAN.', 'type': 'abstractive'}]", "main_doc": "1910.11235.pdf", "documents": "['1910.11235.pdf', '1701.05574.pdf', '1704.00939.pdf', '1708.09609.pdf', '1809.02286.pdf', '1907.09369.pdf', '1805.04033.pdf', '1909.01383.pdf', '1909.03405.pdf', '2003.11645.pdf', '1903.09722.pdf', '1906.10551.pdf', '1604.00400.pdf']"}
{"_id": "paper_tab_175", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What were the number of drug labels and sentences in Test Set 1 and Test Set 2 used to evaluate the performance of your system in the TAC 2018 track for drug-drug interaction extraction from structured product labels?", "answer": "[{'answer': 'Test set 1 contained 57 drug labels and 8208 sentences and test set 2 contained 66 drug labels and 4224 sentences', 'type': 'abstractive'}]", "main_doc": "1905.07464.pdf", "documents": "['1905.07464.pdf', '1711.02013.pdf', '1804.07789.pdf', '1904.10503.pdf', '1906.01081.pdf', '1909.09270.pdf', '1908.05828.pdf', '1912.01214.pdf', '1912.00864.pdf']"}
{"_id": "paper_tab_176", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the GAN model using the Diehl-Martinez-Kamalu (DMK) loss function in this paper show state-of-the-art performance when predicting Airbnb booking success from listing descriptions, based on development accuracy and cross-entropy loss?", "answer": "[{'answer': 'No', 'type': 'boolean'}]", "main_doc": "1806.11432.pdf", "documents": "['1806.11432.pdf', '1912.08960.pdf', '1810.12196.pdf', '1710.09340.pdf', '1901.04899.pdf', '1902.00672.pdf', '2002.06675.pdf', '1711.02013.pdf', '1612.05270.pdf', '1911.08962.pdf', '1910.11204.pdf', '1612.08205.pdf']"}
{"_id": "paper_tab_177", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What GloVe-based baseline configurations, including ensembling and non-ensembling approaches, are compared in the study on using GANs to generate Airbnb listing descriptions with the DMK loss function?", "answer": "[{'answer': 'GloVe vectors trained on Wikipedia Corpus with ensembling, and GloVe vectors trained on Airbnb Data without ensembling', 'type': 'abstractive'}]", "main_doc": "1806.11432.pdf", "documents": "['1806.11432.pdf', '1812.06864.pdf', '1703.07090.pdf', '1909.01958.pdf', '1712.03547.pdf', '1909.00361.pdf', '1912.01772.pdf', '1905.11901.pdf', '1909.13375.pdf', '1809.02286.pdf', '2001.10161.pdf', '1901.04899.pdf', '1911.02821.pdf', '1909.03242.pdf', '1708.09609.pdf', '1910.06592.pdf', '1902.00330.pdf', '1910.11204.pdf']"}
{"_id": "paper_tab_178", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the exact numbers of questions in the CMRC 2018 and DRCD Chinese machine reading comprehension datasets, as listed in the evaluations of this paper?", "answer": "[{'answer': 'Evaluation datasets used:\\nCMRC 2018 - 18939 questions, 10 answers\\nDRCD - 33953 questions, 5 answers\\nNIST MT02/03/04/05/06/08 Chinese-English - Not specified\\n\\nSource language train data:\\nSQuAD - Not specified', 'type': 'abstractive'}]", "main_doc": "1909.00361.pdf", "documents": "['1909.00361.pdf', '1909.00430.pdf', '1911.12579.pdf', '1905.11901.pdf', '1901.02257.pdf', '1706.08032.pdf', '1807.07961.pdf', '1704.08960.pdf', '1612.08205.pdf', '2004.04721.pdf', '1707.00110.pdf', '1909.03242.pdf']"}
{"_id": "paper_tab_179", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What was the highest MRR score achieved by the UNCC system for Factoid questions in Batch 3 of the BioASQ Task-7B, Phase-B competition?", "answer": "[{'answer': '0.5115', 'type': 'abstractive'}, {'answer': '0.6103', 'type': 'extractive'}]", "main_doc": "2002.01984.pdf", "documents": "['2002.01984.pdf', '1909.00015.pdf', '1803.09230.pdf', '1905.10810.pdf', '1711.00106.pdf', '1809.00540.pdf', '1810.12885.pdf']"}
{"_id": "paper_tab_180", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How does the proposed comparative evaluator's Pearson correlation with human judgment in story generation and dialogue response tasks, at both the sample and model levels, compare to the next best automated evaluation metric?", "answer": "[{'answer': 'Pearson correlation to human judgement - proposed vs next best metric\\nSample level comparison:\\n- Story generation: 0.387 vs 0.148\\n- Dialogue: 0.472 vs 0.341\\nModel level comparison:\\n- Story generation:  0.631 vs 0.302\\n- Dialogue: 0.783 vs 0.553', 'type': 'abstractive'}]", "main_doc": "2002.05058.pdf", "documents": "['2002.05058.pdf', '1810.10254.pdf', '1902.09393.pdf', '1905.12260.pdf', '1911.13066.pdf', '1911.02711.pdf', '1703.06492.pdf', '2002.00652.pdf', '1908.07195.pdf', '2002.08899.pdf', '1908.06267.pdf']"}
{"_id": "paper_tab_181", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the specific improvements and slight degradations in automatic evaluation metrics for the proposed model compared to the baselines, especially for the Bleu1 scores on the Zhou split and DuSplit datasets?", "answer": "[{'answer': 'Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1', 'type': 'abstractive'}]", "main_doc": "1910.06036.pdf", "documents": "['1910.06036.pdf', '1809.04960.pdf', '1806.04330.pdf', '1902.09666.pdf', '1908.08345.pdf', '1807.07961.pdf', '1904.05584.pdf', '1910.03814.pdf', '1910.02339.pdf', '2003.11563.pdf', '1705.00108.pdf', '1912.01772.pdf', '1906.01081.pdf']"}
{"_id": "paper_tab_182", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How many dialogs are in the training, validation, and test sets for Task 6, derived from the 2nd Dialog State Tracking Challenge, as reported in the Learning End-to-End Goal-Oriented Dialog paper?", "answer": "[{'answer': '1,618 training dialogs, 500 validation dialogs, and 1,117 test dialogs', 'type': 'abstractive'}]", "main_doc": "1605.07683.pdf", "documents": "['1605.07683.pdf', '1908.06379.pdf', '1710.06700.pdf', '2004.01878.pdf']"}
{"_id": "paper_tab_183", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the empirical evaluation of the S-EFE model, which includes datasets like U.S. Senate speeches, ArXiv papers, and grocery purchases, involve only English-language content?", "answer": "[{'answer': 'No', 'type': 'boolean'}]", "main_doc": "1709.10367.pdf", "documents": "['1709.10367.pdf', '1909.08041.pdf', '1910.10288.pdf', '1905.10810.pdf', '1611.04798.pdf']"}
{"_id": "paper_tab_184", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the paper introducing the Knowledge Aided Reader (KAR) suggest that humans' robustness to noise in comprehension tasks stems from their ability to use general knowledge, such as the semantic connections extracted via WordNet?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]", "main_doc": "1809.03449.pdf", "documents": "['1809.03449.pdf', '1611.03382.pdf', '1808.09920.pdf', '1904.07904.pdf', '1807.07961.pdf', '1901.05280.pdf', '1809.05752.pdf', '1605.07333.pdf', '1906.03538.pdf', '1903.09722.pdf', '1907.03060.pdf', '1912.01214.pdf']"}
{"_id": "paper_tab_185", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which specific ShapeWorldICE datasets, including OneShape, MultiShapes, TwoShapes, Count, and Ratio, are described in the paper for evaluating model performance?", "answer": "[{'answer': 'Existential (OneShape, MultiShapes), Spacial (TwoShapes, Multishapes), Quantification (Count, Ratio) datasets are generated from ShapeWorldICE', 'type': 'abstractive'}, {'answer': 'ShapeWorldICE datasets: OneShape, MultiShapes, TwoShapes, MultiShapes, Count, and Ratio', 'type': 'abstractive'}]", "main_doc": "1912.08960.pdf", "documents": "['1912.08960.pdf', '1909.06162.pdf', '2002.02492.pdf', '1812.01704.pdf', '1810.05241.pdf', '1909.03544.pdf', '1909.00430.pdf', '1911.13066.pdf', '1707.03569.pdf', '1910.08210.pdf', '1711.02013.pdf', '1704.00939.pdf', '1911.03597.pdf', '1805.03710.pdf', '1911.02711.pdf', '1904.03288.pdf', '1611.03382.pdf']"}
{"_id": "paper_tab_186", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How does the data demonstrate that mBERT's language-neutral component leads to better word-alignment performance than FastAlign, even when using large parallel corpora, and what role does explicit projection learning play in this outcome?", "answer": "[{'answer': 'Table TABREF15 shows that word-alignment based on mBERT representations surpasses the outputs of the standard FastAlign tool even if it was provided large parallel corpus. This suggests that word-level semantics are well captured by mBERT contextual embeddings. For this task, learning an explicit projection had a negligible effect on the performance.', 'type': 'extractive'}, {'answer': 'explicit projection had a negligible effect on the performance', 'type': 'extractive'}]", "main_doc": "1911.03310.pdf", "documents": "['1911.03310.pdf', '2003.01769.pdf', '1909.09587.pdf', '1902.09393.pdf', '1809.04960.pdf']"}
{"_id": "paper_tab_187", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the system trained using expectation regularization (XR) with sentence-level (S) and noisy sentence-level (N) data outperform the fully supervised neural system trained solely with aspect-level data (A) in terms of accuracy and Macro-F1 score for Aspect-based Sentiment Classification?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}]", "main_doc": "1909.00430.pdf", "documents": "['1909.00430.pdf', '2001.00137.pdf', '1904.10500.pdf', '1903.09722.pdf', '2004.01980.pdf', '1908.11365.pdf', '2003.03044.pdf']"}
{"_id": "paper_tab_188", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the reported accuracy values for the BiLSTM-XR and BiLSTM-XR-Dev Estimation models, trained using only XR loss on the SemEval-15 and SemEval-16 datasets, in the study on aspect-based sentiment classification using Sentence-level, Noisy sentence-level, and Aspect-level data?", "answer": "[{'answer': 'BiLSTM-XR-Dev Estimation accuracy is 83.31 for SemEval-15 and 87.68 for SemEval-16.\\nBiLSTM-XR accuracy is 83.31 for SemEval-15 and 88.12 for SemEval-16.\\n', 'type': 'abstractive'}]", "main_doc": "1909.00430.pdf", "documents": "['1909.00430.pdf', '1811.12254.pdf', '1911.02086.pdf', '1910.00458.pdf', '1905.12260.pdf', '2003.12218.pdf', '1905.11901.pdf', '1904.10503.pdf', '1810.09774.pdf', '1805.03710.pdf', '2001.05467.pdf', '1701.03214.pdf', '1909.03405.pdf', '1712.00991.pdf', '1909.00361.pdf', '1910.11204.pdf', '1912.08960.pdf', '1609.00559.pdf']"}
{"_id": "paper_tab_189", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What specific speaker verification systems, including adaptations and fine-tunings of VoxCeleb-based models, are evaluated for their % EER on BULATS and Linguaskill test sets?", "answer": "[{'answer': 'BULATS i-vector/PLDA\\nBULATS x-vector/PLDA\\nVoxCeleb x-vector/PLDA\\nPLDA adaptation (X1)\\n Extractor fine-tuning (X2) ', 'type': 'abstractive'}]", "main_doc": "1909.13695.pdf", "documents": "['1909.13695.pdf', '1912.06670.pdf', '1810.12885.pdf', '1912.00864.pdf', '2002.01359.pdf', '1810.12196.pdf', '2004.01878.pdf', '1804.07789.pdf', '2003.12738.pdf', '2004.03744.pdf', '1902.09393.pdf', '1911.02711.pdf']"}
{"_id": "paper_tab_190", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which models are directly compared with the authors' ELMo and Wikidata-based approach for fine-grained entity recognition using the Wiki(gold) dataset?", "answer": "[{'answer': 'Akbik et al. (2018), Link et al. (2012)', 'type': 'abstractive'}, {'answer': 'They compare to Akbik et al. (2018) and Link et al. (2012).', 'type': 'abstractive'}]", "main_doc": "1904.10503.pdf", "documents": "['1904.10503.pdf', '2003.11645.pdf', '1909.04002.pdf', '1809.02286.pdf', '1707.00110.pdf', '1906.03538.pdf', '2001.00137.pdf', '1606.05320.pdf', '2002.01359.pdf', '2003.07996.pdf', '2001.08051.pdf', '2001.08868.pdf', '1909.01958.pdf', '1910.06592.pdf', '1909.00279.pdf', '1810.10254.pdf', '1911.08673.pdf', '2003.05377.pdf']"}
{"_id": "paper_tab_191", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the results reported for the paragraph-level neural network model trained on the PDTB corpus, which explicit discourse relations demonstrate the highest and lowest classification performance?", "answer": "[{'answer': 'explicit discourse relations', 'type': 'extractive'}, {'answer': 'Best: Expansion (Exp). Worst: Comparison (Comp).', 'type': 'abstractive'}]", "main_doc": "1804.05918.pdf", "documents": "['1804.05918.pdf', '1902.10525.pdf', '1909.13714.pdf', '1704.05907.pdf', '2003.05377.pdf', '1905.12260.pdf', '1910.03814.pdf', '1909.05855.pdf', '1905.06566.pdf', '1910.11769.pdf', '1809.00530.pdf', '1606.05320.pdf', '1603.00968.pdf', '1804.00079.pdf', '1902.09314.pdf', '1910.12129.pdf']"}
{"_id": "paper_tab_192", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the paper provide a comparison between hierarchical models like HiBERT and non-hierarchical models such as BERT, evaluated as baselines on the CNNDM dataset?", "answer": "[{'answer': 'There were hierarchical and non-hierarchical baselines; BERT was one of those baselines', 'type': 'abstractive'}]", "main_doc": "1905.06566.pdf", "documents": "['1905.06566.pdf', '1909.13375.pdf', '2002.01664.pdf', '1910.00825.pdf', '1910.06592.pdf', '1901.03866.pdf', '1908.11047.pdf', '1809.02286.pdf', '1911.03310.pdf']"}
{"_id": "paper_tab_193", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the percentage improvements in F1 score and accuracy for the top-performing transfer learning strategy in Task 1 in the paper on BiLSTM-CNN neural networks and background knowledge transfer for offensive language detection on German Twitter data?", "answer": "[{'answer': 'In task 1 best transfer learning strategy improves F1 score by 4.4% and accuracy score by 3.3%, in task 2 best transfer learning strategy improves F1 score by 2.9% and accuracy score by 1.7%', 'type': 'abstractive'}]", "main_doc": "1811.02906.pdf", "documents": "['1811.02906.pdf', '1910.06036.pdf', '1912.01214.pdf', '1612.08205.pdf', '1910.07481.pdf', '1909.13375.pdf', '1908.08345.pdf']"}
{"_id": "paper_tab_194", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the F-score, AUC, and mean accuracy values comparing the unimodal LSTM model and the best multimodal model (FCM) for hate speech detection on the MMHS150K dataset?", "answer": "[{'answer': 'Unimodal LSTM vs Best Multimodal (FCM)\\n- F score: 0.703 vs 0.704\\n- AUC: 0.732 vs 0.734 \\n- Mean Accuracy: 68.3 vs 68.4 ', 'type': 'abstractive'}]", "main_doc": "1910.03814.pdf", "documents": "['1910.03814.pdf', '1705.01265.pdf', '2003.03044.pdf', '1809.01202.pdf', '1905.06566.pdf', '1910.12129.pdf', '1607.06025.pdf', '1704.05907.pdf', '1908.07195.pdf', '1908.08345.pdf', '1909.03405.pdf', '1711.11221.pdf', '1905.00563.pdf', '1910.11204.pdf', '1804.08139.pdf']"}
{"_id": "paper_tab_195", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What recall scores for positions 1, 2, and 5 were reported for the introduced method when evaluated with 10 candidates in the comparison of different models?", "answer": "[{'answer': 'Their model resulted in values of 0.476, 0.672 and 0.893 for recall at position 1,2 and 5 respectively in 10 candidates.', 'type': 'abstractive'}]", "main_doc": "1808.03430.pdf", "documents": "['1808.03430.pdf', '1909.11687.pdf', '1909.00175.pdf', '1910.06592.pdf', '1909.00430.pdf', '1605.07683.pdf', '1704.05907.pdf']"}
{"_id": "paper_tab_196", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Apart from entity tagging, what combined metrics for intent and entities are reported in HERMIT NLU's evaluation results?", "answer": "[{'answer': 'We also report the metrics in BIBREF7 for consistency, we report the span F1,  Exact Match (EM) accuracy of the entire sequence of labels, metric that combines intent and entities', 'type': 'extractive'}]", "main_doc": "1910.00912.pdf", "documents": "['1910.00912.pdf', '1909.03405.pdf', '1605.07683.pdf', '1910.02339.pdf', '1908.06267.pdf']"}
{"_id": "paper_tab_197", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which MRC gold standards published between 2016 and 2019, meeting the citation-based thresholds and aligned with the problem definition in this study, were analyzed for features like lexical ambiguity, factual correctness, and the presence of lexical cues as part of their answer selection styles?", "answer": "[{'answer': 'fit our problem definition and were published in the years 2016 to 2019, have at least $(2019 - publication\\\\ year) \\\\times 20$ citations', 'type': 'extractive'}, {'answer': 'MSMARCO,  HOTPOTQA, RECORD,  MULTIRC, NEWSQA, and DROP.', 'type': 'abstractive'}]", "main_doc": "2003.04642.pdf", "documents": "['2003.04642.pdf', '1910.03814.pdf', '2003.03014.pdf', '1609.00559.pdf', '1611.00514.pdf', '2004.03354.pdf', '1909.01013.pdf', '1810.00663.pdf', '1910.03467.pdf']"}
{"_id": "paper_tab_198", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "According to the corpus statistics presented in the Sindhi word embeddings study, how many unique tokens were retained in the final cleaned vocabulary after preprocessing?", "answer": "[{'answer': '908456 unique words are available in collected corpus.', 'type': 'abstractive'}]", "main_doc": "1911.12579.pdf", "documents": "['1911.12579.pdf', '1910.11769.pdf', '1807.07961.pdf', '1904.05584.pdf', '1704.08960.pdf', '1904.07904.pdf', '1904.10500.pdf', '1701.00185.pdf', '1803.09230.pdf', '1808.09029.pdf', '1910.04269.pdf', '1701.03214.pdf', '1908.10084.pdf', '1910.06036.pdf', '1908.07195.pdf', '1910.00825.pdf', '1804.05918.pdf']"}
{"_id": "paper_tab_199", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How many distinct clinical phenotypes were manually annotated in the dataset of Discharge Summaries and Nursing Progress Notes, focusing on the risk of recurrent ICU readmissions?", "answer": "[{'answer': '15 clinical patient phenotypes', 'type': 'extractive'}, {'answer': 'Thirteen different phenotypes are present in the dataset.', 'type': 'abstractive'}]", "main_doc": "2003.03044.pdf", "documents": "['2003.03044.pdf', '1911.02821.pdf', '1912.10806.pdf', '1809.05752.pdf']"}
{"_id": "paper_tab_200", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How do the BLEU scores vary based on different values of K and sequence lengths, and how do these scores compare to the baseline models with and without attention?", "answer": "[{'answer': 'Ranges from 44.22 to 100.00 depending on K and the sequence length.', 'type': 'abstractive'}]", "main_doc": "1707.00110.pdf", "documents": "['1707.00110.pdf', '2002.05058.pdf', '1908.07245.pdf', '1908.06151.pdf', '1809.08298.pdf', '2001.05493.pdf', '1710.06700.pdf', '1905.11901.pdf']"}
{"_id": "paper_tab_201", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What baseline models are listed for comparison in the Jasper paper when evaluating performance on the Hub5'00 conversational dataset?", "answer": "[{'answer': 'LF-MMI Attention\\nSeq2Seq \\nRNN-T \\nChar E2E LF-MMI \\nPhone E2E LF-MMI \\nCTC + Gram-CTC', 'type': 'abstractive'}]", "main_doc": "1904.03288.pdf", "documents": "['1904.03288.pdf', '1805.03710.pdf', '2004.04721.pdf', '1802.06024.pdf', '1909.00430.pdf', '1803.09230.pdf', '1912.01673.pdf']"}
{"_id": "paper_tab_202", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the WER scores achieved by Jasper's best model on the WSJ nov93/nov92 subsets and the Hub5'00 dataset\u2019s SWB and CHM subsets, and how do these results compare to the current state-of-the-art?", "answer": "[{'answer': 'In case of read speech datasets,  their best model got the highest nov93 score of 16.1 and the highest nov92 score of 13.3.\\nIn case of Conversational Speech, their best model got the highest SWB of 8.3 and the highest CHM of 19.3. ', 'type': 'abstractive'}, {'answer': \"On WSJ datasets author's best approach achieves 9.3 and 6.9 WER compared to best results of 7.5 and 4.1 on nov93 and nov92 subsets.\\nOn Hub5'00 datasets author's best approach achieves WER of 7.8 and 16.2 compared to best result of 7.3 and 14.2 on Switchboard (SWB) and Callhome (CHM) subsets.\", 'type': 'abstractive'}]", "main_doc": "1904.03288.pdf", "documents": "['1904.03288.pdf', '2003.06044.pdf', '1911.03310.pdf', '1610.07809.pdf', '2002.01984.pdf', '1904.10500.pdf', '1908.07816.pdf', '1807.07961.pdf', '1612.08205.pdf', '2004.01878.pdf', '1912.03457.pdf', '1804.05918.pdf', '1911.03597.pdf']"}
{"_id": "paper_tab_203", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "From which 31 languages are evaluated for token-level recall in the proposed conversion from Universal Dependencies tags to UniMorph tags?", "answer": "[{'answer': 'Ar, Bg, Ca, Cs, Da, De, En, Es, Eu, Fa, Fi, Fr, Ga, He, Hi, Hu, It, La, Lt, Lv, Nb, Nl, Nn, PL, Pt, Ro, Ru, Sl, Sv, Tr, Uk, Ur', 'type': 'abstractive'}, {'answer': 'We apply this conversion to the 31 languages, Arabic, Hindi, Lithuanian, Persian, and Russian. , Dutch, Spanish', 'type': 'extractive'}]", "main_doc": "1810.06743.pdf", "documents": "['1810.06743.pdf', '1811.01088.pdf', '1908.08345.pdf', '1905.12260.pdf', '1712.03556.pdf']"}
{"_id": "paper_tab_204", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "In the ablation study results for the non-hierarchical MPAD model, performed on the Reuters, Polarity, and IMDB datasets, which modification resulted in the smallest performance change compared to the baseline?", "answer": "[{'answer': 'Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets.', 'type': 'abstractive'}]", "main_doc": "1908.06267.pdf", "documents": "['1908.06267.pdf', '1909.00578.pdf', '1910.11235.pdf', '1709.05413.pdf', '1909.08859.pdf', '1710.09340.pdf', '1611.00514.pdf', '1706.08032.pdf', '1608.06757.pdf', '1912.08960.pdf']"}
{"_id": "paper_tab_205", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How does increasing the number of message passing iterations and removing the master node affect the classification performance of the non-hierarchical MPAD on the Reuters, Polarity, and IMDB datasets?", "answer": "[{'answer': 'Increasing number of message passing iterations showed consistent improvement in performance - around 1 point improvement compared between 1 and 4 iterations', 'type': 'abstractive'}, {'answer': 'Removing the master node deteriorates performance across all datasets', 'type': 'extractive'}]", "main_doc": "1908.06267.pdf", "documents": "['1908.06267.pdf', '1908.05828.pdf', '1912.08960.pdf', '1911.07555.pdf', '1912.00864.pdf', '1904.10503.pdf', '1910.12129.pdf', '1911.01680.pdf', '1605.08675.pdf', '2003.03044.pdf', '1908.06379.pdf', '2003.03106.pdf', '2001.06888.pdf', '2001.05467.pdf', '1810.00663.pdf']"}
{"_id": "paper_tab_206", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the dataset breakdown, how many annotated data points are provided for SemEval 2015, SemEval 2016, TASS 2015, and SENTIPOLC 2014 in the study?", "answer": "[{'answer': \"Total number of annotated data:\\nSemeval'15: 10712\\nSemeval'16: 28632\\nTass'15: 69000\\nSentipol'14: 6428\", 'type': 'abstractive'}]", "main_doc": "1612.05270.pdf", "documents": "['1612.05270.pdf', '1911.01799.pdf', '1901.04899.pdf', '1905.06566.pdf', '1909.03242.pdf', '1909.11297.pdf', '1810.10254.pdf', '1704.08960.pdf', '1911.04952.pdf']"}
{"_id": "paper_tab_207", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "In the paper analyzing B4MSA for multilingual sentiment classification, which languages, outside of SemEval, TASS, and SENTIPOLC, did B4MSA demonstrate superior performance compared to the reported results?", "answer": "[{'answer': 'Arabic, German, Portuguese, Russian, Swedish', 'type': 'abstractive'}]", "main_doc": "1612.05270.pdf", "documents": "['1612.05270.pdf', '1904.03288.pdf', '1909.13714.pdf', '2003.03044.pdf', '1809.09194.pdf', '1912.10806.pdf', '1707.05236.pdf', '1810.06743.pdf', '1901.02257.pdf', '2003.04642.pdf', '1901.03866.pdf', '1808.03430.pdf']"}
{"_id": "paper_tab_208", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the state-of-the-art dependency parsing metrics (UAS/LAS) and constituent parsing F1 scores achieved on the PTB and CTB datasets in the paper on Concurrent Parsing of Constituency and Dependency?", "answer": "[{'answer': '. On PTB, our model achieves 93.90 F1 score of constituent parsing and 95.91 UAS and 93.86 LAS of dependency parsing., On CTB, our model achieves a new state-of-the-art result on both constituent and dependency parsing.', 'type': 'extractive'}]", "main_doc": "1908.06379.pdf", "documents": "['1908.06379.pdf', '1911.00069.pdf', '1610.07809.pdf', '1909.00754.pdf', '1910.03467.pdf', '1908.11546.pdf', '2002.06644.pdf']"}
{"_id": "paper_tab_209", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the F1 scores achieved by the proposed LSTM-CRF model for Chinese NER on the DL-PS dataset (dialogue domain) and the EC-MT/EC-UQ datasets (e-commerce domain)?", "answer": "[{'answer': 'F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ', 'type': 'abstractive'}, {'answer': 'F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)', 'type': 'abstractive'}]", "main_doc": "1801.05147.pdf", "documents": "['1801.05147.pdf', '1907.09369.pdf', '1910.06592.pdf', '1711.02013.pdf', '1911.02821.pdf', '1910.12129.pdf', '1711.00106.pdf', '1908.07816.pdf', '2001.05493.pdf', '1909.05855.pdf', '1906.10551.pdf']"}
{"_id": "paper_tab_210", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the linear regression results, what is the exact correlation between the prevalence of #MeToo social media activity and official reports of sexual harassment?", "answer": "[{'answer': '0.9098 correlation', 'type': 'abstractive'}]", "main_doc": "2001.05970.pdf", "documents": "['2001.05970.pdf', '1905.07464.pdf', '1810.10254.pdf', '1907.09369.pdf', '1909.03242.pdf', '1711.02013.pdf', '1904.10503.pdf', '1901.02262.pdf', '2004.04721.pdf', '2003.01769.pdf', '1806.07711.pdf', '1909.06937.pdf', '1909.11467.pdf', '1909.09587.pdf']"}
{"_id": "paper_tab_211", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How do the experiments demonstrate the performance of the transformer-based multilingual paraphrasing model in terms of relevance and fluency, compared to the pivoting method?", "answer": "[{'answer': 'our method outperforms the baseline in both relevance and fluency significantly.', 'type': 'extractive'}]", "main_doc": "1911.03597.pdf", "documents": "['1911.03597.pdf', '1810.12196.pdf', '1911.03310.pdf', '1811.12254.pdf', '1910.00825.pdf', '1908.07816.pdf', '1902.09393.pdf', '1904.10500.pdf', '1605.07333.pdf', '1905.07464.pdf', '1904.10503.pdf', '1911.01680.pdf', '1806.07711.pdf', '1912.10011.pdf', '1704.08960.pdf']"}
{"_id": "paper_tab_212", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What accuracy is reported for the multi-view network (MVN) model on the Stanford Sentiment Treebank 5-class classification task?", "answer": "[{'answer': '51.5', 'type': 'abstractive'}]", "main_doc": "1704.05907.pdf", "documents": "['1704.05907.pdf', '1701.03214.pdf', '1605.07333.pdf', '1810.10254.pdf', '1701.06538.pdf', '1909.00175.pdf', '1908.06267.pdf', '1912.06670.pdf', '1910.08210.pdf']"}
{"_id": "paper_tab_213", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Are the results in the \"Emotion Detection in Text\" paper focused exclusively on datasets consisting of English text, including the new dataset tested on?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}]", "main_doc": "1907.09369.pdf", "documents": "['1907.09369.pdf', '1910.14537.pdf', '2002.11402.pdf', '1909.08859.pdf', '1910.06036.pdf', '1705.00108.pdf', '2002.00652.pdf', '1810.09774.pdf', '1802.06024.pdf', '1606.00189.pdf', '1704.08960.pdf', '1701.09123.pdf', '1809.03449.pdf', '1809.05752.pdf', '1905.06566.pdf']"}
{"_id": "paper_tab_214", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What languages were evaluated to measure the cross-lingual transfer learning performance on the XNLI development set, highlighting the impact of translation artifacts?", "answer": "[{'answer': 'English\\nFrench\\nSpanish\\nGerman\\nGreek\\nBulgarian\\nRussian\\nTurkish\\nArabic\\nVietnamese\\nThai\\nChinese\\nHindi\\nSwahili\\nUrdu\\nFinnish', 'type': 'abstractive'}, {'answer': 'English, Spanish, Finnish', 'type': 'extractive'}]", "main_doc": "2004.04721.pdf", "documents": "['2004.04721.pdf', '1605.08675.pdf', '1611.04798.pdf', '1810.09774.pdf', '1904.07904.pdf']"}
{"_id": "paper_tab_215", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What performance metric is used to evaluate the accuracy of models on the WIKIHOP closed test and public validation sets in the Entity-GCN paper?", "answer": "[{'answer': 'Accuracy', 'type': 'extractive'}]", "main_doc": "1808.09920.pdf", "documents": "['1808.09920.pdf', '1712.00991.pdf', '1805.04033.pdf', '1812.06705.pdf', '2004.01980.pdf', '2002.06675.pdf']"}
{"_id": "paper_tab_216", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the testing accuracies reported for the Entity-GCN model (with and without coreference) and the ensemble of 5 models on the WIKIHOP closed test set, in the paper that uses R-GCN for reasoning among entities without fine-tuning a language model?", "answer": "[{'answer': 'During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models', 'type': 'abstractive'}]", "main_doc": "1808.09920.pdf", "documents": "['1808.09920.pdf', '1910.11204.pdf', '1704.08960.pdf', '1911.10049.pdf', '1910.06592.pdf', '1910.11769.pdf']"}
{"_id": "paper_tab_217", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the twenty distinct lyrical topics identified through the LDA topic model from the corpus of 124,288 metal song lyrics?", "answer": "[{'answer': 'Table TABREF10 displays the twenty resulting topics', 'type': 'extractive'}]", "main_doc": "1911.04952.pdf", "documents": "['1911.04952.pdf', '1605.08675.pdf', '1902.00330.pdf', '1909.07734.pdf', '1904.10500.pdf', '1910.02339.pdf', '1907.03060.pdf']"}
{"_id": "paper_tab_218", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the EER(%) performance of the i-vector and x-vector systems trained on VoxCeleb and evaluated using the CN-Celeb evaluation dataset?", "answer": "[{'answer': 'ERR of 19.05 with i-vectors and 15.52 with x-vectors', 'type': 'abstractive'}]", "main_doc": "1911.01799.pdf", "documents": "['1911.01799.pdf', '1810.06743.pdf', '1909.08041.pdf', '1911.01680.pdf', '1908.08345.pdf', '1909.01958.pdf', '1701.02877.pdf', '1604.00400.pdf', '1804.08050.pdf', '1912.03457.pdf', '1901.08079.pdf', '2003.11563.pdf', '2003.12218.pdf', '1909.07734.pdf', '1607.06025.pdf', '1808.09029.pdf']"}
{"_id": "paper_tab_219", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the 11 genres listed in the CN-Celeb speaker recognition dataset for evaluating speaker recognition in unconstrained conditions?", "answer": "[{'answer': 'genre, entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement', 'type': 'abstractive'}]", "main_doc": "1911.01799.pdf", "documents": "['1911.01799.pdf', '1909.00754.pdf', '1911.04952.pdf', '1810.12885.pdf', '1804.08050.pdf']"}
{"_id": "paper_tab_220", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the results presented in the CN-Celeb paper, which model\u2014i-vector or x-vector\u2014showed better performance in terms of EER (%) when evaluated on the CN-Celeb dataset?", "answer": "[{'answer': 'x-vector', 'type': 'abstractive'}]", "main_doc": "1911.01799.pdf", "documents": "['1911.01799.pdf', '1908.06151.pdf', '1904.10503.pdf', '1909.07734.pdf', '1909.09270.pdf', '1911.05153.pdf', '1810.12085.pdf']"}
{"_id": "paper_tab_221", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "*According to the CN-Celeb paper, what is the percentage increase in EER for both the i-vector and x-vector systems when evaluated on CN-Celeb compared to VoxCeleb?*", "answer": "[{'answer': 'For i-vector system, performances are 11.75% inferior to voxceleb. For x-vector system, performances are 10.74% inferior to voxceleb', 'type': 'abstractive'}]", "main_doc": "1911.01799.pdf", "documents": "['1911.01799.pdf', '1910.07481.pdf', '1805.04033.pdf', '2002.00652.pdf', '1612.08205.pdf', '1902.00330.pdf', '2003.06044.pdf', '1901.05280.pdf', '1804.00079.pdf', '1704.05907.pdf', '2003.12218.pdf', '1911.03310.pdf', '1908.05828.pdf', '1804.05918.pdf', '1909.09587.pdf', '1712.03547.pdf', '1907.09369.pdf', '1910.00825.pdf']"}
{"_id": "paper_tab_222", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How effective are the consistent variants of nucleus and top-k sampling in reducing the non-termination ratio compared to baseline decoding algorithms?", "answer": "[{'answer': 'It eliminates non-termination in some models fixing for some models up to 6% of non-termination ratio.', 'type': 'abstractive'}]", "main_doc": "2002.02492.pdf", "documents": "['2002.02492.pdf', '1903.09588.pdf', '1804.11346.pdf', '1904.05584.pdf', '1609.00559.pdf', '2003.05377.pdf', '1612.08205.pdf', '1902.09393.pdf', '1810.05241.pdf', '1905.11901.pdf', '2003.12218.pdf', '1909.06937.pdf', '2002.11910.pdf', '2001.05493.pdf', '1901.05280.pdf']"}
{"_id": "paper_tab_223", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which models are listed as baselines in this paper for comparing Masque's performance on the MS MARCO V2 leaderboard?", "answer": "[{'answer': 'BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D', 'type': 'abstractive'}]", "main_doc": "1901.02262.pdf", "documents": "['1901.02262.pdf', '1908.07245.pdf', '1902.09666.pdf', '1806.04330.pdf', '1604.00400.pdf', '2003.04642.pdf', '2002.01207.pdf', '1908.10084.pdf', '1909.03405.pdf', '1909.01247.pdf', '1909.11467.pdf', '1910.12795.pdf', '1809.02286.pdf', '1906.01081.pdf', '2003.03106.pdf', '1712.03556.pdf', '1904.05584.pdf', '2002.06644.pdf']"}
{"_id": "paper_tab_224", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How does the Masque model perform on the NarrativeQA test set in terms of BLEU-1, BLEU-4, METEOR, and ROUGE-L scores?", "answer": "[{'answer': 'Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87', 'type': 'abstractive'}]", "main_doc": "1901.02262.pdf", "documents": "['1901.02262.pdf', '2002.02492.pdf', '1912.10435.pdf', '1612.05270.pdf', '1909.05855.pdf', '2001.05970.pdf', '1711.02013.pdf']"}
{"_id": "paper_tab_225", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the dataset summary, how many labeled data points are there for the datasets Book, Electronics, Beauty, Music, IMDB, Yelp, Cell Phone, and Baby?", "answer": "[{'answer': '719313', 'type': 'abstractive'}, {'answer': 'Book, Electronics, Beauty and Music each have 6000, IMDB 84919, Yelp 231163, Cell Phone 194792 and Baby 160792 labeled data.', 'type': 'abstractive'}]", "main_doc": "1809.00530.pdf", "documents": "['1809.00530.pdf', '1910.02339.pdf', '1909.06162.pdf', '1911.10049.pdf', '1804.05918.pdf', '1611.02550.pdf', '1707.00110.pdf', '1702.03342.pdf', '1901.04899.pdf', '1902.00672.pdf', '1811.12254.pdf', '2002.01664.pdf', '1908.06083.pdf', '2002.08899.pdf', '1809.09795.pdf']"}
{"_id": "paper_tab_226", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which specific domains were selected as the source and target for sentiment classification in this research?", "answer": "[{'answer': 'Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen', 'type': 'abstractive'}, {'answer': 'we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)', 'type': 'extractive'}]", "main_doc": "1809.00530.pdf", "documents": "['1809.00530.pdf', '1910.02339.pdf', '1808.09920.pdf', '2003.12218.pdf', '1904.01608.pdf', '1911.03597.pdf']"}
{"_id": "paper_tab_227", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which sentence embedding methods are evaluated alongside SBERT and SRoBERTa based on Spearman rank correlation for the STS tasks in the paper?", "answer": "[{'answer': 'GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent', 'type': 'abstractive'}, {'answer': 'Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder.', 'type': 'abstractive'}]", "main_doc": "1908.10084.pdf", "documents": "['1908.10084.pdf', '2002.06424.pdf', '1611.04798.pdf', '2003.12218.pdf']"}
{"_id": "paper_tab_228", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "According to the \"Dataset statistics\" section of the Nepali NER study, how many sentences are present in both the training and testing sets of the dataset?", "answer": "[{'answer': '3606', 'type': 'abstractive'}, {'answer': '6946', 'type': 'extractive'}]", "main_doc": "1908.05828.pdf", "documents": "['1908.05828.pdf', '1606.00189.pdf', '2002.01359.pdf', '1910.10288.pdf', '1910.03814.pdf', '2004.01878.pdf', '1612.08205.pdf', '1611.03382.pdf', '1909.11687.pdf', '1912.01772.pdf', '1809.04960.pdf', '1806.04330.pdf', '1909.13695.pdf', '2002.05058.pdf', '1809.02286.pdf', '1907.03060.pdf', '1909.00105.pdf']"}
{"_id": "paper_tab_229", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which baseline models are used to compare the proposed grapheme-level neural-based Nepali NER to other models for performance evaluation?", "answer": "[{'answer': 'CNN modelBIBREF0, Stanford CRF modelBIBREF21', 'type': 'extractive'}, {'answer': 'Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec', 'type': 'abstractive'}]", "main_doc": "1908.05828.pdf", "documents": "['1908.05828.pdf', '1705.01214.pdf', '1809.01541.pdf', '1910.08210.pdf', '1811.02906.pdf', '1909.07734.pdf', '1712.03556.pdf', '1910.11769.pdf', '1901.04899.pdf', '1809.10644.pdf', '1909.11687.pdf']"}
{"_id": "paper_tab_230", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the total number of sentences and entities in the ILPRL and OurNepali datasets in the neural-based Nepali NER study?", "answer": "[{'answer': 'Dataset contains 3606 total sentences and 79087 total entities.', 'type': 'abstractive'}, {'answer': 'ILPRL contains 548 sentences, OurNepali contains 3606 sentences', 'type': 'abstractive'}]", "main_doc": "1908.05828.pdf", "documents": "['1908.05828.pdf', '1909.00015.pdf', '1909.09270.pdf', '1909.00361.pdf', '1701.05574.pdf', '1704.00939.pdf', '1909.00105.pdf', '1903.09722.pdf', '1804.11346.pdf', '1809.01541.pdf', '1910.02339.pdf', '1701.02877.pdf', '2002.01207.pdf', '1802.06024.pdf', '1703.07090.pdf', '1909.13375.pdf', '1701.00185.pdf']"}
{"_id": "paper_tab_231", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How many distinct entity types are listed for the OurNepali and ILPRL datasets in the \"Named Entity Recognition for Nepali Language\" paper?", "answer": "[{'answer': 'OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities', 'type': 'abstractive'}, {'answer': 'three', 'type': 'extractive'}]", "main_doc": "1908.05828.pdf", "documents": "['1908.05828.pdf', '1608.06757.pdf', '2002.01359.pdf', '1706.08032.pdf', '1910.04269.pdf', '1904.09678.pdf', '1908.06083.pdf', '1903.09588.pdf', '1707.08559.pdf', '1907.09369.pdf', '2002.05058.pdf', '1910.02339.pdf', '1701.00185.pdf', '1904.03288.pdf', '2004.04721.pdf']"}
{"_id": "paper_tab_232", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "In the dataset statistics section of the paper on neural-based Nepali NER, how many sentences and entities are present in the newly created dataset?", "answer": "[{'answer': '3606 sentences', 'type': 'abstractive'}, {'answer': 'Dataset contains 3606 total sentences and 79087 total entities.', 'type': 'abstractive'}]", "main_doc": "1908.05828.pdf", "documents": "['1908.05828.pdf', '1706.08032.pdf', '1810.12196.pdf', '1909.00694.pdf', '1910.04269.pdf']"}
{"_id": "paper_tab_233", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the performance comparison between the grapheme-level and character-level BiLSTM+CNN models on the OurNepali and ILPRL test datasets as reported in the Nepali NER paper?", "answer": "[{'answer': 'On OurNepali test dataset Grapheme-level representation model achieves average 0.16% improvement, on ILPRL test dataset it achieves maximum 1.62% improvement', 'type': 'abstractive'}, {'answer': 'BiLSTM+CNN(grapheme-level) which turns out to be performing on par with BiLSTM+CNN(character-level) under the same configuration', 'type': 'extractive'}]", "main_doc": "1908.05828.pdf", "documents": "['1908.05828.pdf', '1704.05907.pdf', '1911.03310.pdf', '1905.07464.pdf', '1706.08032.pdf', '1909.08824.pdf', '1909.00105.pdf', '2002.01207.pdf', '2002.01664.pdf', '1902.00672.pdf']"}
{"_id": "paper_tab_234", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What performance metrics are reported to evaluate the PRPN model's variants on word/character-level language modeling tasks after the removal of different model components?", "answer": "[{'answer': 'BPC, Perplexity', 'type': 'abstractive'}]", "main_doc": "1711.02013.pdf", "documents": "['1711.02013.pdf', '1804.05918.pdf', '1909.07734.pdf', '1704.08960.pdf', '1908.10084.pdf', '1809.08298.pdf', '1904.05584.pdf', '1805.04033.pdf', '1909.00175.pdf', '2004.01878.pdf', '2003.08385.pdf', '1909.13714.pdf']"}
{"_id": "paper_tab_235", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "In the evaluation results, for which VQA task did the method achieve state-of-the-art accuracy of 60.34%, especially using basic questions and VGG?", "answer": "[{'answer': 'in open-ended task esp. for counting-type questions ', 'type': 'abstractive'}]", "main_doc": "1703.06492.pdf", "documents": "['1703.06492.pdf', '1908.07816.pdf', '1712.05999.pdf', '1610.07809.pdf', '1910.08987.pdf', '1810.12196.pdf', '2002.11402.pdf', '1704.05907.pdf', '2002.04181.pdf', '1701.03214.pdf', '2003.06044.pdf', '1910.10288.pdf']"}
{"_id": "paper_tab_236", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the exact EM and F1 scores for multi-BERT when fine-tuned on English and tested on Chinese, as compared to QANet trained on Chinese?", "answer": "[{'answer': 'Table TABREF6, Table TABREF8', 'type': 'extractive'}, {'answer': 'when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En', 'type': 'extractive'}]", "main_doc": "1909.09587.pdf", "documents": "['1909.09587.pdf', '1910.11769.pdf', '1901.04899.pdf', '2002.01207.pdf', '1810.09774.pdf', '1705.00108.pdf', '1909.13375.pdf', '1905.10810.pdf', '2001.08051.pdf', '1902.09666.pdf']"}
{"_id": "paper_tab_237", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What source-target language pairs were fine-tuned and tested for zero-shot cross-lingual reading comprehension tasks in this paper's multi-BERT experiments?", "answer": "[{'answer': 'En-Fr, En-Zh, En-Jp, En-Kr, Zh-En, Zh-Fr, Zh-Jp, Zh-Kr to English, Chinese or Korean', 'type': 'abstractive'}, {'answer': 'English , Chinese', 'type': 'extractive'}, {'answer': 'English, Chinese, Korean, we translated the English and Chinese datasets into more languages, with Google Translate', 'type': 'extractive'}]", "main_doc": "1909.09587.pdf", "documents": "['1909.09587.pdf', '1809.09194.pdf', '1908.05434.pdf', '1912.08960.pdf']"}
{"_id": "paper_tab_238", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What performance metrics were used to evaluate the classifiers in the first version of the training set for your proposed hybrid architecture in the finance domain?", "answer": "[{'answer': 'precision, recall, F1 and accuracy', 'type': 'abstractive'}, {'answer': 'Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy.', 'type': 'abstractive'}]", "main_doc": "1705.01214.pdf", "documents": "['1705.01214.pdf', '1607.06025.pdf', '1707.00110.pdf', '1701.00185.pdf', '1905.06566.pdf', '1611.02550.pdf', '1912.06670.pdf', '1906.10551.pdf']"}
{"_id": "paper_tab_239", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What three methods did Red Dragon AI employ for explanation regeneration in their TextGraphs-13 Shared Task submission, and how are they ranked by MAP scoring?", "answer": "[{'answer': 'Optimized TF-IDF, iterated TF-IDF, BERT re-ranking.', 'type': 'abstractive'}]", "main_doc": "1911.08976.pdf", "documents": "['1911.08976.pdf', '1704.06194.pdf', '1710.06700.pdf', '1912.01673.pdf', '1809.01202.pdf', '1804.08139.pdf', '1607.06025.pdf', '1809.03449.pdf', '1806.04511.pdf', '2003.05377.pdf', '1701.02877.pdf', '2004.03744.pdf']"}
{"_id": "paper_tab_240", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the specific improvements in exact match (EM) and F1 scores of the Stochastic Answer Network (SAN) compared to baseline models, and how does SAN rank in F1 performance on the SQuAD test set?", "answer": "[{'answer': 'Compared to baselines SAN (Table 1) shows  improvement of 1.096% on EM and 0.689% F1. Compared to other published SQuAD results (Table 2) SAN is ranked second. ', 'type': 'abstractive'}]", "main_doc": "1712.03556.pdf", "documents": "['1712.03556.pdf', '1610.00879.pdf', '1611.04642.pdf', '1909.03242.pdf', '1908.06379.pdf', '1909.08089.pdf', '1704.05907.pdf', '1909.13714.pdf', '2004.01878.pdf', '1912.01673.pdf', '1909.13695.pdf', '1904.10500.pdf', '1910.08987.pdf', '1909.08041.pdf', '2002.10361.pdf', '1611.02550.pdf']"}
{"_id": "paper_tab_241", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the exact document counts for the in-house and ACE05 datasets in the training, development, and test sets in the paper on neural cross-lingual relation extraction using bilingual word embedding mapping?", "answer": "[{'answer': 'In-house dataset consists of  3716 documents \\nACE05 dataset consists of  1635 documents', 'type': 'abstractive'}]", "main_doc": "1911.00069.pdf", "documents": "['1911.00069.pdf', '1605.07683.pdf', '1909.00015.pdf', '1908.11546.pdf', '2002.10361.pdf', '1912.13109.pdf', '1812.01704.pdf', '2002.11910.pdf', '2003.03014.pdf', '1912.01673.pdf', '1910.11204.pdf', '1905.00563.pdf', '1605.07333.pdf', '1912.01214.pdf', '1904.10500.pdf', '1804.08050.pdf']"}
{"_id": "paper_tab_242", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the ACC and NMI performance results of the STC^2 framework for the SearchSnippets, StackOverflow, and Biomedical datasets?", "answer": "[{'answer': 'On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%', 'type': 'abstractive'}]", "main_doc": "1701.00185.pdf", "documents": "['1701.00185.pdf', '2003.12218.pdf', '1704.08960.pdf', '1901.04899.pdf', '1611.04642.pdf', '2002.01664.pdf', '1712.03556.pdf', '1909.11467.pdf', '1911.10049.pdf', '2003.01769.pdf', '1605.08675.pdf']"}
{"_id": "paper_tab_243", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "By what percentage did the STC^2 model outperform other non-biased clustering methods in terms of ACC and NMI on both the SearchSnippets and Biomedical datasets?", "answer": "[{'answer': 'on SearchSnippets dataset by 6.72% in ACC, by 6.94% in NMI; on Biomedical dataset by 5.77% in ACC, 3.91% in NMI', 'type': 'abstractive'}]", "main_doc": "1701.00185.pdf", "documents": "['1701.00185.pdf', '1707.08559.pdf', '1910.12129.pdf', '1909.06162.pdf', '1809.00540.pdf', '2003.07723.pdf', '1809.09795.pdf', '1611.02550.pdf', '1905.00563.pdf', '1911.03597.pdf', '2004.03788.pdf']"}
{"_id": "paper_tab_244", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the four datasets reported for evaluating the improvements brought by the Multi-source Word-aligned Attention (MWA) method in this paper?", "answer": "[{'answer': 'weibo-100k, Ontonotes, LCQMC and XNLI', 'type': 'abstractive'}]", "main_doc": "1911.02821.pdf", "documents": "['1911.02821.pdf', '1909.09484.pdf', '1605.08675.pdf', '1911.12579.pdf', '2002.11910.pdf', '1806.11432.pdf', '2002.06644.pdf', '1804.07789.pdf', '1907.03060.pdf']"}
{"_id": "paper_tab_245", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the two question-answer datasets used to evaluate NeurON\u2019s tuple extraction performance?", "answer": "[{'answer': 'AmazonQA and ConciergeQA datasets', 'type': 'abstractive'}]", "main_doc": "1903.00172.pdf", "documents": "['1903.00172.pdf', '1911.08976.pdf', '1909.05855.pdf', '1902.09314.pdf', '2001.06888.pdf', '1908.06264.pdf', '1910.11235.pdf', '1908.08345.pdf', '1605.07333.pdf', '1609.00559.pdf', '1911.10049.pdf', '1708.09609.pdf', '2001.08868.pdf', '1704.05907.pdf', '1901.09755.pdf', '2004.03788.pdf']"}
{"_id": "paper_tab_246", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which two real-world datasets are listed in the NeurON paper for evaluating the system's performance in Open Information Extraction from question-answer pairs?", "answer": "[{'answer': 'ConciergeQA and AmazonQA', 'type': 'abstractive'}]", "main_doc": "1903.00172.pdf", "documents": "['1903.00172.pdf', '1607.06025.pdf', '1812.06705.pdf', '1909.07734.pdf', '2002.11910.pdf', '1908.07195.pdf', '2002.04181.pdf', '1909.08041.pdf', '1809.02279.pdf', '1906.10225.pdf', '1911.08962.pdf']"}
{"_id": "paper_tab_247", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What specific compression techniques, including NoKD and PKD, were compared in the evaluation of mixed-vocabulary training models in the \"Extremely Small BERT Models from Mixed-Vocabulary Training\" paper?", "answer": "[{'answer': 'baseline without knowledge distillation (termed NoKD), Patient Knowledge Distillation (PKD)', 'type': 'extractive'}, {'answer': 'NoKD, PKD, BERTBASE teacher model', 'type': 'extractive'}]", "main_doc": "1909.11687.pdf", "documents": "['1909.11687.pdf', '1710.06700.pdf', '2004.03354.pdf', '1811.01088.pdf', '1910.00912.pdf', '1909.09270.pdf', '1909.11297.pdf', '1909.13714.pdf']"}
{"_id": "paper_tab_248", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What languages are reported in the official CoNLL-SIGMORPHON 2018 shared task test set results for evaluating the Copenhagen system's morphological reinflection approach?", "answer": "[{'answer': 'German, English, Spanish, Finnish, French, Russian,  Swedish.', 'type': 'abstractive'}]", "main_doc": "1809.01541.pdf", "documents": "['1809.01541.pdf', '2003.11645.pdf', '1606.05320.pdf', '1902.09666.pdf', '1909.00512.pdf', '1909.01383.pdf', '1701.02877.pdf', '1705.01214.pdf']"}
{"_id": "paper_tab_249", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What specific type of MSD tag prediction (e.g., V;PST;V.PTCP;PASS) is used as an auxiliary task in the Copenhagen system for the inflection-in-context task in CoNLL--SIGMORPHON 2018, particularly highlighted in Track 1?", "answer": "[{'answer': 'The task of predicting MSD tags: V, PST, V.PCTP, PASS.', 'type': 'abstractive'}, {'answer': 'morphosyntactic descriptions (MSD)', 'type': 'extractive'}]", "main_doc": "1809.01541.pdf", "documents": "['1809.01541.pdf', '1608.06757.pdf', '1911.10049.pdf', '2002.06675.pdf', '1902.10525.pdf', '1904.09678.pdf', '2002.02070.pdf', '1707.00110.pdf', '1804.08139.pdf', '2003.05377.pdf', '1804.07789.pdf', '1806.07711.pdf', '1607.06025.pdf', '1909.08089.pdf', '1809.02286.pdf', '2001.06888.pdf', '1711.11221.pdf', '1910.03814.pdf']"}
{"_id": "paper_tab_250", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What was the specific improvement in F1 scores for both intent detection and slot filling when incorporating multimodal inputs (speech embeddings, audio, vision) in the NLU models?", "answer": "[{'answer': 'by 2.3-6.8 points in f1 score for intent recognition and 0.8-3.5 for slot filling', 'type': 'abstractive'}, {'answer': 'F1 score increased from 0.89 to 0.92', 'type': 'abstractive'}]", "main_doc": "1909.13714.pdf", "documents": "['1909.13714.pdf', '1804.08139.pdf', '1911.10049.pdf', '1703.02507.pdf', '1912.10011.pdf', '1911.02086.pdf', '1909.03405.pdf', '1701.09123.pdf', '1909.03135.pdf']"}
{"_id": "paper_tab_251", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which neural embedding model, CRX or 3C, achieves the highest accuracy in the dataless concept categorization task as presented in the paper?", "answer": "[{'answer': 'the CRX model', 'type': 'abstractive'}, {'answer': '3C model', 'type': 'extractive'}]", "main_doc": "1702.03342.pdf", "documents": "['1702.03342.pdf', '1801.05147.pdf', '1909.07734.pdf', '1912.01673.pdf', '1910.03814.pdf']"}
{"_id": "paper_tab_252", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How does the data manipulation method in this paper improve text classification accuracy on low-data datasets like SST-5, TREC, and IMDB, as well as imbalanced label distributions such as 100:1000 and 20:1000?", "answer": "[{'answer': 'Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000', 'type': 'abstractive'}]", "main_doc": "1910.12795.pdf", "documents": "['1910.12795.pdf', '1809.03449.pdf', '1812.06705.pdf', '1606.00189.pdf', '1910.06592.pdf', '1909.11297.pdf', '1909.11687.pdf', '1905.07464.pdf', '2002.11910.pdf', '1910.10288.pdf']"}
{"_id": "paper_tab_253", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the paper indicate that the language identification models combine both log-Mel spectrograms and raw waveforms as input features simultaneously?", "answer": "[{'answer': 'No', 'type': 'boolean'}]", "main_doc": "1910.04269.pdf", "documents": "['1910.04269.pdf', '1912.00864.pdf', '2001.05493.pdf', '1910.12129.pdf', '1909.00015.pdf']"}
{"_id": "paper_tab_254", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which unsupervised models are compared with the proposed sentence embedding method on supervised evaluation tasks in the study?", "answer": "[{'answer': 'Sequential (Denoising) Autoencoder, TF-IDF BOW, SkipThought, FastSent, Siamese C-BOW, C-BOW, C-PHRASE, ParagraphVector', 'type': 'extractive'}]", "main_doc": "1703.02507.pdf", "documents": "['1703.02507.pdf', '2001.08051.pdf', '1910.06592.pdf', '1910.03814.pdf']"}
{"_id": "paper_tab_255", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the BLEU score difference between FlowSeq and state-of-the-art baselines that utilize advanced decoding techniques like iterative refinement and NPD rescoring on NMT tasks?", "answer": "[{'answer': 'Difference is around 1 BLEU score lower on average than state of the art methods.', 'type': 'abstractive'}]", "main_doc": "1909.02480.pdf", "documents": "['1909.02480.pdf', '1909.00512.pdf', '1703.02507.pdf', '1908.07195.pdf', '1810.12085.pdf', '1806.07711.pdf', '1908.06151.pdf', '1911.08673.pdf', '1903.09722.pdf', '1909.00578.pdf', '1910.00825.pdf', '1909.00279.pdf', '2003.12218.pdf']"}
{"_id": "paper_tab_256", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the performance improvements in test scores for MVCNN with pretraining across the classification tasks Binary, Fine-Grained, Senti140, and Subjectivity?", "answer": "[{'answer': '0.8 points on Binary; 0.7 points on Fine-Grained; 0.6 points on Senti140; 0.7 points on Subj', 'type': 'abstractive'}]", "main_doc": "1603.04513.pdf", "documents": "['1603.04513.pdf', '1904.10503.pdf', '1905.10810.pdf', '1905.12260.pdf', '1806.11432.pdf', '1905.06566.pdf', '1808.09029.pdf', '1909.09270.pdf', '1809.06537.pdf', '1810.12196.pdf', '1711.11221.pdf', '1712.03547.pdf', '1706.08032.pdf', '1612.08205.pdf', '1909.09587.pdf', '1909.00105.pdf', '1711.00106.pdf', '1911.08673.pdf']"}
{"_id": "paper_tab_257", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How do the variable-size convolution filters (especially sizes 5 and 7) contribute to the extraction of multigranular phrase features and overall performance in the MVCNN architecture?", "answer": "[{'answer': 'The system benefits from filters of each size., features of multigranular phrases are extracted with variable-size convolution filters.', 'type': 'extractive'}]", "main_doc": "1603.04513.pdf", "documents": "['1603.04513.pdf', '1611.04798.pdf', '1908.10084.pdf', '1910.08987.pdf']"}
{"_id": "paper_tab_258", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How does the removal of individual pretrained word embedding versions in the MVCNN model affect sentence classification performance?", "answer": "[{'answer': 'each embedding version is crucial for good performance', 'type': 'extractive'}]", "main_doc": "1603.04513.pdf", "documents": "['1603.04513.pdf', '2002.05058.pdf', '1707.03569.pdf', '1911.03597.pdf', '1911.05153.pdf', '2004.04721.pdf', '1908.06083.pdf', '1911.00069.pdf', '1912.13109.pdf', '1701.03214.pdf', '1901.09755.pdf', '1912.08960.pdf', '1701.09123.pdf', '1610.00879.pdf', '1804.11346.pdf']"}
{"_id": "paper_tab_259", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the domains and services listed in the Schema-Guided Dialogue State Tracking Task dataset, as described for intents and dialogues across the train, dev, and test sets?", "answer": "[{'answer': 'Alarm, Banks, Buses, Calendar, Events, Flights, Homes, Hotels, Media, Messaging, Movies, Music, Payment, Rental Cars, Restaurants, Ride Sharing, Services, Train, Travel, Weather', 'type': 'abstractive'}]", "main_doc": "2002.01359.pdf", "documents": "['2002.01359.pdf', '1809.10644.pdf', '1910.12795.pdf', '1605.07333.pdf', '2002.01207.pdf', '1806.04511.pdf', '1811.01088.pdf', '2003.07723.pdf', '1707.03569.pdf', '1910.11204.pdf', '1711.11221.pdf', '2003.05377.pdf', '1909.11297.pdf', '1911.07555.pdf']"}
{"_id": "paper_tab_260", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which stance detection model achieved state-of-the-art weighted accuracy and standard accuracy on the Fake News Challenge Stage 1 (FNC-I) benchmark by leveraging a RoBERTa-based deep bidirectional transformer with bidirectional cross-attention between claim-article pairs?", "answer": "[{'answer': 'To the best of our knowledge, our method achieves state-of-the-art results in weighted-accuracy and standard accuracy on the dataset', 'type': 'extractive'}]", "main_doc": "1911.11951.pdf", "documents": "['1911.11951.pdf', '1911.02711.pdf', '1901.09755.pdf', '1901.02257.pdf', '1810.05241.pdf', '1605.07333.pdf', '1712.00991.pdf', '1904.01608.pdf', '1912.13109.pdf']"}
{"_id": "paper_tab_261", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which non-English language achieved the highest sentiment analysis accuracy after translation using the RNN model trained on English reviews, as reported in the experimental results with Russian, Spanish, Turkish, and Dutch?", "answer": "[{'answer': 'Russian', 'type': 'extractive'}, {'answer': 'Russsian', 'type': 'abstractive'}]", "main_doc": "1806.04511.pdf", "documents": "['1806.04511.pdf', '1912.01673.pdf', '1906.01081.pdf', '1909.00578.pdf', '2003.05377.pdf', '1901.02257.pdf', '1911.11951.pdf', '1805.03710.pdf', '1908.06379.pdf', '1611.00514.pdf', '1905.06566.pdf', '1908.06083.pdf']"}
{"_id": "paper_tab_262", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the cost benchmarks of the HULK energy efficiency benchmark platform, what is the minimal fine-tuning cost for a model using a 4-core TPU v3 priced at $8/hour?", "answer": "[{'answer': '$1,728', 'type': 'abstractive'}]", "main_doc": "2002.05829.pdf", "documents": "['2002.05829.pdf', '1910.00458.pdf', '1609.00559.pdf', '2002.01664.pdf', '1911.13066.pdf', '1909.03242.pdf', '1904.09678.pdf', '1606.00189.pdf', '1909.00361.pdf', '1902.09314.pdf', '1805.04033.pdf', '1707.03569.pdf', '1908.06379.pdf', '1809.08298.pdf', '1804.05918.pdf', '1910.12129.pdf', '1901.09755.pdf']"}
{"_id": "paper_tab_263", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What pretrained models are evaluated in the HULK benchmark paper for their pretraining costs, using hardware metrics and current TPU and V100 GPU pricing?", "answer": "[{'answer': 'BERT, XLNET RoBERTa, ALBERT, DistilBERT', 'type': 'abstractive'}]", "main_doc": "2002.05829.pdf", "documents": "['2002.05829.pdf', '1804.08050.pdf', '1911.03310.pdf', '2003.06044.pdf', '2002.04181.pdf', '1809.03449.pdf']"}
{"_id": "paper_tab_264", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What were the exact match (EM), generalization metric (GM), edit distance (ED), and F1 scores of the proposed model for both the test-repeated and test-new datasets?", "answer": "[{'answer': 'For test-repeated set, EM score of 61.17, F1 of 93.54, ED of 0.75 and GM of 61.36. For test-new set, EM score of 41.71, F1 of 91.02, ED of 1.22 and GM of 41.81', 'type': 'abstractive'}]", "main_doc": "1810.00663.pdf", "documents": "['1810.00663.pdf', '1810.06743.pdf', '2001.05970.pdf', '1911.10049.pdf', '1909.05855.pdf', '1804.07789.pdf', '2002.01359.pdf']"}
{"_id": "paper_tab_265", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the exact parameter count and memory size (in MB) of the Attentional Encoder Network (AEN) for targeted sentiment classification?", "answer": "[{'answer': 'Proposed model has 1.16 million parameters and 11.04 MB.', 'type': 'abstractive'}]", "main_doc": "1902.09314.pdf", "documents": "['1902.09314.pdf', '1904.09678.pdf', '1605.07333.pdf', '1906.05474.pdf', '1909.04002.pdf', '1810.06743.pdf', '2004.04721.pdf', '1909.00279.pdf', '1910.11235.pdf', '1611.04642.pdf', '1909.02480.pdf', '1911.04952.pdf', '1908.11546.pdf', '1809.01541.pdf', '2004.01878.pdf']"}
{"_id": "paper_tab_266", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How do data augmentation techniques (es, cs, cs+es) influence adversarial performance improvements in the task-oriented dialog system, specifically for the systems, and by what percentage do adversarial sets (Adv es, Adv cs) improve compared to the baseline?", "answer": "[{'answer': 'Data augmentation (es)  improved Adv es by 20% comparing to baseline \\nData augmentation (cs) improved Adv cs by 16.5% comparing to baseline\\nData augmentation (cs+es) improved both Adv cs and Adv es by at least 10% comparing to baseline \\nAll models show improvements over adversarial sets  \\n', 'type': 'abstractive'}]", "main_doc": "1911.05153.pdf", "documents": "['1911.05153.pdf', '1804.08050.pdf', '1809.06537.pdf', '2003.03106.pdf', '1904.10500.pdf']"}
{"_id": "paper_tab_267", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which 15 celebrities from domains such as politics, business, music, and entertainment provided the dataset of tweets used in the analysis of characterization scores and tweet popularity in the study on identifying a subject's most representative tweets?", "answer": "[{'answer': 'Amitabh Bachchan, Ariana Grande, Barack Obama, Bill Gates, Donald Trump,\\nEllen DeGeneres, J K Rowling, Jimmy Fallon, Justin Bieber, Kevin Durant, Kim Kardashian, Lady Gaga, LeBron James,Narendra Modi, Oprah Winfrey', 'type': 'abstractive'}, {'answer': 'Celebrities from varioius domains - Acting, Music, Politics, Business, TV, Author, Sports, Modeling. ', 'type': 'abstractive'}]", "main_doc": "1909.04002.pdf", "documents": "['1909.04002.pdf', '1809.10644.pdf', '1610.00879.pdf', '2001.08051.pdf', '1903.09722.pdf', '1911.02086.pdf', '1605.07333.pdf', '2004.01878.pdf', '1912.10806.pdf', '2002.00652.pdf', '1911.02711.pdf', '1710.09340.pdf', '1806.11432.pdf', '1604.00400.pdf', '1910.04269.pdf']"}
{"_id": "paper_tab_268", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the F1 scores reported for the proposed model's performance on the MSR and AS datasets, and how do they compare to the baseline models in the closed test evaluation?", "answer": "[{'answer': 'F1 score of 97.5 on MSR and 95.7 on AS', 'type': 'abstractive'}, {'answer': 'MSR: 97.7 compared to 97.5 of baseline\\nAS: 95.7 compared to 95.6 of baseline', 'type': 'abstractive'}]", "main_doc": "1910.14537.pdf", "documents": "['1910.14537.pdf', '1905.11901.pdf', '1606.05320.pdf', '1809.05752.pdf', '1908.11546.pdf', '1603.00968.pdf', '1611.04798.pdf', '1905.00563.pdf', '2002.11910.pdf', '1712.03547.pdf', '1909.01013.pdf', '2003.04866.pdf', '1809.03449.pdf', '1703.06492.pdf', '2003.05377.pdf', '2002.02070.pdf', '1909.00015.pdf', '1909.08824.pdf']"}
{"_id": "paper_tab_269", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the baseline models that the proposed CWS model is compared against in the closed test setting?", "answer": "[{'answer': 'Baseline models are:\\n- Chen et al., 2015a\\n- Chen et al., 2015b\\n- Liu et al., 2016\\n- Cai and Zhao, 2016\\n- Cai et al., 2017\\n- Zhou et al., 2017\\n- Ma et al., 2018\\n- Wang et al., 2019', 'type': 'abstractive'}]", "main_doc": "1910.14537.pdf", "documents": "['1910.14537.pdf', '1810.03459.pdf', '1912.06670.pdf', '1905.12260.pdf', '1911.13066.pdf', '2003.04642.pdf']"}
{"_id": "paper_tab_270", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What types of systems are compared in the GlossBERT paper for fine-grained English all-words WSD, including the baseline, knowledge-based, traditional supervised, and neural-based approaches evaluated in the Raganato et al. (2017b) framework?", "answer": "[{'answer': 'Two knowledge-based systems,\\ntwo traditional word expert supervised systems, six recent neural-based systems, and one BERT feature-based system.', 'type': 'abstractive'}]", "main_doc": "1908.07245.pdf", "documents": "['1908.07245.pdf', '1604.00400.pdf', '1808.09029.pdf', '1906.10225.pdf', '1703.06492.pdf', '1810.05241.pdf', '2003.03014.pdf', '1811.01088.pdf']"}
{"_id": "paper_tab_271", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How does the TL-TranSum model, incorporating sentence length and hyperedge coverage, perform compared to the MMR (Maximal Marginal Relevance) baseline in terms of redundancy and relevance on the DUC benchmark datasets?", "answer": "[{'answer': ' Moreover, our TL-TranSum method also outperforms other approaches such as MaxCover ( $5\\\\%$ ) and MRMR ( $7\\\\%$ )', 'type': 'extractive'}]", "main_doc": "1902.00672.pdf", "documents": "['1902.00672.pdf', '1909.08089.pdf', '2002.00652.pdf', '1909.04002.pdf', '1909.09587.pdf', '2001.10161.pdf', '1901.09755.pdf', '1909.00105.pdf', '1704.06194.pdf', '1809.09194.pdf', '1903.09722.pdf', '1909.00578.pdf', '1910.04269.pdf']"}
{"_id": "paper_tab_272", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which baseline models and advanced methods, including PBSMT, NMT, and their variations, did the authors evaluate in addressing the low-resource Japanese--Russian translation problem prior to introducing their back-translation and multi-directional approaches?", "answer": "[{'answer': 'pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17', 'type': 'extractive'}, {'answer': 'M2M Transformer', 'type': 'abstractive'}]", "main_doc": "1907.03060.pdf", "documents": "['1907.03060.pdf', '1909.11687.pdf', '1707.05236.pdf', '1911.08962.pdf', '1906.05474.pdf', '1704.00939.pdf', '1808.09920.pdf', '1705.00108.pdf', '1908.11047.pdf', '1909.07734.pdf', '1906.10551.pdf', '1909.00175.pdf', '1909.08089.pdf', '1909.05855.pdf', '1908.06264.pdf', '1609.00559.pdf']"}
{"_id": "paper_tab_273", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What baseline models are listed comparing the performance of different data representations in fine-grained sentiment classification against the multitask learning approach?", "answer": "[{'answer': 'SVMs, LR, BIBREF2', 'type': 'extractive'}, {'answer': 'SVM INLINEFORM0, SVM INLINEFORM1, LR INLINEFORM2, MaxEnt', 'type': 'extractive'}]", "main_doc": "1707.03569.pdf", "documents": "['1707.03569.pdf', '1905.00563.pdf', '2001.00137.pdf', '1711.00106.pdf', '2003.03044.pdf', '1608.06757.pdf', '2002.06644.pdf', '2003.12738.pdf', '1709.05413.pdf', '1908.06151.pdf', '1703.02507.pdf', '1902.00330.pdf', '1901.05280.pdf', '1910.07481.pdf', '1911.08673.pdf', '1812.01704.pdf', '1909.11687.pdf', '1910.06036.pdf']"}
{"_id": "paper_tab_274", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What was the reported MAE reduction achieved by the multitask biLSTM model for fine-grained sentiment classification in comparison to state-of-the-art methods?", "answer": "[{'answer': 'They decrease MAE in 0.34', 'type': 'abstractive'}]", "main_doc": "1707.03569.pdf", "documents": "['1707.03569.pdf', '1901.04899.pdf', '1912.08960.pdf', '1908.07195.pdf', '1810.10254.pdf', '1906.10551.pdf', '1911.05153.pdf', '1909.09484.pdf', '2004.01980.pdf', '1809.00530.pdf', '1910.11204.pdf', '1904.05584.pdf']"}
{"_id": "paper_tab_275", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the total number of run-on and non-run-on sentences mentioned in the paper on using machine learning models for run-on sentence correction?", "answer": "[{'answer': '4.756 million sentences', 'type': 'abstractive'}]", "main_doc": "1809.08298.pdf", "documents": "['1809.08298.pdf', '1910.03467.pdf', '1906.03538.pdf', '2002.02070.pdf', '2002.01984.pdf', '1904.10503.pdf', '1907.03060.pdf', '1912.13109.pdf', '1707.08559.pdf', '1701.09123.pdf']"}
{"_id": "paper_tab_276", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the 15 modification types detailed in the annotation instructions for sentence transformations in the Czech-based COSTRA 1.0 dataset, which are used to illustrate various sentence-level changes?", "answer": "[{'answer': '- paraphrase 1\\n- paraphrase 2\\n- different meaning\\n- opposite meaning\\n- nonsense\\n- minimal change\\n- generalization\\n- gossip\\n- formal sentence\\n- non-standard sentence\\n- simple sentence\\n- possibility\\n- ban\\n- future\\n- past', 'type': 'abstractive'}]", "main_doc": "1912.01673.pdf", "documents": "['1912.01673.pdf', '1901.05280.pdf', '1912.10435.pdf', '2002.06675.pdf', '1911.01799.pdf', '1909.00430.pdf', '1603.00968.pdf', '1901.08079.pdf', '1810.12885.pdf', '2001.05970.pdf', '1901.03866.pdf', '1801.05147.pdf', '1808.03430.pdf', '1611.00514.pdf', '1912.08960.pdf']"}
{"_id": "paper_tab_277", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What architecture is reported as state-of-the-art on WSJ, combining HMM with convolutional, recurrent, and fully connected layers?", "answer": "[{'answer': 'CNN-DNN-BLSTM-HMM', 'type': 'abstractive'}, {'answer': 'HMM-based system', 'type': 'extractive'}]", "main_doc": "1812.06864.pdf", "documents": "['1812.06864.pdf', '1605.07333.pdf', '1709.10217.pdf', '1904.07904.pdf', '1908.06379.pdf']"}
{"_id": "paper_tab_278", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the absolute improvements in BLEU scores for xIntent, xReact, and oReact on both the Event2Mind and Atomic datasets using the context-aware variational autoencoder model, as reported in comparison to previous state-of-the-art methods?", "answer": "[{'answer': 'ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.\\nOn Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively.', 'type': 'abstractive'}]", "main_doc": "1909.08824.pdf", "documents": "['1909.08824.pdf', '1911.13066.pdf', '1910.12129.pdf', '1911.00069.pdf', '1804.08139.pdf', '1909.05855.pdf', '2004.01980.pdf']"}
{"_id": "paper_tab_279", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which baseline models, including RNN-based Seq2Seq and CWVAE-Unpretrained, are compared with the proposed context-aware variational autoencoder on the Atomic dataset for If-Then reasoning?", "answer": "[{'answer': 'RNN-based Seq2Seq, Variational Seq2Seq, VRNMT , CWVAE-Unpretrained', 'type': 'extractive'}]", "main_doc": "1909.08824.pdf", "documents": "['1909.08824.pdf', '1804.07789.pdf', '1908.06379.pdf', '1611.03382.pdf', '1909.00430.pdf', '1809.01202.pdf', '1804.08139.pdf', '1811.12254.pdf', '1909.13375.pdf', '1909.00279.pdf', '2001.05493.pdf', '1707.08559.pdf']"}
{"_id": "paper_tab_280", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the results, which language shows the smallest reduction in character error rate when applying all components of the system?", "answer": "[{'answer': 'thai', 'type': 'abstractive'}]", "main_doc": "1902.10525.pdf", "documents": "['1902.10525.pdf', '1908.06267.pdf', '1809.02279.pdf', '1711.00106.pdf', '1809.04960.pdf', '2003.07996.pdf', '1810.05241.pdf', '1703.07090.pdf', '1803.09230.pdf', '2002.06424.pdf', '2003.03106.pdf', '1912.13109.pdf', '1911.02086.pdf', '1908.07195.pdf', '2001.06888.pdf', '1705.01214.pdf', '2003.08385.pdf', '1901.05280.pdf']"}
{"_id": "paper_tab_281", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the specific datasets from which sentence pairs were drawn for each task in the multi-task learning framework, according to the paper?", "answer": "[{'answer': '- En-Fr (WMT14)\\n- En-De (WMT15)\\n- Skipthought (BookCorpus)\\n- AllNLI (SNLI + MultiNLI)\\n- Parsing (PTB + 1-billion word)', 'type': 'abstractive'}]", "main_doc": "1804.00079.pdf", "documents": "['1804.00079.pdf', '1912.10435.pdf', '2002.01207.pdf', '1908.06151.pdf', '1902.09393.pdf', '1910.14497.pdf', '1906.11180.pdf', '1905.12260.pdf', '1909.03135.pdf', '2002.05829.pdf', '1911.07555.pdf', '2002.05058.pdf', '1903.09588.pdf', '1805.03710.pdf']"}
{"_id": "paper_tab_282", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the automatic evaluation results, how does ARAML improve in reverse perplexity, Self-BLEU, grammaticality, and relevance compared to baseline methods on the COCO and EMNLP2017 WMT datasets?", "answer": "[{'answer': 'ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.', 'type': 'abstractive'}, {'answer': 'Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively.', 'type': 'abstractive'}]", "main_doc": "1908.07195.pdf", "documents": "['1908.07195.pdf', '1912.01772.pdf', '1908.06267.pdf', '1910.02339.pdf', '1908.06151.pdf', '1908.05828.pdf', '1701.06538.pdf', '1805.03710.pdf']"}
{"_id": "paper_tab_283", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which language's subword segmentations, including examples like \"\u3008hell, o, o, o\u3009\" and \"\u3008louis, ana\u3009,\" are used to generate word embeddings based on cosine distance in the paper?", "answer": "[{'answer': 'English', 'type': 'abstractive'}]", "main_doc": "1805.03710.pdf", "documents": "['1805.03710.pdf', '1910.11769.pdf', '1706.08032.pdf', '1705.01265.pdf', '1910.14497.pdf', '1909.00694.pdf', '1709.10367.pdf', '1707.03569.pdf', '1909.03242.pdf', '1707.00110.pdf', '1809.02286.pdf', '1907.03060.pdf', '1906.11180.pdf']"}
{"_id": "paper_tab_284", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the paper list the macro-average F1 scores for the 'favor' and 'against' classes in a zero-shot cross-lingual stance detection setting involving German, French, and Italian languages?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]", "main_doc": "2003.08385.pdf", "documents": "['2003.08385.pdf', '1605.08675.pdf', '1909.03405.pdf', '1908.06267.pdf', '1902.00672.pdf', '1908.07195.pdf', '2002.06675.pdf', '1909.01383.pdf', '2003.03014.pdf']"}
{"_id": "paper_tab_285", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What F1 macro score and accuracy did M-BERT achieve on the supervised stance detection benchmarks in the X-Stance paper?", "answer": "[{'answer': 'M-Bert had 76.6 F1 macro score.', 'type': 'abstractive'}, {'answer': '75.1% and 75.6% accuracy', 'type': 'abstractive'}]", "main_doc": "2003.08385.pdf", "documents": "['2003.08385.pdf', '2002.10361.pdf', '1809.01541.pdf', '1708.09609.pdf', '1910.00912.pdf', '1701.03214.pdf', '1810.12085.pdf', '1810.12196.pdf', '1912.01772.pdf', '2001.08868.pdf', '2003.04642.pdf', '1709.05413.pdf', '1910.03467.pdf', '2002.02070.pdf', '1808.03430.pdf']"}
{"_id": "paper_tab_286", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What was the F1 macro score that multilingual BERT achieved on the Swiss election comments dataset for multilingual multi-target stance detection detailed in the X-Stance paper?", "answer": "[{'answer': 'BERT had 76.6 F1 macro score on x-stance dataset.', 'type': 'abstractive'}]", "main_doc": "2003.08385.pdf", "documents": "['2003.08385.pdf', '2002.01359.pdf', '2003.06044.pdf', '1910.12795.pdf', '1912.10011.pdf', '1904.01608.pdf', '1804.08050.pdf', '1804.11346.pdf', '1805.03710.pdf', '2001.06888.pdf', '1806.04330.pdf']"}
{"_id": "paper_tab_287", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the model with DS-Init and MAtt have fewer parameters than the baseline Transformer for the WMT14 En-De translation task?", "answer": "[{'answer': 'No', 'type': 'boolean'}]", "main_doc": "1908.11365.pdf", "documents": "['1908.11365.pdf', '1603.00968.pdf', '1808.09920.pdf', '1910.14497.pdf', '1910.06592.pdf', '1901.09755.pdf']"}
{"_id": "paper_tab_288", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the evaluation results, what were the highest F1 scores for Task 1 and the top performance metrics for Task 2, including Ratio, Satisfaction, Fluency, Turns, and Guide, in the iFLYTEK Corporation's evaluation of Chinese human-computer dialogue technology?", "answer": "[{'answer': 'For task 1 best F1 score was 0.9391 on closed and 0.9414 on open test.\\nFor task2 best result had: Ratio 0.3175 , Satisfaction 64.53, Fluency 0, Turns -1 and Guide 2', 'type': 'abstractive'}]", "main_doc": "1709.10217.pdf", "documents": "['1709.10217.pdf', '1909.08041.pdf', '1909.00754.pdf', '1909.02480.pdf', '1909.00015.pdf', '1909.03135.pdf']"}
{"_id": "paper_tab_289", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the micro-averaged evaluation metric officially reported in the results for assessing the performance of the semi-supervised bidirectional language model on named entity recognition and chunking tasks?", "answer": "[{'answer': 'micro-averaged F1', 'type': 'abstractive'}]", "main_doc": "1705.00108.pdf", "documents": "['1705.00108.pdf', '1909.03405.pdf', '1812.06864.pdf', '1809.05752.pdf', '2004.03744.pdf', '2001.05493.pdf', '1908.11365.pdf', '1709.10217.pdf', '1911.05153.pdf']"}
{"_id": "paper_tab_290", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "In the comparison of sequence tagging models without incorporating additional labeled data or task-specific gazetteers, which systems are listed in the semi-supervised sequence tagging with bidirectional language models paper?", "answer": "[{'answer': 'Chiu and Nichols (2016), Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), Hashimoto et al. (2016), S\u00f8gaard and Goldberg (2016) ', 'type': 'abstractive'}]", "main_doc": "1705.00108.pdf", "documents": "['1705.00108.pdf', '1908.11546.pdf', '1911.02086.pdf', '2002.08899.pdf', '1609.00559.pdf', '1902.09314.pdf', '1909.01013.pdf', '1904.10500.pdf', '1701.00185.pdf', '1905.10810.pdf', '1810.06743.pdf', '1809.09795.pdf', '1805.04033.pdf', '1911.01799.pdf', '1804.08139.pdf']"}
{"_id": "paper_tab_291", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What were the different German-English training data sizes, including subsets of the IWSLT14 corpus, used in the TED shared translation task experiments, and how was the development set split in this study?", "answer": "[{'answer': 'Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development', 'type': 'abstractive'}, {'answer': 'ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words)', 'type': 'extractive'}]", "main_doc": "1905.11901.pdf", "documents": "['1905.11901.pdf', '2003.07723.pdf', '1705.01214.pdf', '1909.11687.pdf', '1911.05153.pdf', '1707.05236.pdf', '1810.05241.pdf', '1611.04642.pdf', '1809.09795.pdf']"}
{"_id": "paper_tab_292", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the accuracy results of the BiGRU and BERT models on the ACP test set when trained with different configurations, including the full dataset and subsets, in the paper titled \"Minimally Supervised Learning of Affective Events Using Discourse Relations\"?", "answer": "[{'answer': 'Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \\nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.', 'type': 'abstractive'}]", "main_doc": "1909.00694.pdf", "documents": "['1909.00694.pdf', '1912.03457.pdf', '1911.08976.pdf', '1908.11365.pdf', '1912.13109.pdf', '1910.06036.pdf', '1905.00563.pdf']"}
{"_id": "paper_tab_293", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How many affective event pairs were extracted from both the Japanese web corpus and the ACP corpus in your minimally supervised learning of affective events via discourse relations?", "answer": "[{'answer': '7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus', 'type': 'abstractive'}, {'answer': 'The ACP corpus has around 700k events split into positive and negative polarity ', 'type': 'abstractive'}]", "main_doc": "1909.00694.pdf", "documents": "['1909.00694.pdf', '1904.05584.pdf', '2004.03744.pdf', '1908.07195.pdf', '1811.02906.pdf', '1909.11467.pdf', '1611.00514.pdf', '1710.09340.pdf', '1710.06700.pdf', '1912.01673.pdf', '1604.00400.pdf', '2002.02492.pdf']"}
{"_id": "paper_tab_294", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the percentage improvement in BERT's performance when using the minimally supervised method that propagates affective polarity through discourse relations, when trained on a small set of labeled Japanese data compared to the baseline supervised approach?", "answer": "[{'answer': '3%', 'type': 'abstractive'}]", "main_doc": "1909.00694.pdf", "documents": "['1909.00694.pdf', '1704.00939.pdf', '1909.01247.pdf', '2003.03106.pdf', '1912.03457.pdf', '2002.06644.pdf', '1909.00015.pdf', '1805.04033.pdf', '1806.07711.pdf', '2001.08051.pdf', '1809.09795.pdf', '1705.00108.pdf', '1703.06492.pdf', '1704.08960.pdf']"}
{"_id": "paper_tab_295", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What were the accuracy results reported for the STS, Sanders, and HCR datasets when evaluated using the proposed DeepCNN-BiLSTM model for binary sentiment classification?", "answer": "[{'answer': 'accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR', 'type': 'abstractive'}]", "main_doc": "1706.08032.pdf", "documents": "['1706.08032.pdf', '1911.02711.pdf', '1611.02550.pdf', '2003.11563.pdf', '1701.02877.pdf', '1809.09194.pdf', '1911.02086.pdf', '2003.05377.pdf']"}
{"_id": "paper_tab_296", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which semantic rules, as referenced in the Twitter sentiment classification model, are excluded from the main method because they are used to compute polarity after POS tagging or parsing steps?", "answer": "[{'answer': 'rules that compute polarity of words after POS tagging or parsing steps', 'type': 'abstractive'}]", "main_doc": "1706.08032.pdf", "documents": "['1706.08032.pdf', '1909.01958.pdf', '1909.08824.pdf', '1701.09123.pdf', '1702.03342.pdf', '1908.07816.pdf', '2004.03354.pdf', '1912.06670.pdf', '1607.06025.pdf', '1704.08960.pdf', '1810.12885.pdf', '2003.03106.pdf', '2003.01769.pdf', '1704.06194.pdf', '1910.00825.pdf', '1902.00672.pdf', '2002.08899.pdf', '1908.05828.pdf']"}
{"_id": "paper_tab_297", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the paper provide a complete list of all 75 fine-grained entity types annotated within the CORD-NER dataset?", "answer": "[{'answer': 'No', 'type': 'boolean'}]", "main_doc": "2003.12218.pdf", "documents": "['2003.12218.pdf', '1703.07090.pdf', '1911.00069.pdf', '1909.00361.pdf']"}
{"_id": "paper_tab_298", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What method is used in the paper to compute static embeddings from the contextualized word representations across different layers of ELMo, BERT, and GPT-2?", "answer": "[{'answer': \"They use the first principal component of a word's contextualized representation in a given layer as its static embedding.\", 'type': 'abstractive'}, {'answer': ' by taking the first principal component (PC) of its contextualized representations in a given layer', 'type': 'extractive'}]", "main_doc": "1909.00512.pdf", "documents": "['1909.00512.pdf', '2004.03788.pdf', '1701.06538.pdf', '2001.08051.pdf', '1911.07228.pdf', '2003.12218.pdf', '1711.00106.pdf', '1701.09123.pdf', '1908.06267.pdf', '1910.14537.pdf', '1906.10551.pdf', '2001.05970.pdf', '1902.09314.pdf', '1906.01081.pdf', '2001.00137.pdf']"}
{"_id": "paper_tab_299", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the 12 educational subjects listed in the Kurdish Textbooks Corpus (KTC) paper, specifically related to the Sorani dialect's K-12 textbooks, with 693,800 tokens categorized into these subjects?", "answer": "[{'answer': 'Economics, Genocide, Geography, History, Human Rights, Kurdish, Kurdology, Philosophy, Physics, Theology, Sociology, Social Study', 'type': 'abstractive'}]", "main_doc": "1909.11467.pdf", "documents": "['1909.11467.pdf', '1909.03135.pdf', '1909.09484.pdf', '1805.03710.pdf', '1606.05320.pdf']"}
{"_id": "paper_tab_300", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Are all Twitter customer service conversations used in the evaluation datasets focused exclusively on English?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}]", "main_doc": "1709.05413.pdf", "documents": "['1709.05413.pdf', '2002.04181.pdf', '1909.02480.pdf', '1910.12129.pdf', '1909.01013.pdf', '1908.07195.pdf', '1807.07961.pdf', '1701.05574.pdf', '1908.05828.pdf', '1701.09123.pdf', '2002.02070.pdf', '1904.03288.pdf', '2003.04642.pdf', '1810.12085.pdf', '1910.11769.pdf', '1912.03457.pdf', '1809.02286.pdf', '2001.00137.pdf']"}
{"_id": "paper_tab_301", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How does the gCAS model's performance in terms of Entity F1 and Success F1 compare across the movie, taxi, and restaurant domains, and in which domains does it particularly excel?", "answer": "[{'answer': 'For entity  F1 in the movie, taxi and restaurant domain it results in scores of 50.86, 64, and 60.35. For success, it results it outperforms in the movie and restaurant domain with scores of 77.95 and 71.52', 'type': 'abstractive'}]", "main_doc": "1908.11546.pdf", "documents": "['1908.11546.pdf', '1908.11365.pdf', '1702.03342.pdf', '1705.01265.pdf', '1707.08559.pdf']"}
{"_id": "paper_tab_302", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What specific evaluation metrics, such as BLEU scores and coherence-related measures, were used to assess the proposed dynamic and topic cache models on the NIST Chinese-English translation tasks?", "answer": "[{'answer': 'BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence.', 'type': 'abstractive'}]", "main_doc": "1711.11221.pdf", "documents": "['1711.11221.pdf', '1911.00069.pdf', '1603.00968.pdf', '1908.11365.pdf', '1712.05999.pdf', '1811.12254.pdf', '1909.00512.pdf', '2003.08385.pdf', '1809.01541.pdf', '1812.01704.pdf', '1906.10225.pdf', '1702.03342.pdf', '1612.08205.pdf', '1809.04960.pdf', '1809.02286.pdf']"}
{"_id": "paper_tab_303", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the 17 domains, including the one excluded from training, in the Schema-Guided Dialogue Dataset paper?", "answer": "[{'answer': 'Alarm\\nBank\\nBus\\nCalendar\\nEvent\\nFlight\\nHome\\nHotel\\nMedia\\nMovie\\nMusic\\nRentalCar\\nRestaurant\\nRideShare\\nService\\nTravel\\nWeather', 'type': 'abstractive'}]", "main_doc": "1909.05855.pdf", "documents": "['1909.05855.pdf', '1806.04511.pdf', '1909.07734.pdf', '1911.01799.pdf', '1809.09795.pdf', '1909.08859.pdf', '1906.01081.pdf', '1801.05147.pdf', '1605.08675.pdf', '1606.00189.pdf', '1911.08673.pdf', '1901.08079.pdf', '1910.12795.pdf', '1603.07044.pdf', '1809.05752.pdf', '1911.07228.pdf', '1811.01088.pdf']"}
{"_id": "paper_tab_304", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the 7 Indian languages listed in the paper that implements Ghost-VLAD pooling for language identification experiments?", "answer": "[{'answer': 'Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam', 'type': 'abstractive'}, {'answer': 'Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)', 'type': 'abstractive'}]", "main_doc": "2002.01664.pdf", "documents": "['2002.01664.pdf', '1909.13714.pdf', '1908.07195.pdf', '1901.08079.pdf', '1909.13695.pdf', '1803.09230.pdf', '1909.08089.pdf', '2004.03744.pdf', '1603.00968.pdf', '1901.04899.pdf', '2002.01984.pdf', '1903.00172.pdf']"}
{"_id": "paper_tab_305", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What accuracy metrics are presented to assess the performance of the proposed multi-task learning model on each of the 16 text classification tasks and as an overall average across all tasks?", "answer": "[{'answer': 'Accuracy on each dataset and the average accuracy on all datasets.', 'type': 'abstractive'}]", "main_doc": "1804.08139.pdf", "documents": "['1804.08139.pdf', '1801.05147.pdf', '1712.03556.pdf', '1809.09194.pdf', '1912.00864.pdf', '2002.08899.pdf', '1909.09587.pdf', '1907.09369.pdf', '2001.08868.pdf', '1905.07464.pdf']"}
{"_id": "paper_tab_306", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the percentage improvement of the model presented over the state-of-the-art systems on the Gigaword dataset in terms of both Rouge-1 and Rouge-L scores?", "answer": "[{'answer': 'w.r.t Rouge-1 their model outperforms by 0.98% and w.r.t Rouge-L their model outperforms by 0.45%', 'type': 'abstractive'}]", "main_doc": "1611.03382.pdf", "documents": "['1611.03382.pdf', '1912.06670.pdf', '1901.02262.pdf', '2002.02070.pdf', '1909.03544.pdf', '2001.05970.pdf', '1806.04330.pdf', '2002.00652.pdf', '1909.08859.pdf', '1701.02877.pdf', '1910.06592.pdf']"}
{"_id": "paper_tab_307", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the specific knowledge graphs that were used to evaluate the performance and robustness of the link prediction models in this study?", "answer": "[{'answer': ' Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs ', 'type': 'abstractive'}, {'answer': 'WN18 and YAGO3-10', 'type': 'extractive'}]", "main_doc": "1905.00563.pdf", "documents": "['1905.00563.pdf', '1812.06864.pdf', '1901.05280.pdf', '2003.11645.pdf', '2002.05058.pdf']"}
{"_id": "paper_tab_308", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which specific datasets, including those from the ABSA restaurants domain for languages like English, Spanish, French, and Russian, are detailed with token counts, target counts, and multiword targets?", "answer": "[{'answer': 'ABSA SemEval 2014-2016 datasets\\nYelp Academic Dataset\\nWikipedia dumps', 'type': 'abstractive'}]", "main_doc": "1901.09755.pdf", "documents": "['1901.09755.pdf', '1809.01541.pdf', '1810.12085.pdf', '1707.08559.pdf']"}
{"_id": "paper_tab_309", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the paper\u2019s ablation study, how much improvement is achieved in exact match (EM) and F1 scores when using the mixed objective, compared to only using cross-entropy, on the SQuAD development set?", "answer": "[{'answer': 'The mixed objective improves EM by 2.5% and F1 by 2.2%', 'type': 'abstractive'}]", "main_doc": "1711.00106.pdf", "documents": "['1711.00106.pdf', '1610.00879.pdf', '1707.00110.pdf', '1905.12260.pdf', '1811.02906.pdf', '1812.01704.pdf', '1909.01247.pdf', '2003.07723.pdf']"}
{"_id": "paper_tab_310", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the character error rates (CER) for the 9-layer, 2-layer regular-trained, and 2-layer distilled LSTM models as reported in the paper's experiments on Shenma voice search data?", "answer": "[{'answer': '2.49% for  layer-wise training, 2.63% for distillation, 6.26% for transfer learning.', 'type': 'abstractive'}, {'answer': 'Their best model achieved a 2.49% Character Error Rate.', 'type': 'abstractive'}]", "main_doc": "1703.07090.pdf", "documents": "['1703.07090.pdf', '1909.00015.pdf', '1909.01013.pdf', '2003.06044.pdf', '1704.00939.pdf', '1909.09587.pdf', '1909.00512.pdf', '1811.12254.pdf', '1605.07683.pdf', '1910.08987.pdf', '1912.06670.pdf', '1611.04798.pdf']"}
{"_id": "paper_tab_311", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What were the different layer configurations of the unidirectional LSTM models specifically evaluated for real-time streaming speech recognition in the study?", "answer": "[{'answer': 'Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers.', 'type': 'abstractive'}]", "main_doc": "1703.07090.pdf", "documents": "['1703.07090.pdf', '1904.01608.pdf', '1909.00175.pdf', '2002.01359.pdf']"}
{"_id": "paper_tab_312", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How do previous methods perform on the Switchboard Dialogue Act and DailyDialog datasets compared to the proposed hierarchical attention-based model\u2019s reported accuracies of 80.34% and 85.81%?", "answer": "[{'answer': 'Table TABREF20 , Table TABREF22, Table TABREF23', 'type': 'extractive'}]", "main_doc": "2003.06044.pdf", "documents": "['2003.06044.pdf', '1910.10288.pdf', '1801.05147.pdf', '1810.10254.pdf', '2004.01878.pdf', '1909.08824.pdf', '1810.03459.pdf', '1902.09314.pdf', '1908.06083.pdf', '1707.05236.pdf', '2002.08899.pdf', '1811.02906.pdf', '1912.01673.pdf', '1908.06151.pdf', '2002.11402.pdf', '1811.01088.pdf', '1802.06024.pdf']"}
{"_id": "paper_tab_313", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which baseline methods are compared against the proposed hierarchical self-attention-based model for the Switchboard Dialogue Act (SwDA) dataset?", "answer": "[{'answer': 'BLSTM+Attention+BLSTM\\nHierarchical BLSTM-CRF\\nCRF-ASN\\nHierarchical CNN (window 4)\\nmLSTM-RNN\\nDRLM-Conditional\\nLSTM-Softmax\\nRCNN\\nCNN\\nCRF\\nLSTM\\nBERT', 'type': 'abstractive'}]", "main_doc": "2003.06044.pdf", "documents": "['2003.06044.pdf', '1711.11221.pdf', '1704.00939.pdf', '1910.03467.pdf', '2003.08385.pdf', '1812.01704.pdf', '1608.06757.pdf', '1809.10644.pdf', '1911.02086.pdf', '1909.01247.pdf']"}
{"_id": "paper_tab_314", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What F1 score is reported for the Bengali NER corpus annotated by non-speakers using noisy and incomplete data, as highlighted by the authors' improvement over the previous state of the art?", "answer": "[{'answer': '52.0%', 'type': 'abstractive'}]", "main_doc": "1909.09270.pdf", "documents": "['1909.09270.pdf', '2002.05058.pdf', '1706.08032.pdf', '1703.07090.pdf', '2001.10161.pdf']"}
{"_id": "paper_tab_315", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which baseline systems and corresponding authors are compared against in the evaluation on both the HotpotQA and FEVER datasets in the \"Revealing the Importance of Semantic Retrieval for Machine Reading at Scale\" paper?", "answer": "[{'answer': 'HotspotQA: Yang, Ding, Muppet\\nFever: Hanselowski, Yoneda, Nie', 'type': 'abstractive'}]", "main_doc": "1909.08041.pdf", "documents": "['1909.08041.pdf', '1912.10435.pdf', '1701.05574.pdf', '2002.10361.pdf', '1701.02877.pdf', '1710.09340.pdf', '1611.02550.pdf', '1901.08079.pdf', '1704.08960.pdf', '1909.07734.pdf', '1609.00559.pdf', '1909.04002.pdf', '2002.05058.pdf', '1910.11769.pdf', '1909.11687.pdf']"}
{"_id": "paper_tab_316", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How does the effectiveness of sentence-level versus paragraph-level neural retrieval differ in terms of their impact on FEVER Score, Label Accuracy, and classification F1 scores across the SUPPORT, REFUTE, and NOT ENOUGH INFO labels?", "answer": "[{'answer': 'This seems to indicate that the downstream QA module relies more on the upstream paragraph-level retrieval whereas the verification module relies more on the upstream sentence-level retrieval.', 'type': 'extractive'}]", "main_doc": "1909.08041.pdf", "documents": "['1909.08041.pdf', '2001.06888.pdf', '1910.10288.pdf', '1705.01265.pdf', '1605.08675.pdf', '1806.04330.pdf', '1909.13695.pdf', '1810.12885.pdf', '2003.01769.pdf', '1909.09484.pdf']"}
{"_id": "paper_tab_317", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What evaluation metrics are reported for the multi-turn adversarial task, specifically comparing models that incorporate dialogue context with BERT dialogue segment features?", "answer": "[{'answer': 'F1 and Weighted-F1', 'type': 'abstractive'}]", "main_doc": "1908.06083.pdf", "documents": "['1908.06083.pdf', '1905.11901.pdf', '1608.06757.pdf', '1701.09123.pdf', '1909.02480.pdf', '2001.00137.pdf', '1910.12795.pdf', '1912.10435.pdf']"}
{"_id": "paper_tab_318", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the total number of Twitter accounts and tweets in the dataset specifically used for detecting fake news on Twitter through neural recurrent models and timeline-based profiling?", "answer": "[{'answer': 'Total dataset size: 171 account (522967 tweets)', 'type': 'abstractive'}, {'answer': '212 accounts', 'type': 'abstractive'}]", "main_doc": "1910.06592.pdf", "documents": "['1910.06592.pdf', '1910.05154.pdf', '1908.07245.pdf', '1910.11235.pdf', '1606.00189.pdf', '1703.07090.pdf', '1806.07711.pdf', '1706.08032.pdf', '1611.02550.pdf', '1906.01081.pdf', '1810.06743.pdf', '2001.06286.pdf', '1603.00968.pdf', '1912.01673.pdf', '1901.02257.pdf', '1909.00279.pdf', '1909.03242.pdf']"}
{"_id": "paper_tab_319", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Is the supervised morphological learner evaluated on Japanese data in the Japanese-Vietnamese translation experiments outlined in this paper?", "answer": "[{'answer': 'No', 'type': 'boolean'}]", "main_doc": "1910.03467.pdf", "documents": "['1910.03467.pdf', '2003.03044.pdf', '1707.08559.pdf', '1904.03288.pdf', '1911.08976.pdf', '1910.04269.pdf', '1909.13375.pdf', '1907.09369.pdf', '2002.06424.pdf', '1909.00430.pdf', '1908.07816.pdf', '1910.06748.pdf', '1909.03544.pdf', '2003.12738.pdf', '1809.00540.pdf', '2002.04181.pdf', '1911.13066.pdf']"}
{"_id": "paper_tab_320", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the mean prediction accuracy achieved by the DP-LSTM model for S&P 500 stock predictions in the paper?", "answer": "[{'answer': 'mean prediction accuracy 0.99582651\\nS&P 500 Accuracy 0.99582651', 'type': 'abstractive'}]", "main_doc": "1912.10806.pdf", "documents": "['1912.10806.pdf', '2002.06675.pdf', '1809.01202.pdf', '2003.12218.pdf', '2003.05377.pdf', '2004.03788.pdf']"}
{"_id": "paper_tab_321", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the train, dev, and test set sizes for the Onsei Intent Slot dataset in the slot filling experiments?", "answer": "[{'answer': 'Dataset has 1737 train, 497 dev and 559 test sentences.', 'type': 'abstractive'}]", "main_doc": "1911.01680.pdf", "documents": "['1911.01680.pdf', '1806.07711.pdf', '1708.09609.pdf', '1908.05434.pdf', '1909.09270.pdf', '2002.01984.pdf']"}
{"_id": "paper_tab_322", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the data, how much higher is the correlation of the PARENT metric with human judgments compared to the best-performing baseline on both the WikiBio and WebNLG datasets, and what are the specific improvements in correlation values?", "answer": "[{'answer': 'Best proposed metric has average correlation with human judgement of 0.913 and 0.846 compared to best compared metrics result of 0.758 and 0.829 on WikiBio and WebNLG challenge.', 'type': 'abstractive'}, {'answer': 'Their average correlation tops the best other model by 0.155 on WikiBio.', 'type': 'abstractive'}]", "main_doc": "1906.01081.pdf", "documents": "['1906.01081.pdf', '1703.07090.pdf', '1806.07711.pdf', '1703.02507.pdf']"}
{"_id": "paper_tab_323", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "In the human evaluation results, TitleStylist outperforms which summarization model by 9.68% in terms of attraction?", "answer": "[{'answer': 'pure summarization model NHG', 'type': 'extractive'}]", "main_doc": "2004.01980.pdf", "documents": "['2004.01980.pdf', '1712.03556.pdf', '1905.07464.pdf', '1710.06700.pdf', '1611.02550.pdf', '1911.01680.pdf', '1908.05828.pdf', '1905.11901.pdf', '2003.07723.pdf', '1704.00939.pdf']"}
{"_id": "paper_tab_324", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the percentage improvements in relevance, attraction, and fluency for humor-styled headlines generated using the TitleStylist method compared to the multitask baseline, as reported in the human evaluation results?", "answer": "[{'answer': 'Humor in headlines (TitleStylist vs Multitask baseline):\\nRelevance: +6.53% (5.87 vs 5.51)\\nAttraction: +3.72% (8.93 vs 8.61)\\nFluency: 1,98% (9.29 vs 9.11)', 'type': 'abstractive'}]", "main_doc": "2004.01980.pdf", "documents": "['2004.01980.pdf', '1909.01247.pdf', '2002.06675.pdf', '2002.04181.pdf', '1905.10810.pdf', '1910.02339.pdf', '1909.03135.pdf', '1701.06538.pdf', '1705.00108.pdf', '1912.10435.pdf', '1910.03467.pdf', '1901.02257.pdf', '2002.01359.pdf']"}
{"_id": "paper_tab_325", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the exact MAP score differences between the neural attention-based RNN encoder model and the handcrafted feature-based approach for Tasks A, B, and C as reported in the SemEval-2016 cQA experiments?", "answer": "[{'answer': '0.007 MAP on Task A, 0.032 MAP on Task B, 0.055 MAP on Task C', 'type': 'abstractive'}]", "main_doc": "1603.07044.pdf", "documents": "['1603.07044.pdf', '2003.03044.pdf', '2002.08899.pdf', '1810.05241.pdf', '1907.09369.pdf', '1701.09123.pdf']"}
{"_id": "paper_tab_326", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How much improvement in Query Matching and Interaction Matching does the Turn+SQL Attn+Action Copy model in this paper achieve over the previous state-of-the-art on the SParC dataset without using BERT?", "answer": "[{'answer': 'Compared with the previous SOTA without BERT on SParC, our model improves Ques.Match and Int.Match by $10.6$ and $5.4$ points, respectively.', 'type': 'extractive'}]", "main_doc": "2002.00652.pdf", "documents": "['2002.00652.pdf', '1909.08041.pdf', '2003.03014.pdf', '2002.04181.pdf', '1804.08139.pdf', '2002.08899.pdf', '1909.11467.pdf', '1909.08824.pdf', '1812.06864.pdf', '1801.05147.pdf', '1702.03342.pdf', '1712.03547.pdf', '1809.05752.pdf', '1603.07044.pdf', '2003.03106.pdf']"}
{"_id": "paper_tab_327", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What specific metrics are reported for evaluating distinctness, user matching accuracy, and preference alignment in generated personalized recipes within this study?", "answer": "[{'answer': 'Byte-Pair Encoding perplexity  (BPE PPL),\\nBLEU-1,\\nBLEU-4,\\nROUGE-L,\\npercentage of distinct unigram (D-1),\\npercentage of distinct bigrams(D-2),\\nuser matching accuracy(UMA),\\nMean Reciprocal Rank(MRR)\\nPairwise preference over baseline(PP)', 'type': 'abstractive'}, {'answer': 'BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence', 'type': 'extractive'}, {'answer': ' Distinct-1/2, UMA = User Matching Accuracy, MRR\\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)', 'type': 'abstractive'}]", "main_doc": "1909.00105.pdf", "documents": "['1909.00105.pdf', '1901.02262.pdf', '1812.06864.pdf', '1910.12795.pdf', '1905.00563.pdf', '1908.06264.pdf', '1603.07044.pdf', '2003.04642.pdf', '1910.02339.pdf', '2001.06286.pdf', '1810.12085.pdf', '1812.10479.pdf', '1910.00458.pdf', '1902.09666.pdf', '1909.01383.pdf']"}
{"_id": "paper_tab_328", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the paper specifically analyze how the term \"homosexual\" has come to be more strongly associated with dehumanizing language compared to \"gay and lesbian,\" particularly in the context of the Gallup survey's change in wording?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]", "main_doc": "2003.03014.pdf", "documents": "['2003.03014.pdf', '1611.04642.pdf', '1909.06162.pdf', '1910.11769.pdf', '1909.03544.pdf', '1910.03467.pdf', '1912.10011.pdf', '1909.03242.pdf', '1905.12260.pdf', '1806.04330.pdf', '1912.03457.pdf', '2004.03744.pdf', '1808.09029.pdf', '1909.05855.pdf', '1711.11221.pdf', '1808.03430.pdf', '2003.08385.pdf']"}
{"_id": "paper_tab_329", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "According to the Vietnamese NER error analysis paper, how do the F1 scores of the BLSTM-CNN-CRF and BLSTM-CRF models compare when using the VLSP 2016 dataset with Kyubyong Park\u2019s or Edouard Grave\u2019s pre-trained word embeddings?", "answer": "[{'answer': 'Best BLSTM-CNN-CRF had F1 score 86.87 vs 86.69 of best BLSTM-CRF ', 'type': 'abstractive'}]", "main_doc": "1911.07228.pdf", "documents": "['1911.07228.pdf', '1909.00105.pdf', '1707.00110.pdf', '1909.00015.pdf', '1906.11180.pdf', '1712.05999.pdf', '1903.00172.pdf', '1912.00864.pdf', '2004.03788.pdf', '1810.05241.pdf', '1709.05413.pdf', '1904.10503.pdf', '1909.00252.pdf', '1910.12795.pdf', '1905.06566.pdf', '1909.02480.pdf']"}
{"_id": "paper_tab_330", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which prior work, using a task-specific summarization architecture, is compared against the pre-trained sequence-to-sequence model in this paper?", "answer": "[{'answer': 'BIBREF26 ', 'type': 'extractive'}, {'answer': 'BIBREF26', 'type': 'extractive'}]", "main_doc": "1903.09722.pdf", "documents": "['1903.09722.pdf', '1804.07789.pdf', '1909.00430.pdf', '1906.01081.pdf', '2002.06675.pdf', '1909.09270.pdf', '1905.06566.pdf', '1804.11346.pdf', '1611.00514.pdf', '1802.06024.pdf']"}
{"_id": "paper_tab_331", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the 14 genres that categorize the 138,368 Brazilian song lyrics used for genre classification in the study?", "answer": "[{'answer': 'Gospel, Sertanejo, MPB, Forr\u00f3, Pagode, Rock, Samba, Pop, Ax\u00e9, Funk-carioca, Infantil, Velha-guarda, Bossa-nova and Jovem-guarda', 'type': 'abstractive'}]", "main_doc": "2003.05377.pdf", "documents": "['2003.05377.pdf', '1704.06194.pdf', '1910.10288.pdf', '1910.06592.pdf', '1911.02711.pdf', '2004.03788.pdf', '1908.06379.pdf', '1703.02507.pdf', '1906.05474.pdf']"}
{"_id": "paper_tab_332", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What corpus, consisting of a large Web-crawled dataset of $5 \\times 10^{10}$ tokens from the University of Waterloo and targeted science content from platforms like Wikipedia and SimpleWikipedia, underpins the Aristo system's success on the New York Regents Science Exams?", "answer": "[{'answer': 'Aristo Corpus\\nRegents 4th\\nRegents 8th\\nRegents `12th\\nARC-Easy\\nARC-challenge ', 'type': 'abstractive'}]", "main_doc": "1909.01958.pdf", "documents": "['1909.01958.pdf', '1912.01772.pdf', '1912.03457.pdf', '1812.01704.pdf', '2002.05829.pdf', '1605.07683.pdf']"}
{"_id": "paper_tab_333", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the P@1 accuracy results for EN-IT, IT-EN, and other language pairs on the Vecmap benchmark after applying the proposed duality regularization method?", "answer": "[{'answer': 'New best results of accuracy (P@1) on Vecmap:\\nOurs-GeoMMsemi: EN-IT 50.00 IT-EN 42.67 EN-DE 51.60 DE-EN 47.22 FI-EN 39.62 EN-ES 39.47 ES-EN 36.43', 'type': 'abstractive'}]", "main_doc": "1909.01013.pdf", "documents": "['1909.01013.pdf', '1804.08139.pdf', '1709.10217.pdf', '1607.06025.pdf']"}
{"_id": "paper_tab_334", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the P@1 accuracy results comparing the proposed duality regularization method and the best baseline method for each language pair (EN-IT, IT-EN, EN-DE, DE-EN, EN-FI, FI-EN, EN-ES, and ES-EN) on Vecmap?", "answer": "[{'answer': 'Proposed method vs best baseline result on Vecmap (Accuracy P@1):\\nEN-IT: 50 vs 50\\nIT-EN: 42.67 vs 42.67\\nEN-DE: 51.6 vs 51.47\\nDE-EN: 47.22 vs 46.96\\nEN-FI: 35.88 vs 36.24\\nFI-EN: 39.62 vs 39.57\\nEN-ES: 39.47 vs 39.30\\nES-EN: 36.43 vs 36.06', 'type': 'abstractive'}]", "main_doc": "1909.01013.pdf", "documents": "['1909.01013.pdf', '1810.00663.pdf', '1901.02262.pdf', '1912.10435.pdf', '1611.00514.pdf', '1701.03214.pdf', '1809.05752.pdf', '1902.09393.pdf', '2002.06675.pdf', '1701.02877.pdf', '2003.06044.pdf']"}
{"_id": "paper_tab_335", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the six language pairs on which the duality regularization method significantly reduced back-translation inconsistencies compared to Adv-C?", "answer": "[{'answer': 'EN<->ES\\nEN<->DE\\nEN<->IT\\nEN<->EO\\nEN<->MS\\nEN<->FI', 'type': 'abstractive'}]", "main_doc": "1909.01013.pdf", "documents": "['1909.01013.pdf', '1604.00400.pdf', '1709.10367.pdf', '1904.09678.pdf', '1910.03467.pdf']"}
{"_id": "paper_tab_336", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the Spearman\u2019s \u03c1, Kendall\u2019s \u03c4, and Pearson\u2019s r correlation values for the BEST-ROUGE versions across the DUC-05, DUC-06, and DUC-07 datasets, as evaluated by the SumQE model on questions Q1\u2013Q5?", "answer": "[{'answer': 'High correlation results range from 0.472 to 0.936', 'type': 'abstractive'}]", "main_doc": "1909.00578.pdf", "documents": "['1909.00578.pdf', '1909.06937.pdf', '2002.02070.pdf', '1606.00189.pdf', '1911.01680.pdf', '1805.04033.pdf', '1703.06492.pdf']"}
{"_id": "paper_tab_337", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which baseline models were used for comparison against MMM on the DREAM dataset to demonstrate the accuracy improvements in the paper?", "answer": "[{'answer': 'FTLM++, BERT-large, XLNet', 'type': 'abstractive'}]", "main_doc": "1910.00458.pdf", "documents": "['1910.00458.pdf', '1910.10288.pdf', '2003.01769.pdf', '1808.09920.pdf', '1910.12129.pdf', '1805.03710.pdf', '2003.03044.pdf', '2004.04721.pdf', '1908.07816.pdf', '1901.09755.pdf', '1912.08960.pdf', '2002.10361.pdf', '1707.08559.pdf', '2003.08385.pdf']"}
{"_id": "paper_tab_338", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What bilingual model specifically translates word-level bilingual data into unsegmented phonemic sequences during the language documentation process detailed in the paper?", "answer": "[{'answer': ' Neural Machine Translation (NMT) models are trained between language pairs, using as source language the translation (word-level) and as target', 'type': 'extractive'}]", "main_doc": "1910.05154.pdf", "documents": "['1910.05154.pdf', '1902.10525.pdf', '2002.01207.pdf', '1812.06864.pdf', '1909.04002.pdf', '2003.07996.pdf', '1705.00108.pdf', '1911.03597.pdf', '1902.00672.pdf', '1804.11346.pdf']"}
{"_id": "paper_tab_339", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What models are evaluated in the ReCoRD dataset paper, comparing machine reading comprehension methods to human performance on commonsense reasoning tasks?", "answer": "[{'answer': 'DocQA, SAN, QANet, ASReader, LM, Random Guess', 'type': 'abstractive'}]", "main_doc": "1810.12885.pdf", "documents": "['1810.12885.pdf', '2002.00652.pdf', '2003.05377.pdf', '1904.03288.pdf', '1909.08041.pdf', '1909.02480.pdf', '1806.11432.pdf', '1910.06748.pdf', '1809.00540.pdf', '1901.02257.pdf', '1911.07228.pdf', '1906.01081.pdf', '1908.05434.pdf', '1909.01383.pdf', '1909.00430.pdf', '1908.07245.pdf', '2002.11910.pdf', '1709.10217.pdf']"}
{"_id": "paper_tab_340", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which four languages are included in the SER datasets presented in the *Cross Lingual Cross Corpus Speech Emotion Recognition* paper?", "answer": "[{'answer': 'German, English, Italian, Chinese', 'type': 'abstractive'}]", "main_doc": "2003.07996.pdf", "documents": "['2003.07996.pdf', '2003.06044.pdf', '1609.00559.pdf', '1906.01081.pdf', '2002.02492.pdf', '1810.09774.pdf', '1906.11180.pdf', '2002.00652.pdf']"}
{"_id": "paper_tab_341", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which sentiment class was predicted with the highest accuracy by the NLP tools in the ELS task in the study comparing named-entity recognition and entity-level sentiment analysis for political tweets?", "answer": "[{'answer': 'neutral sentiment', 'type': 'abstractive'}]", "main_doc": "2002.04181.pdf", "documents": "['2002.04181.pdf', '1912.01673.pdf', '1901.02262.pdf', '2004.04721.pdf', '1910.11204.pdf']"}
{"_id": "paper_tab_342", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the precision, recall, and F1 score improvements for both named entity recognition and nominal mention when applying the boundary assembling method compared to the models by He and Sun (2017) and Peng and Dredze (2017)?", "answer": "[{'answer': 'Overall F1 score:\\n- He and Sun (2017) 58.23\\n- Peng and Dredze (2017) 58.99\\n- Xu et al. (2018) 59.11', 'type': 'abstractive'}, {'answer': 'For Named entity the maximum precision was 66.67%, and the average 62.58%, same values for Recall was 55.97% and 50.33%, and for F1 57.14% and 55.64%. Where for Nominal Mention had maximum recall of 74.48% and average of 73.67%, Recall had values of 54.55% and 53.7%,  and F1 had values of  62.97% and 62.12%. Finally the Overall F1 score had maximum value of 59.11% and average of 58.77%', 'type': 'abstractive'}]", "main_doc": "2002.11910.pdf", "documents": "['2002.11910.pdf', '2004.04721.pdf', '1904.10503.pdf', '1909.03405.pdf', '1809.01541.pdf', '1912.00864.pdf', '2003.05377.pdf']"}
{"_id": "paper_tab_344", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What perplexity scores on the dev and test sets are reported for the language model trained using augmented code-switched sentences in the \"Learn to Code-Switch\" paper?", "answer": "[{'answer': 'Perplexity score 142.84 on dev and 138.91 on test', 'type': 'abstractive'}]", "main_doc": "1810.10254.pdf", "documents": "['1810.10254.pdf', '1912.01673.pdf', '1908.06151.pdf', '1905.11901.pdf', '2001.05970.pdf', '2002.11402.pdf', '1711.11221.pdf', '1909.06162.pdf', '2001.06888.pdf', '1904.10500.pdf', '1908.06267.pdf', '1810.06743.pdf', '1911.10049.pdf', '2004.01980.pdf', '2001.10161.pdf', '2002.05058.pdf', '1909.08824.pdf', '1610.00879.pdf']"}
{"_id": "paper_tab_345", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What baseline models were used in the experiments on subjective bias detection in the Wiki Neutrality Corpus before comparing them to the proposed BERT-based ensembles?", "answer": "[{'answer': 'FastText, BiLSTM, BERT', 'type': 'extractive'}, {'answer': 'FastText, BERT , two-layer BiLSTM architecture with GloVe word embeddings', 'type': 'extractive'}]", "main_doc": "2002.06644.pdf", "documents": "['2002.06644.pdf', '2002.01359.pdf', '1904.09678.pdf', '1705.01214.pdf', '1911.03310.pdf', '1910.03467.pdf', '1804.08139.pdf', '1610.00879.pdf', '2001.08868.pdf', '1904.03288.pdf', '1811.12254.pdf', '1911.11951.pdf', '2002.02070.pdf', '1707.03569.pdf', '1910.02339.pdf', '1911.08976.pdf', '1910.06036.pdf']"}
{"_id": "paper_tab_346", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What metrics are reported that assess BERT's performance on the unseen portion of the training set and the development set, when applying different augmentation techniques to the Propaganda Techniques Corpus (PTC)?", "answer": "[{'answer': 'precision, recall , F1 score', 'type': 'extractive'}]", "main_doc": "2003.11563.pdf", "documents": "['2003.11563.pdf', '1809.01541.pdf', '1904.05584.pdf', '2003.11645.pdf', '1909.00361.pdf', '2004.03744.pdf', '1908.05434.pdf', '1911.08673.pdf', '1910.02339.pdf', '1610.07809.pdf', '2002.01984.pdf', '2003.03014.pdf', '1701.02877.pdf', '1911.07555.pdf', '1902.09314.pdf', '1810.06743.pdf', '1809.06537.pdf']"}
{"_id": "paper_tab_347", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which training dataset showed the highest test accuracy and best generalization across different natural language inference benchmarks?", "answer": "[{'answer': 'MultiNLI', 'type': 'abstractive'}]", "main_doc": "1810.09774.pdf", "documents": "['1810.09774.pdf', '1908.11546.pdf', '1701.05574.pdf', '1903.09588.pdf', '1908.06083.pdf', '2003.07723.pdf', '1908.06151.pdf', '1911.11951.pdf', '1810.00663.pdf', '1905.07464.pdf']"}
{"_id": "paper_tab_348", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What baseline models are listed in the CAIL2019-SCM paper for the Similar Case Matching task on the valid and test datasets?", "answer": "[{'answer': 'CNN, LSTM, BERT', 'type': 'abstractive'}]", "main_doc": "1911.08962.pdf", "documents": "['1911.08962.pdf', '1908.07195.pdf', '1808.03430.pdf', '1901.02257.pdf', '1707.00110.pdf', '2001.05467.pdf', '1806.07711.pdf', '2002.06424.pdf', '2001.06888.pdf', '1809.00530.pdf', '2003.11563.pdf', '1907.03060.pdf', '1909.01013.pdf', '1909.13714.pdf', '1902.10525.pdf', '1809.08298.pdf']"}
{"_id": "paper_tab_349", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the NLI-PT dataset indicate that the distribution of essays among the various L1 groups is balanced?", "answer": "[{'answer': 'No', 'type': 'boolean'}, {'answer': 'No', 'type': 'boolean'}]", "main_doc": "1804.11346.pdf", "documents": "['1804.11346.pdf', '1909.13695.pdf', '1908.06264.pdf', '1707.08559.pdf', '2004.04721.pdf', '1611.00514.pdf', '1910.06748.pdf', '1706.08032.pdf', '1812.06705.pdf', '1902.10525.pdf', '2002.10361.pdf', '1911.00069.pdf', '1909.11687.pdf', '1912.01214.pdf', '1902.09666.pdf', '1701.03214.pdf']"}
{"_id": "paper_tab_350", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What baseline NER models were compared to the proposed multimodal deep learning approaches that incorporate image features?", "answer": "[{'answer': 'Stanford NER, BiLSTM+CRF, LSTM+CNN+CRF, T-NER and BiLSTM+CNN+Co-Attention', 'type': 'abstractive'}]", "main_doc": "2001.06888.pdf", "documents": "['2001.06888.pdf', '1901.09755.pdf', '1708.09609.pdf', '1908.07816.pdf', '2003.07996.pdf', '1909.09587.pdf', '1904.01608.pdf', '1912.01772.pdf', '1804.00079.pdf', '1909.13714.pdf', '1612.05270.pdf', '1809.05752.pdf', '1910.00912.pdf', '1910.07481.pdf', '1905.07464.pdf', '1911.03597.pdf', '1909.00578.pdf']"}
{"_id": "paper_tab_351", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which references are cited in the caption when evaluating 11 downstream sentence-level transfer tasks using word vectors fed into a BiLSTM with max-pooling?", "answer": "[{'answer': 'BIBREF13 , BIBREF18', 'type': 'extractive'}]", "main_doc": "1904.05584.pdf", "documents": "['1904.05584.pdf', '1811.01088.pdf', '1906.03538.pdf', '1610.00879.pdf', '2003.08385.pdf', '1606.05320.pdf', '1809.02279.pdf', '1910.06592.pdf', '1909.08089.pdf', '1908.08345.pdf']"}
{"_id": "paper_tab_352", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What were the exact F1 improvements reported after incorporating both Simple Skip and Transformer Skip connections into the coattention-enhanced base BERT model, in comparison to the baseline performance?", "answer": "[{'answer': 'Simple Skip improves F1 from 74.34 to 74.81\\nTransformer Skip improes F1 from 74.34 to 74.95 ', 'type': 'abstractive'}]", "main_doc": "1912.10435.pdf", "documents": "['1912.10435.pdf', '2001.10161.pdf', '2002.01664.pdf', '1910.00458.pdf', '1610.07809.pdf', '1811.12254.pdf', '1701.09123.pdf', '2004.03744.pdf', '1901.08079.pdf', '2001.05493.pdf', '1607.06025.pdf', '1712.05999.pdf', '2001.08868.pdf', '2002.02070.pdf', '1910.03814.pdf']"}
{"_id": "paper_tab_353", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the domain-specific F1 scores for the topic extraction models, and how do the Substance, Interpersonal, and Mood domains compare in terms of model performance in predicting psychiatric readmissions?", "answer": "[{'answer': 'Achieved the highest per-domain scores on Substance (F1 \u2248 0.8) and the lowest scores on Interpersonal and Mood (F1 \u2248 0.5), and show consistency in per-domain performance rankings between MLP and RBF models.', 'type': 'abstractive'}]", "main_doc": "1809.05752.pdf", "documents": "['1809.05752.pdf', '1909.01013.pdf', '1901.01010.pdf', '1909.04002.pdf', '1910.12129.pdf', '1605.07333.pdf', '1901.03866.pdf', '1611.03382.pdf', '1909.03135.pdf', '1703.02507.pdf', '1712.03547.pdf', '1906.01081.pdf', '1712.05999.pdf']"}
{"_id": "paper_tab_354", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "In the retrieval and generative evaluation settings, by how much does the proposed model + IR2 outperform NVDM in terms of MRR, MR, Recall@10, BLEU, CIDEr, ROUGE, and METEOR in the \"Unsupervised Machine Commenting with Neural Variational Topic Model\" paper?", "answer": "[{'answer': 'Under the retrieval evaluation setting, their proposed model + IR2 had better MRR than NVDM by 0.3769, better MR by 4.6, and better Recall@10 by  20 . \\nUnder the generative evaluation setting the proposed model + IR2 had better BLEU by 0.044 , better CIDEr by 0.033, better ROUGE by 0.032, and better METEOR by 0.029', 'type': 'abstractive'}, {'answer': 'Proposed model is better than both lexical based models by significan margin in all metrics: BLEU 0.261 vs 0.250, ROUGLE 0.162 vs 0.155 etc.', 'type': 'abstractive'}]", "main_doc": "1809.04960.pdf", "documents": "['1809.04960.pdf', '1911.12579.pdf', '2002.11910.pdf', '1804.11346.pdf', '1909.00754.pdf', '1909.00578.pdf', '1910.11235.pdf', '1912.01772.pdf', '1911.04952.pdf', '2003.07996.pdf', '1909.00175.pdf', '1707.05236.pdf', '1909.00694.pdf']"}
{"_id": "paper_tab_355", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the reported absolute performance gain on the RTE dataset when applying the 3-class NSP with PSP method, and how does it compare to BERTBase?", "answer": "[{'answer': ' improvement on the RTE dataset is significant, i.e., 4% absolute gain over the BERTBase', 'type': 'extractive'}, {'answer': 'The average score improved by 1.4 points over the previous best result.', 'type': 'abstractive'}]", "main_doc": "1909.03405.pdf", "documents": "['1909.03405.pdf', '1911.13066.pdf', '1909.00361.pdf', '1912.10435.pdf', '1711.11221.pdf', '1910.04269.pdf', '1801.05147.pdf', '2001.00137.pdf', '1909.13695.pdf', '1911.10049.pdf', '1910.12129.pdf', '1709.05413.pdf', '2004.03744.pdf', '1912.10806.pdf']"}
{"_id": "paper_tab_356", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the 14 industry categories that were used for classifying social media users in the industry-detection system presented in this paper?", "answer": "[{'answer': 'technology, religion, fashion, publishing, sports or recreation, real estate, agriculture/environment, law, security/military, tourism, construction, museums or libraries, banking/investment banking, automotive', 'type': 'abstractive'}, {'answer': 'Technology, Religion, Fashion, Publishing, Sports coach, Real Estate, Law, Environment, Tourism, Construction, Museums, Banking, Security, Automotive.', 'type': 'abstractive'}]", "main_doc": "1612.08205.pdf", "documents": "['1612.08205.pdf', '1905.07464.pdf', '1708.09609.pdf', '1904.09678.pdf', '1810.09774.pdf', '1810.12885.pdf', '1911.10049.pdf', '1711.02013.pdf']"}
{"_id": "paper_tab_357", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "On which specific datasets did the authors validate the performance of the conditional BERT contextual augmentation, particularly in the domains of sentiment analysis, subjectivity detection, opinion polarity, and question classification?", "answer": "[{'answer': 'SST (Stanford Sentiment Treebank), Subj (Subjectivity dataset), MPQA Opinion Corpus, RT is another movie review sentiment dataset, TREC is a dataset for classification of the six question types', 'type': 'extractive'}]", "main_doc": "1812.06705.pdf", "documents": "['1812.06705.pdf', '1912.10806.pdf', '1909.04002.pdf', '1812.01704.pdf', '1909.09270.pdf', '2001.08051.pdf', '1804.05918.pdf', '1909.08089.pdf']"}
{"_id": "paper_tab_358", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What metric is used to evaluate CBERT's performance on six different datasets across two classifier architectures?", "answer": "[{'answer': 'Accuracy across six datasets', 'type': 'abstractive'}]", "main_doc": "1812.06705.pdf", "documents": "['1812.06705.pdf', '1910.12129.pdf', '1702.03342.pdf', '1907.03060.pdf', '1808.09920.pdf', '1909.00175.pdf', '1910.06036.pdf', '1912.01673.pdf', '2003.04642.pdf', '1909.06162.pdf', '1810.06743.pdf', '2003.11645.pdf', '1605.08675.pdf', '1603.04513.pdf', '1911.02086.pdf', '1709.05413.pdf', '2001.05467.pdf']"}
{"_id": "paper_tab_359", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "In the experiments presented, are any other pretrained language models besides conditional BERT evaluated for contextual augmentation across all datasets?", "answer": "[{'answer': 'No', 'type': 'boolean'}]", "main_doc": "1812.06705.pdf", "documents": "['1812.06705.pdf', '1803.09230.pdf', '2002.05829.pdf', '1704.00939.pdf', '1912.10435.pdf', '1911.11951.pdf', '2002.11402.pdf', '1711.00106.pdf', '1909.11467.pdf', '1712.03556.pdf', '1910.06748.pdf', '1905.00563.pdf', '1804.00079.pdf', '1906.01081.pdf', '1911.03310.pdf', '2001.08051.pdf']"}
{"_id": "paper_tab_360", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which baseline models, both in ensemble and single variations, are the MPFN model compared against in the experimental results?", "answer": "[{'answer': 'SLQA, Rusalka, HMA Model (single), TriAN (single), jiangnan (ensemble), MITRE (ensemble), TriAN (ensemble), HMA Model (ensemble)', 'type': 'abstractive'}]", "main_doc": "1901.02257.pdf", "documents": "['1901.02257.pdf', '1807.07961.pdf', '2002.11402.pdf', '1704.06194.pdf', '1909.11687.pdf', '1910.14497.pdf', '2003.06044.pdf', '1908.06264.pdf', '1904.03288.pdf', '1811.12254.pdf', '2003.01769.pdf', '1711.02013.pdf', '1909.11467.pdf', '1701.00185.pdf', '1911.03597.pdf', '2003.03044.pdf', '2004.03788.pdf']"}
{"_id": "paper_tab_361", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the dataset's intent annotation results, do the intent labels show an imbalance in the distribution for the utterance-level AV intent detection task?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}]", "main_doc": "1904.10500.pdf", "documents": "['1904.10500.pdf', '1703.02507.pdf', '2003.07723.pdf', '1901.09755.pdf', '2002.01207.pdf', '1701.09123.pdf', '1911.00069.pdf', '2004.04721.pdf', '1604.00400.pdf', '1909.13695.pdf', '1908.06379.pdf', '1901.02257.pdf', '1611.02550.pdf', '1912.08960.pdf', '1808.09029.pdf', '1707.00110.pdf', '2002.06424.pdf']"}
{"_id": "paper_tab_362", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "According to the performance results, on which QA benchmarks did their proposed KBQA system achieve either state-of-the-art or near-state-of-the-art accuracy?", "answer": "[{'answer': 'SimpleQuestions, WebQSP', 'type': 'extractive'}, {'answer': 'WebQSP, SimpleQuestions', 'type': 'extractive'}]", "main_doc": "1704.06194.pdf", "documents": "['1704.06194.pdf', '2001.08051.pdf', '1707.03569.pdf', '1812.10479.pdf', '2001.00137.pdf']"}
{"_id": "paper_tab_363", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How much does the \"mixed fine tuning\" method improve BLEU-4 scores compared to standard fine tuning for the IWSLT-CE domain on the 2011, 2012, and 2013 test sets using the NTCIR-CE data for domain adaptation?", "answer": "[{'answer': '0.08 points on the 2011 test set, 0.44 points on the 2012 test set, 0.42 points on the 2013 test set for IWSLT-CE.', 'type': 'abstractive'}]", "main_doc": "1701.03214.pdf", "documents": "['1701.03214.pdf', '1703.06492.pdf', '1809.01541.pdf', '1908.11546.pdf', '1909.02480.pdf', '2002.01664.pdf', '1611.02550.pdf', '1904.09678.pdf', '1911.03597.pdf']"}
{"_id": "paper_tab_364", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How much does HAS-QA improve over traditional RC models like GA, BiDAF, and AQA on the QuasarT dataset in terms of EM and F1 scores?", "answer": "[{'answer': 'For example, in QuasarT, it improves 16.8% in EM score and 20.4% in F1 score. , For example, in QuasarT, it improves 4.6% in EM score and 3.5% in F1 score.', 'type': 'extractive'}]", "main_doc": "1901.03866.pdf", "documents": "['1901.03866.pdf', '1911.02821.pdf', '1904.05584.pdf', '2002.06675.pdf', '1707.08559.pdf', '1809.01541.pdf', '2002.10361.pdf', '1904.10503.pdf', '1908.07245.pdf', '1809.01202.pdf']"}
{"_id": "paper_tab_365", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How many corrected image-sentence pairs are there in the validation and test sets of the e-SNLI-VE-2.0 dataset with human-written natural language explanations?", "answer": "[{'answer': 'Totally 6980 validation and test image-sentence pairs have been corrected.', 'type': 'abstractive'}]", "main_doc": "2004.03744.pdf", "documents": "['2004.03744.pdf', '2001.05493.pdf', '1904.10500.pdf', '1904.07904.pdf', '1902.00330.pdf', '1909.00361.pdf', '1904.09678.pdf', '1810.00663.pdf', '1912.10011.pdf', '1808.09920.pdf', '2004.03354.pdf', '2003.03106.pdf', '1810.05241.pdf', '1912.08960.pdf', '2002.05829.pdf']"}
{"_id": "paper_tab_366", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How do different variants of ROUGE scores (e.g., ROUGE-1-P and ROUGE-3-F) correlate with human Pyramid scores using Pearson, Spearman, and Kendall correlations in the context of scientific article summarization?", "answer": "[{'answer': 'we observe that many variants of Rouge scores do not have high correlations with human pyramid scores', 'type': 'extractive'}, {'answer': 'Using Pearson corelation measure,  for example, ROUGE-1-P is 0.257 and ROUGE-3-F 0.878.', 'type': 'abstractive'}]", "main_doc": "1604.00400.pdf", "documents": "['1604.00400.pdf', '2001.06888.pdf', '1909.00512.pdf', '1704.00939.pdf', '1904.01608.pdf', '1909.00430.pdf', '1806.07711.pdf', '1611.04642.pdf', '2003.04642.pdf', '1908.11365.pdf']"}
{"_id": "paper_tab_367", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What language technologies are listed as being particularly challenging to adopt for extremely-low resource languages, such as Gondi, due to data collection difficulties?", "answer": "[{'answer': '- Font & Keyboard\\n- Speech-to-Text\\n- Text-to-Speech\\n- Text Prediction\\n- Spell Checker\\n- Grammar Checker\\n- Text Search\\n- Machine Translation\\n- Voice to Text Search\\n- Voice to Speech Search', 'type': 'abstractive'}]", "main_doc": "1912.03457.pdf", "documents": "['1912.03457.pdf', '1912.01772.pdf', '2003.03044.pdf', '1908.11365.pdf', '1909.00015.pdf', '1908.07816.pdf', '1809.03449.pdf', '1809.08298.pdf', '1707.03569.pdf', '1707.05236.pdf', '1904.07904.pdf']"}
{"_id": "paper_tab_368", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What was the lemmatization accuracy reported for the WikiNews test set in the paper that evaluated an Arabic lemmatizer?", "answer": "[{'answer': '97.32%', 'type': 'abstractive'}]", "main_doc": "1710.06700.pdf", "documents": "['1710.06700.pdf', '1711.00106.pdf', '2003.05377.pdf', '1908.06264.pdf']"}
{"_id": "paper_tab_369", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the Full Testing Set and Cleaned Testing Set accuracies achieved by TP-N2F on the AlgoLisp dataset, and how does this performance compare to LSTM-based Seq2Seq models?", "answer": "[{'answer': 'Full Testing Set accuracy: 84.02\\nCleaned Testing Set accuracy: 93.48', 'type': 'abstractive'}]", "main_doc": "1910.02339.pdf", "documents": "['1910.02339.pdf', '1804.08050.pdf', '1911.01799.pdf', '1804.05918.pdf', '1703.02507.pdf', '1910.03467.pdf', '1911.01680.pdf', '1910.03814.pdf', '1809.10644.pdf', '2004.01878.pdf', '1603.00968.pdf', '1705.01265.pdf']"}
{"_id": "paper_tab_370", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the Full Testing Set Accuracy and Cleaned Testing Set Accuracy of the TP-N2F model on the AlgoLisp dataset?", "answer": "[{'answer': 'Full Testing Set Accuracy: 84.02\\nCleaned Testing Set Accuracy: 93.48', 'type': 'abstractive'}]", "main_doc": "1910.02339.pdf", "documents": "['1910.02339.pdf', '1805.04033.pdf', '2003.03014.pdf', '2004.01878.pdf', '1909.01383.pdf', '1703.07090.pdf', '2003.05377.pdf', '1711.00106.pdf', '1612.08205.pdf', '1909.09484.pdf', '1909.11687.pdf', '1909.01013.pdf', '1909.00361.pdf', '1705.00108.pdf', '1909.00754.pdf']"}
{"_id": "paper_tab_371", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What operation and execution accuracies did the TP-N2F model achieve on the MathQA dataset in comparison to the SEQ2PROG model?", "answer": "[{'answer': 'Operation accuracy: 71.89\\nExecution accuracy: 55.95', 'type': 'abstractive'}]", "main_doc": "1910.02339.pdf", "documents": "['1910.02339.pdf', '1908.05828.pdf', '2004.03354.pdf', '1809.01202.pdf', '1911.08976.pdf', '1810.00663.pdf']"}
{"_id": "paper_tab_372", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What reduction in test perplexity does the MoE model achieve compared to the best published model after 10 epochs in the language modeling benchmark?", "answer": "[{'answer': 'Perpexity is improved from 34.7 to 28.0.', 'type': 'abstractive'}]", "main_doc": "1701.06538.pdf", "documents": "['1701.06538.pdf', '1603.00968.pdf', '1908.05828.pdf', '1605.08675.pdf', '1911.13066.pdf', '1910.04269.pdf', '1912.10435.pdf', '1912.13109.pdf', '1909.06937.pdf', '1910.08987.pdf', '1701.03214.pdf', '1703.06492.pdf', '1605.07683.pdf', '1911.07228.pdf', '1901.03866.pdf', '1708.09609.pdf']"}
{"_id": "paper_tab_373", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How did the accuracy and F1 score of the Transformer model on the Short Jokes dataset, as reported in the \"Humor Detection: A Transformer Gets the Last Laugh\" paper, compare to previous models like CNNs, and what was the magnitude of improvement?", "answer": "[{'answer': 'It had the highest accuracy comparing to all datasets 0.986% and It had the highest improvement comparing to previous methods on the same dataset by 8%', 'type': 'abstractive'}]", "main_doc": "1909.00252.pdf", "documents": "['1909.00252.pdf', '1906.01081.pdf', '1908.07245.pdf', '1910.12795.pdf', '1912.10011.pdf', '1910.03814.pdf', '2003.05377.pdf', '2002.02492.pdf', '1809.01541.pdf', '1911.04952.pdf', '1909.03135.pdf']"}
{"_id": "paper_tab_374", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "In the experiment where you modified the layer configuration to 6-6-4 in the $N_{src}$-$N_{mt}$-$N_{pe}$ setup, which result demonstrated that reducing the number of layers in the final decoder ($N_{pe}$) does not notably affect APE performance, as detailed in Section 5.1?", "answer": "[{'answer': 'Exp. 5.1', 'type': 'extractive'}]", "main_doc": "1908.06151.pdf", "documents": "['1908.06151.pdf', '1810.12885.pdf', '1911.12579.pdf', '1810.05241.pdf', '1909.09270.pdf', '2002.05058.pdf', '1909.13375.pdf', '1703.07090.pdf', '1812.06705.pdf', '1912.01772.pdf', '1909.06937.pdf', '1912.01214.pdf']"}
{"_id": "paper_tab_375", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the experimental evidence presented in the paper, how does reducing the number of layers in the $enc_{src \\rightarrow mt}$ encoder block (Exp. 5.2) affect BLEU and TER scores, particularly for test2016 and test2017, and how do these results compare to the impact of reducing the number of decoder layers as discussed in Exp. 5.1?", "answer": "[{'answer': 'comparing to the results from reducing the number of layers in the decoder, the BLEU score was 69.93 which is less than 1% in case of test2016 and in case of test2017 it was less by 0.2 %. In terms of TER it had higher score by 0.7 in case of test2016 and 0.1 in case of test2017. ', 'type': 'abstractive'}]", "main_doc": "1908.06151.pdf", "documents": "['1908.06151.pdf', '1903.09588.pdf', '1809.01202.pdf', '1909.09484.pdf', '2002.00652.pdf', '1910.03467.pdf', '1810.00663.pdf']"}
{"_id": "paper_tab_376", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "By how much does the combined voting-based model of ER-CNN and R-RNN surpass the top state-of-the-art performance on the SemEval 2010 relation classification task?", "answer": "[{'answer': '0.8% F1 better than the best state-of-the-art', 'type': 'abstractive'}, {'answer': 'Best proposed model achieves F1 score of 84.9 compared to best previous result of 84.1.', 'type': 'abstractive'}]", "main_doc": "1605.07333.pdf", "documents": "['1605.07333.pdf', '1902.00330.pdf', '1911.08962.pdf', '1909.09587.pdf', '1604.00400.pdf', '2002.06675.pdf', '1912.03457.pdf', '1711.00106.pdf', '1603.04513.pdf', '1909.09270.pdf', '1611.03382.pdf']"}
