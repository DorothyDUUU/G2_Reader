{"_id": "paper_tab_0", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What baseline approaches using state-of-the-art PDTB taggers were employed for the evaluation of causality prediction in the automatic causal explanation analysis pipeline?", "answer": "[{'answer': 'state-of-the-art PDTB taggers', 'type': 'extractive'}, {'answer': 'Linear SVM, RBF SVM, and Random Forest', 'type': 'abstractive'}]", "main_doc": "1809.01202.pdf", "documents": "['1809.01202.pdf', '1908.06083.pdf', '2002.01984.pdf', '1809.03449.pdf', '2002.06675.pdf', '2002.01207.pdf', '1912.01772.pdf', '1809.02279.pdf', '1904.05584.pdf', '1704.05907.pdf', '1910.14497.pdf', '1903.00172.pdf', '1911.05153.pdf', '1901.04899.pdf']"}
{"_id": "paper_tab_1", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the joint goal accuracy and inference time complexity, does the model demonstrate superior performance in the single-domain WoZ2.0 test set or the multi-domain MultiWoZ test set?", "answer": "[{'answer': 'single-domain setting', 'type': 'abstractive'}]", "main_doc": "1909.00754.pdf", "documents": "['1909.00754.pdf', '1710.09340.pdf', '2004.01980.pdf', '1910.12129.pdf', '1701.06538.pdf', '1811.12254.pdf', '1902.10525.pdf']"}
{"_id": "paper_tab_2", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which hierarchical model variant achieves a higher BLEU score of 17.5 compared to the Flat scenario (16.7)?", "answer": "[{'answer': 'Hierarchical-k', 'type': 'extractive'}]", "main_doc": "1912.10011.pdf", "documents": "['1912.10011.pdf', '1809.08298.pdf', '1810.12085.pdf', '1905.11901.pdf', '1908.10084.pdf', '2001.08051.pdf', '1909.08041.pdf', '1910.14497.pdf', '1909.08859.pdf', '2004.04721.pdf', '1809.00540.pdf', '1909.11297.pdf', '1610.00879.pdf', '2002.02070.pdf', '2002.01359.pdf', '1811.02906.pdf']"}
{"_id": "paper_tab_3", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What were the F-scores of the best performing model on the NALCS (English) and LMS (Traditional Chinese) test sets, as reported in the video highlight prediction paper using multimodal and multilingual audience chat reactions?", "answer": "[{'answer': 'Best model achieved F-score 74.7 on NALCS and F-score of 70.0 on LMS on test set', 'type': 'abstractive'}]", "main_doc": "1707.08559.pdf", "documents": "['1707.08559.pdf', '1909.03135.pdf', '2003.11563.pdf', '1707.05236.pdf', '1906.05474.pdf', '2002.01664.pdf', '1901.08079.pdf', '2002.08899.pdf', '1904.05584.pdf', '1810.12085.pdf', '1712.00991.pdf', '1901.05280.pdf', '1709.05413.pdf', '1901.02262.pdf', '1701.06538.pdf', '1909.00252.pdf', '1707.03569.pdf', '1912.01772.pdf']"}
{"_id": "paper_tab_4", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the average token count of claims in the PERSPECTRUM dataset?", "answer": "[{'answer': 'Average claim length is 8.9 tokens.', 'type': 'abstractive'}]", "main_doc": "1906.03538.pdf", "documents": "['1906.03538.pdf', '1705.00108.pdf', '1901.09755.pdf', '1908.05828.pdf', '2001.08051.pdf', '1705.01214.pdf', '1909.08089.pdf', '2002.08899.pdf']"}
{"_id": "paper_tab_5", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does incorporating shallow syntax-aware embeddings lead to better performance on the sentiment classification task compared to using standard ELMo embeddings?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'No', 'type': 'boolean'}]", "main_doc": "1908.11047.pdf", "documents": "['1908.11047.pdf', '1910.03467.pdf', '1703.06492.pdf', '1810.12885.pdf', '1906.11180.pdf', '1809.10644.pdf']"}
{"_id": "paper_tab_6", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Out of the 9 probe tasks, how many tasks show better performance with the mSynC model compared to baseline ELMo-transformer embeddings in the \"Shallow Syntax in Deep Water\" study?", "answer": "[{'answer': 'performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks', 'type': 'extractive'}, {'answer': '3', 'type': 'abstractive'}]", "main_doc": "1908.11047.pdf", "documents": "['1908.11047.pdf', '2003.11645.pdf', '1906.01081.pdf', '1904.10500.pdf', '1911.08673.pdf', '1907.09369.pdf']"}
{"_id": "paper_tab_7", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What specific set of black-box probes, as described in the evidence section of this paper, were used to analyze the role of shallow syntactic awareness in the enhanced ELMo architecture's embeddings for capturing linguistic properties?", "answer": "[{'answer': 'CCG Supertagging CCGBank , PTB part-of-speech tagging, EWT part-of-speech tagging,\\nChunking, Named Entity Recognition, Semantic Tagging, Grammar Error Detection, Preposition Supersense Role, Preposition Supersense Function, Event Factuality Detection', 'type': 'abstractive'}, {'answer': 'Probes are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases.', 'type': 'extractive'}]", "main_doc": "1908.11047.pdf", "documents": "['1908.11047.pdf', '2001.10161.pdf', '1911.11951.pdf', '1911.01799.pdf', '1908.07816.pdf', '1811.02906.pdf', '1809.02279.pdf', '1904.10500.pdf', '1910.10288.pdf']"}
{"_id": "paper_tab_8", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the findings in *Shallow Syntax in Deep Water*, how does mSynC's performance on downstream NLP tasks, especially in chunk tag prediction, compare to that of ELMo-transformer, and what does this imply about the utility of shallow syntactic features?", "answer": "[{'answer': 'only modest gains on three of the four downstream tasks', 'type': 'extractive'}, {'answer': ' the performance differences across all tasks are small enough ', 'type': 'extractive'}]", "main_doc": "1908.11047.pdf", "documents": "['1908.11047.pdf', '1711.11221.pdf', '1611.03382.pdf', '1709.10367.pdf', '1910.00458.pdf', '1701.02877.pdf', '1911.02086.pdf', '1909.09484.pdf']"}
{"_id": "paper_tab_9", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How were the six predefined proficiency indicators scored in the 2017/2018 recordings of the TLT-school corpus for both spoken and written items, and what values did human experts assign to reflect proficiency levels?", "answer": "[{'answer': 'They used 6 indicators for proficiency (same for written and spoken) each marked by bad, medium or good by one expert.', 'type': 'abstractive'}]", "main_doc": "2001.08051.pdf", "documents": "['2001.08051.pdf', '1909.08041.pdf', '1901.02257.pdf', '1912.10435.pdf', '1910.06748.pdf', '1909.00754.pdf', '2001.10161.pdf', '1903.00172.pdf', '1712.03547.pdf', '1904.10503.pdf', '1804.07789.pdf', '2003.04866.pdf', '1901.03866.pdf', '1909.08824.pdf', '1902.00672.pdf', '2002.01664.pdf', '1902.00330.pdf', '1908.05828.pdf']"}
{"_id": "paper_tab_10", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the six linguistic proficiency indicators outlined in the TLT-school corpus that human experts used to assess the English and German utterances of non-native speaking students?", "answer": "[{'answer': '6 indicators:\\n- lexical richness\\n- pronunciation and fluency\\n- syntactical correctness\\n- fulfillment of delivery\\n- coherence and cohesion\\n- communicative, descriptive, narrative skills', 'type': 'abstractive'}]", "main_doc": "2001.08051.pdf", "documents": "['2001.08051.pdf', '1901.05280.pdf', '1712.03547.pdf', '1809.00540.pdf', '1910.12795.pdf', '1603.00968.pdf', '2001.05970.pdf', '1910.04269.pdf', '1806.04330.pdf', '2002.02492.pdf', '1908.08345.pdf', '1901.04899.pdf', '1811.12254.pdf', '1909.13695.pdf', '1804.08050.pdf']"}
{"_id": "paper_tab_11", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the Word Error Rates (WER) achieved by the adapted Automatic Speech Recognition (ASR) system on the non-native English and German children's speech evaluation data in the TLT-school corpus?", "answer": "[{'answer': 'Accuracy not available: WER results are reported 42.6 German, 35.9 English', 'type': 'abstractive'}]", "main_doc": "2001.08051.pdf", "documents": "['2001.08051.pdf', '1804.08139.pdf', '1909.08041.pdf', '1809.09194.pdf', '1804.08050.pdf', '1701.00185.pdf']"}
{"_id": "paper_tab_12", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What metric is used to evaluate the performance of the adapted ASR system on non-native children's English and German speech?", "answer": "[{'answer': 'Speech recognition system is evaluated using WER metric.', 'type': 'abstractive'}]", "main_doc": "2001.08051.pdf", "documents": "['2001.08051.pdf', '1910.06036.pdf', '2004.01878.pdf', '2004.01980.pdf', '1908.05434.pdf', '1911.12579.pdf', '1911.08962.pdf', '1901.09755.pdf', '1610.07809.pdf', '1709.10217.pdf', '1910.03814.pdf', '1902.09393.pdf', '1909.01013.pdf', '1812.10479.pdf', '1704.05907.pdf', '1710.06700.pdf', '2001.10161.pdf']"}
{"_id": "paper_tab_13", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How many utterances were manually transcribed in the TLT-school corpus, across both English and German languages, including train and evaluation sets?", "answer": "[{'answer': 'Total number of transcribed utterances including Train and Test for both Eng and Ger language is 5562 (2188 cleaned)', 'type': 'abstractive'}]", "main_doc": "2001.08051.pdf", "documents": "['2001.08051.pdf', '1809.00540.pdf', '1809.06537.pdf', '1711.00106.pdf', '1811.12254.pdf', '2002.11402.pdf', '1908.11365.pdf', '1904.03288.pdf', '1808.03430.pdf', '1606.00189.pdf', '1910.11769.pdf', '1809.09194.pdf', '1902.09314.pdf', '1909.06937.pdf', '1909.11687.pdf', '1902.09393.pdf', '2002.06424.pdf', '1809.04960.pdf']"}
{"_id": "paper_tab_14", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the baseline models compared in this paper, which summarizes prior work on span and dependency SRL, including models like Fitzgerald2015?", "answer": "[{'answer': '2008 Punyakanok et al. \\n2009 Zhao et al. + ME \\n2008 Toutanova et al. \\n2010 Bjorkelund et al.  \\n2015 FitzGerald et al. \\n2015 Zhou and Xu \\n2016 Roth and Lapata \\n2017 He et al. \\n2017 Marcheggiani et al.\\n2017 Marcheggiani and Titov \\n2018 Tan et al. \\n2018 He et al. \\n2018 Strubell et al. \\n2018 Cai et al. \\n2018 He et al. \\n2018 Li et al. \\n', 'type': 'abstractive'}]", "main_doc": "1901.05280.pdf", "documents": "['1901.05280.pdf', '1910.08987.pdf', '1911.10049.pdf', '1706.08032.pdf', '1806.07711.pdf', '2002.04181.pdf', '1908.06264.pdf', '1910.06036.pdf', '1904.05584.pdf', '2003.12738.pdf', '1909.06162.pdf', '1704.06194.pdf', '1809.04960.pdf', '1911.00069.pdf', '2001.05970.pdf']"}
{"_id": "paper_tab_15", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the eight biomedical NER tasks that were used to compare the performance of GreenBioBERT and BioBERT in the context of inexpensive domain adaptation?", "answer": "[{'answer': 'BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800', 'type': 'abstractive'}, {'answer': 'BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800', 'type': 'abstractive'}]", "main_doc": "2004.03354.pdf", "documents": "['2004.03354.pdf', '1903.09722.pdf', '2003.11645.pdf', '2002.01359.pdf', '1712.03547.pdf', '1908.05434.pdf', '1906.03538.pdf', '1908.10084.pdf', '1809.09795.pdf', '2002.04181.pdf', '2003.03106.pdf', '1908.06267.pdf', '1702.03342.pdf', '1711.00106.pdf']"}
{"_id": "paper_tab_16", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What specific metadata fields are provided, in addition to the claim and its label, in the example claim instance from the MultiFC dataset?", "answer": "[{'answer': 'besides claim, label and claim url, it also includes a claim ID, reason, category, speaker, checker, tags, claim entities, article title, publish data and claim date', 'type': 'abstractive'}]", "main_doc": "1909.03242.pdf", "documents": "['1909.03242.pdf', '1912.01772.pdf', '1905.10810.pdf', '2002.01207.pdf', '1611.04642.pdf', '1811.01088.pdf', '1906.01081.pdf', '1809.05752.pdf', '1704.08960.pdf']"}
{"_id": "paper_tab_17", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the NMI values between the clustered tone contours and the ground truth tone categories for all syllables in both Mandarin and Cantonese, as reported in your phonemic tone clustering analysis?", "answer": "[{'answer': 'NMI between cluster assignments and ground truth tones for all sylables is:\\nMandarin: 0.641\\nCantonese: 0.464', 'type': 'abstractive'}]", "main_doc": "1910.08987.pdf", "documents": "['1910.08987.pdf', '1903.09588.pdf', '1705.01265.pdf', '1910.00912.pdf', '1701.06538.pdf', '1912.10806.pdf', '1911.03597.pdf', '2003.11563.pdf', '1904.05584.pdf', '1910.12795.pdf', '2004.04721.pdf']"}
{"_id": "paper_tab_18", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the paper compare the execution time of their GDSConv-based model with other state-of-the-art keyword spotting models that use traditional acoustic feature extraction techniques?", "answer": "[{'answer': 'No', 'type': 'boolean'}]", "main_doc": "1911.02086.pdf", "documents": "['1911.02086.pdf', '1806.04330.pdf', '2003.03014.pdf', '2002.02492.pdf', '1911.08976.pdf', '1910.06592.pdf', '1811.02906.pdf', '1911.01680.pdf', '1912.01214.pdf', '1908.06264.pdf', '2002.06644.pdf', '1904.10500.pdf', '1710.09340.pdf']"}
{"_id": "paper_tab_19", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How much do the F1 scores improve when using the hybrid attention method and DCA compared to BiDAF and DCN on the SQuAD dataset?", "answer": "[{'answer': 'In terms of F1 score, the Hybrid approach improved by 23.47% and 1.39% on BiDAF and DCN respectively. The DCA approach improved by 23.2% and 1.12% on BiDAF and DCN respectively.', 'type': 'abstractive'}]", "main_doc": "1803.09230.pdf", "documents": "['1803.09230.pdf', '1809.00530.pdf', '1712.05999.pdf', '1910.00912.pdf', '1904.10503.pdf', '2002.06424.pdf', '1911.05153.pdf']"}
{"_id": "paper_tab_20", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the accuracy score of the highest-performing McM model on the 12-class bilingual SMS dataset?", "answer": "[{'answer': 'the best performing model obtained an accuracy of 0.86', 'type': 'abstractive'}]", "main_doc": "1911.13066.pdf", "documents": "['1911.13066.pdf', '1902.09666.pdf', '1812.06864.pdf', '1810.03459.pdf', '1701.09123.pdf', '1902.00330.pdf', '1910.08210.pdf', '1810.10254.pdf', '1804.08139.pdf', '1911.08962.pdf', '1908.08345.pdf']"}
{"_id": "paper_tab_21", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the 12 class labels and their distribution (as given in %) for the bilingual Roman Urdu-English SMS feedback dataset discussed in the study on McM deep learning model for SMS classification?", "answer": "[{'answer': 'Appreciation, Satisfied, Peripheral complaint, Demanded inquiry, Corruption, Lagged response, Unresponsive, Medicine payment, Adverse behavior, Grievance ascribed and Obnoxious/irrelevant', 'type': 'abstractive'}]", "main_doc": "1911.13066.pdf", "documents": "['1911.13066.pdf', '2001.10161.pdf', '2003.12218.pdf', '1809.09795.pdf', '1810.03459.pdf', '2002.02070.pdf', '1910.08210.pdf', '1906.10551.pdf', '1905.10810.pdf', '1804.05918.pdf', '1812.06705.pdf', '1912.03457.pdf', '2003.08385.pdf', '2001.08868.pdf', '1909.00361.pdf', '1911.08962.pdf']"}
{"_id": "paper_tab_22", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What metrics (precision, recall, F1) are reported for evaluating the NER system's performance in the CoNLL 2003 English results?", "answer": "[{'answer': 'Precision, Recall, F1', 'type': 'abstractive'}]", "main_doc": "1701.09123.pdf", "documents": "['1701.09123.pdf', '2003.01769.pdf', '1805.04033.pdf', '1711.11221.pdf']"}
{"_id": "paper_tab_23", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the datasets that were used for training, development, and out-of-domain evaluation in the multilingual named entity recognition experiments, including corpora such as MUC7, SONAR-1, and Ancora 2.0?", "answer": "[{'answer': 'CoNLL 2003, GermEval 2014, CoNLL 2002, Egunkaria, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0', 'type': 'abstractive'}]", "main_doc": "1701.09123.pdf", "documents": "['1701.09123.pdf', '1809.06537.pdf', '1707.05236.pdf', '1905.10810.pdf', '1904.05584.pdf', '2004.04721.pdf', '1812.10479.pdf', '1910.08210.pdf', '1911.02086.pdf', '1809.00540.pdf', '1803.09230.pdf', '1810.09774.pdf', '1701.05574.pdf', '1603.07044.pdf', '1808.09920.pdf']"}
{"_id": "paper_tab_24", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the total number of documents in the dataset in the paper on modeling stock movements using a recurrent state transition model influenced by news events?", "answer": "[{'answer': '553,451 documents', 'type': 'abstractive'}]", "main_doc": "2004.01878.pdf", "documents": "['2004.01878.pdf', '1909.00754.pdf', '1902.10525.pdf', '1707.05236.pdf', '1911.08962.pdf', '1911.02086.pdf', '1909.11687.pdf', '1605.07683.pdf', '1909.06162.pdf', '1809.06537.pdf', '2001.00137.pdf', '1812.10479.pdf', '2001.06286.pdf', '1909.03242.pdf', '1701.03214.pdf']"}
{"_id": "paper_tab_25", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which 12 languages, including both major and less-resourced ones, are listed summarizing the language datasets in the Multi-SimLex lexical resource?", "answer": "[{'answer': 'Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese', 'type': 'abstractive'}, {'answer': 'Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese', 'type': 'abstractive'}]", "main_doc": "2003.04866.pdf", "documents": "['2003.04866.pdf', '1911.01680.pdf', '1901.01010.pdf', '1903.00172.pdf', '1809.06537.pdf', '1603.07044.pdf', '1901.08079.pdf', '2001.05970.pdf']"}
{"_id": "paper_tab_26", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the exact EM and F1 score achieved by the proposed adversarial domain adaptation model to address ASR errors, as reported in the study on mitigating ASR errors in spoken question answering?", "answer": "[{'answer': 'Best results authors obtain is EM 51.10 and F1 63.11', 'type': 'abstractive'}, {'answer': 'EM Score of 51.10', 'type': 'abstractive'}]", "main_doc": "1904.07904.pdf", "documents": "['1904.07904.pdf', '2004.04721.pdf', '1910.06036.pdf', '2003.03106.pdf', '1704.05907.pdf', '2001.08868.pdf', '1603.00968.pdf', '1902.00672.pdf', '1903.09722.pdf', '2002.01664.pdf', '1804.11346.pdf']"}
{"_id": "paper_tab_27", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What baseline models are compared for pun detection and location in terms of precision, recall, and F1-score against the proposed approach?", "answer": "[{'answer': 'They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF.', 'type': 'abstractive'}]", "main_doc": "1909.00175.pdf", "documents": "['1909.00175.pdf', '1912.01772.pdf', '1805.03710.pdf', '1908.11365.pdf', '2002.01359.pdf', '1703.02507.pdf']"}
{"_id": "paper_tab_28", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What were the token counts for the English and Russian training corpora in the experiments comparing lemmatized and non-lemmatized inputs for word sense disambiguation?", "answer": "[{'answer': '2174000000, 989000000', 'type': 'abstractive'}, {'answer': '2174 million tokens for English and 989 million tokens for Russian', 'type': 'abstractive'}]", "main_doc": "1909.03135.pdf", "documents": "['1909.03135.pdf', '1908.11365.pdf', '1709.10217.pdf', '1908.05434.pdf', '1808.09029.pdf', '1809.06537.pdf', '1611.04798.pdf', '1805.03710.pdf', '1912.01214.pdf']"}
{"_id": "paper_tab_29", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What baseline models, including LSTM, HRED, VHRED (with and without attention), and others reported in prior work, are compared to the AvgOut-based models in terms of F1 score and dialogue diversity/relevance?", "answer": "[{'answer': 'LSTMs with and without attention, HRED, VHRED with and without attention, MMI and Reranking-RL', 'type': 'abstractive'}]", "main_doc": "2001.05467.pdf", "documents": "['2001.05467.pdf', '1610.00879.pdf', '1605.07683.pdf', '2004.01980.pdf', '1909.01013.pdf', '1603.04513.pdf', '1911.03597.pdf', '2001.00137.pdf', '1910.11235.pdf', '1907.09369.pdf', '1704.08960.pdf', '1605.08675.pdf', '1910.12129.pdf', '2002.00652.pdf', '2002.06424.pdf', '1912.01673.pdf']"}
{"_id": "paper_tab_30", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What is the reported improvement in both diversity and relevance scores for the MinAvgOut, LFT, RL, and hybrid models compared to the base LSTM-RNN?", "answer": "[{'answer': 'on diversity 6.87 and on relevance 4.6 points higher', 'type': 'abstractive'}]", "main_doc": "2001.05467.pdf", "documents": "['2001.05467.pdf', '2002.04181.pdf', '1704.06194.pdf', '1901.05280.pdf']"}
{"_id": "paper_tab_31", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the ROUGE-1, ROUGE-2, and ROUGE-L scores achieved by the best-performing abstractive model on the CNN/DailyMail, XSum, and NYT datasets, as reported in the \"Text Summarization with Pretrained Encoders\" paper?", "answer": "[{'answer': 'Best results on unigram:\\nCNN/Daily Mail: Rogue F1 43.85\\nNYT: Rogue Recall 49.02\\nXSum: Rogue F1 38.81', 'type': 'abstractive'}, {'answer': 'Highest scores for ROUGE-1, ROUGE-2 and ROUGE-L on CNN/DailyMail test set are 43.85, 20.34 and 39.90 respectively; on the XSum test set 38.81, 16.50 and 31.27 and on the NYT test set 49.02, 31.02 and 45.55', 'type': 'abstractive'}]", "main_doc": "1908.08345.pdf", "documents": "['1908.08345.pdf', '1908.11047.pdf', '1909.01247.pdf', '1809.02279.pdf', '2003.05377.pdf', '1911.01680.pdf', '1910.08987.pdf', '1910.00912.pdf', '1910.03467.pdf', '1708.09609.pdf', '2002.00652.pdf', '1909.03405.pdf', '1910.14537.pdf', '2001.06888.pdf', '2002.01359.pdf', '2004.03744.pdf', '1603.04513.pdf']"}
{"_id": "paper_tab_32", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which dataset, consisting of 1,312 error-annotated sentences with 30,144 tokens and using the M2 Scorer v3.2 for evaluation, did the authors employ for grammatical error correction in their neural network translation models study?", "answer": "[{'answer': 'CoNLL 2014', 'type': 'extractive'}]", "main_doc": "1606.00189.pdf", "documents": "['1606.00189.pdf', '1801.05147.pdf', '1904.10500.pdf', '1905.10810.pdf', '1809.01202.pdf', '1910.14497.pdf', '2001.08051.pdf', '1909.00512.pdf', '1910.08987.pdf']"}
{"_id": "paper_tab_33", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the joint model combining text and visual features, as described in the paper's experimental results, outperform text-only and visual-only models on both Wikipedia articles and arXiv subsets?", "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'No', 'type': 'boolean'}]", "main_doc": "1901.01010.pdf", "documents": "['1901.01010.pdf', '2002.06424.pdf', '1903.09722.pdf', '1809.01541.pdf', '2001.10161.pdf', '1909.01383.pdf', '1804.11346.pdf', '1812.06864.pdf', '1612.08205.pdf', '1606.00189.pdf']"}
{"_id": "paper_tab_34", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the exact accuracy results on the Wikipedia dataset, as well as the peer-reviewed Archive AI, Archive Computation and Language, and Archive Machine Learning papers, as reported in the document quality assessment paper?", "answer": "[{'answer': '59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers', 'type': 'abstractive'}]", "main_doc": "1901.01010.pdf", "documents": "['1901.01010.pdf', '1901.03866.pdf', '1909.00361.pdf', '1912.01214.pdf', '1705.08142.pdf', '1811.01088.pdf', '1911.07228.pdf', '1911.01680.pdf', '1603.00968.pdf', '1906.03538.pdf', '2003.03014.pdf', '1612.08205.pdf']"}
{"_id": "paper_tab_35", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which datasets are used to report the link prediction results for the Embedded Knowledge Graph Network (EKGN) model in the \"Link Prediction using Embedded Knowledge Graphs\" paper?", "answer": "[{'answer': 'WN18 and FB15k', 'type': 'abstractive'}]", "main_doc": "1611.04642.pdf", "documents": "['1611.04642.pdf', '2004.01980.pdf', '1603.00968.pdf', '1911.07555.pdf', '1910.04269.pdf', '1611.00514.pdf', '2003.11645.pdf', '1910.03467.pdf', '1910.11235.pdf', '1910.11204.pdf', '1808.03430.pdf']"}
{"_id": "paper_tab_36", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Based on the analysis of syntactic patterns in WordNet's glosses, how many distinct semantic roles are introduced?", "answer": "[{'answer': '12', 'type': 'abstractive'}]", "main_doc": "1806.07711.pdf", "documents": "['1806.07711.pdf', '1901.09755.pdf', '2003.07996.pdf', '1809.08298.pdf', '1911.01680.pdf', '1702.03342.pdf', '1909.00694.pdf', '1810.05241.pdf', '1909.06162.pdf', '1804.05918.pdf', '1910.06036.pdf', '2003.11563.pdf', '1912.00864.pdf', '1810.12085.pdf', '1810.12196.pdf', '1911.00069.pdf', '1909.05855.pdf']"}
{"_id": "paper_tab_37", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What specific HPI categories are outlined under the Annotation Instructions for the LSTM model's task of extractive summarization in this study?", "answer": "[{'answer': 'Demographics Age, DiagnosisHistory, MedicationHistory, ProcedureHistory, Symptoms/Signs, Vitals/Labs, Procedures/Results, Meds/Treatments, Movement, Other.', 'type': 'abstractive'}, {'answer': 'Demographics, Diagnosis History, Medication History, Procedure History, Symptoms, Labs, Procedures, Treatments, Hospital movements, and others', 'type': 'abstractive'}]", "main_doc": "1810.12085.pdf", "documents": "['1810.12085.pdf', '1901.03866.pdf', '1909.08041.pdf', '1902.09314.pdf', '1909.01383.pdf', '1902.00672.pdf', '1912.00864.pdf', '2003.04642.pdf', '1608.06757.pdf', '1804.08139.pdf']"}
{"_id": "paper_tab_38", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Did the evaluation of the LSTM model on the 515 annotated history of present illness notes include a comparison with other existing extractive summarization techniques?", "answer": "[{'answer': 'No', 'type': 'boolean'}]", "main_doc": "1810.12085.pdf", "documents": "['1810.12085.pdf', '2001.08868.pdf', '1709.10217.pdf', '1909.00105.pdf', '1909.00175.pdf', '1909.01013.pdf', '2003.04866.pdf', '1901.02257.pdf', '1603.07044.pdf', '1707.08559.pdf', '1911.01680.pdf', '1811.12254.pdf', '1910.14497.pdf', '1901.04899.pdf']"}
{"_id": "paper_tab_39", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "According to previous methods for POS tagging and lemmatization in Czech text processing, which were outperformed by the BERT and Flair-based approaches?", "answer": "[{'answer': 'Table TABREF44, Table TABREF44, Table TABREF47, Table TABREF47', 'type': 'extractive'}]", "main_doc": "1909.03544.pdf", "documents": "['1909.03544.pdf', '1910.06036.pdf', '1909.00105.pdf', '1912.10435.pdf', '1809.04960.pdf', '1909.11467.pdf']"}
{"_id": "paper_tab_40", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "According to the ROUGE metrics, which baseline models are outperformed by the proposed regularization approach in the context of abstractive Chinese social media text summarization?", "answer": "[{'answer': 'RNN-context, SRB, CopyNet, RNN-distract, DRGD', 'type': 'abstractive'}]", "main_doc": "1805.04033.pdf", "documents": "['1805.04033.pdf', '1603.07044.pdf', '1910.12795.pdf', '1902.10525.pdf', '1904.05584.pdf', '1909.06162.pdf', '1711.11221.pdf', '1809.10644.pdf', '2003.06044.pdf', '1909.03242.pdf', '1912.01673.pdf', '1909.09484.pdf', '1606.00189.pdf', '1809.01541.pdf']"}
{"_id": "paper_tab_41", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How did participants in the evaluations on interactive fiction generation rate the neural model's performance in terms of coherence, genre resemblance, and overall quality compared to rule-based methods and human-made games across both mystery and fairy-tale genres?", "answer": "[{'answer': 'the neural approach is generally preferred by a greater percentage of participants than the rules or random, human-made game outperforms them all', 'type': 'extractive'}]", "main_doc": "2001.10161.pdf", "documents": "['2001.10161.pdf', '1801.05147.pdf', '1611.02550.pdf', '1909.00578.pdf', '1810.03459.pdf', '1604.00400.pdf', '1709.10367.pdf', '2004.01980.pdf', '1911.00069.pdf', '2002.11402.pdf', '1707.03569.pdf', '2002.02070.pdf']"}
{"_id": "paper_tab_42", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What specific evaluation metrics, including unlabeled INLINEFORM0 scores, are reported for assessing the performance of your compound probabilistic context-free grammar model compared to baseline models in unsupervised parsing experiments on English and Chinese?", "answer": "[{'answer': 'INLINEFORM0 scores', 'type': 'extractive'}, {'answer': 'Unlabeled sentence-level F1, perplexity, grammatically judgment performance', 'type': 'abstractive'}]", "main_doc": "1906.10225.pdf", "documents": "['1906.10225.pdf', '1908.05828.pdf', '1704.00939.pdf', '1912.13109.pdf', '1909.13714.pdf', '1611.00514.pdf', '1912.01772.pdf', '1910.06592.pdf', '1810.12085.pdf', '1804.08050.pdf', '2001.10161.pdf', '1911.03597.pdf', '2004.01980.pdf', '1805.04033.pdf', '1909.00694.pdf', '1904.09678.pdf']"}
{"_id": "paper_tab_43", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What are the precision scores for semantic typing and entity matching as reported in the paper, specifically on the S-Lite and R-Lite datasets compared to the baselines?", "answer": "[{'answer': '0.8320 on semantic typing, 0.7194 on entity matching', 'type': 'abstractive'}]", "main_doc": "1906.11180.pdf", "documents": "['1906.11180.pdf', '2004.01980.pdf', '1909.08824.pdf', '2003.04866.pdf', '1910.08210.pdf']"}
{"_id": "paper_tab_44", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Does the paper summarize any transformer-based models such as BERT or GPT for sentence pair modeling?", "answer": "[{'answer': 'No', 'type': 'boolean'}, {'answer': 'No', 'type': 'boolean'}]", "main_doc": "1806.04330.pdf", "documents": "['1806.04330.pdf', '1902.10525.pdf', '1901.02262.pdf', '1910.04269.pdf', '2003.07996.pdf', '2003.06044.pdf', '1706.08032.pdf']"}
{"_id": "paper_tab_45", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "How are non-standard pronunciations and mispronunciations annotated in the transcriptions of the 142-hour Mapudungun medical conversations corpus provided for computational linguistic experiments?", "answer": "[{'answer': 'Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation.', 'type': 'abstractive'}]", "main_doc": "1912.01772.pdf", "documents": "['1912.01772.pdf', '1812.06705.pdf', '1909.00754.pdf', '1810.12885.pdf', '2002.05829.pdf', '1701.02877.pdf', '1904.10503.pdf', '1912.03457.pdf', '1603.04513.pdf', '1901.05280.pdf', '1905.00563.pdf', '1912.08960.pdf', '2002.06675.pdf', '2004.03788.pdf']"}
{"_id": "paper_tab_46", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Which NER systems were evaluated with micro-precision, recall, and F1 scores on datasets like CoNLL2003, KORE50, ACE2004, and MSNBC in the 'Robust Named Entity Recognition in Idiosyncratic Domains' paper?", "answer": "[{'answer': 'Babelfy, DBpedia Spotlight, Entityclassifier.eu, FOX, LingPipe MUC-7, NERD-ML, Stanford NER, TagMe 2', 'type': 'abstractive'}]", "main_doc": "1608.06757.pdf", "documents": "['1608.06757.pdf', '1908.07816.pdf', '1704.06194.pdf', '2004.01980.pdf', '1903.09588.pdf', '1704.05907.pdf', '1910.12129.pdf', '1701.02877.pdf', '1812.06864.pdf']"}
{"_id": "paper_tab_47", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What recurrent neural network models are benchmarked against the Pyramidal Recurrent Unit to evaluate its performance on word-level language modeling?", "answer": "[{'answer': 'Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM', 'type': 'abstractive'}]", "main_doc": "1808.09029.pdf", "documents": "['1808.09029.pdf', '1908.11546.pdf', '1905.12260.pdf', '1906.05474.pdf', '2001.05493.pdf', '1912.01772.pdf', '1704.06194.pdf', '1604.00400.pdf', '1910.07481.pdf', '1909.00754.pdf', '1909.08859.pdf']"}
{"_id": "paper_tab_48", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "Was the baseline Transformer-based sequence-to-sequence model evaluated on the newly introduced video game data-to-text corpus, and what were its performance metrics, including BLEU, METEOR, ROUGE, CIDEr, and SER?", "answer": "[{'answer': 'Yes, Transformer based seq2seq is evaluated with average BLEU 0.519, METEOR 0.388, ROUGE 0.631 CIDEr 2.531 and SER 2.55%.', 'type': 'abstractive'}]", "main_doc": "1910.12129.pdf", "documents": "['1910.12129.pdf', '1908.06264.pdf', '1909.00361.pdf', '2002.04181.pdf', '2002.06644.pdf', '1812.10479.pdf', '1603.00968.pdf', '1911.04952.pdf', '1910.10288.pdf', '1804.07789.pdf', '1912.01772.pdf', '1809.02279.pdf', '1904.09678.pdf']"}
{"_id": "paper_tab_49", "domain": "VisDoM", "sub_domain": "paper_tab", "question": "What linguistic features are specified in the biLSTM-based Arabic diacritic recovery model for case ending reconstruction?", "answer": "[{'answer': 'POS, gender/number and stem POS', 'type': 'abstractive'}]", "main_doc": "2002.01207.pdf", "documents": "['2002.01207.pdf', '1909.06162.pdf', '1911.10049.pdf', '1911.07228.pdf', '1810.05241.pdf']"}